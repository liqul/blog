<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liqun&#39;s Homepage</title>
  
  <subtitle>A place for knowledge</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="liqul.github.io/blog/"/>
  <updated>2018-04-11T13:50:20.819Z</updated>
  <id>liqul.github.io/blog/</id>
  
  <author>
    <name>Liqun Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Eventual Consistency vs. Strong Consistency</title>
    <link href="liqul.github.io/blog/consistency_model/"/>
    <id>liqul.github.io/blog/consistency_model/</id>
    <published>2018-03-16T07:05:39.000Z</published>
    <updated>2018-04-11T13:50:20.819Z</updated>
    
    <content type="html"><![CDATA[<p>Here is a very good explanation about eventual consistency and strong consistency. I’d like to put the two figures on that page below:</p><p><img src="/blog/assets/eventual-consistency.png"><br>Fig. 1 figure for eventual consistency</p><p>In this example above, Node A is the master, which replicate X to its followers Node B and C. Suppose the time when X is successfully writen to Node A is t_1, and the time when X  is replicated to Node B is t_2. Any time between t_1 and t_2, if a client reads from Node A, it gets the latest value of X. But if the client reads from Node B, it gets an old version of X. In other words, the result of a read depends on which Node the client reads from, and therefore, the storage service presents an inconsistent global view for the client. </p><p>In contrast, if the storage service provides a strong consistency semantic, the client should always read the same result. This figure below illustrates an example of strong consistency. </p><p><img src="/blog/assets/strong-consistency.png"><br>Fig. 2 figure for strong consistency</p><p>The single difference between Fig. 1 and Fig. 2 is that before X has been successfully replicated to Node B and C, a read request of X to Node B and C should be blocked. How about reading from Node A before all replications done? It should be blocked as well, and therefore, there is a missing  ‘lock’ symbol in Fig. 2. The full picture should has the following steps:</p><ol><li>A client issues a write request of X to Node A;</li><li>Node A locks X globally to prevent any read or write to X;</li><li>Node A store X locally, and then replicate X to Node B and C;</li><li>Node B and C store X locally and send Node A a response;</li><li>After receiving from Node B and C, Node A release the lock of X and respond to the client;</li></ol><p>These steps are only used to understand the basic idea of strong consistency, which is not necessary a best practice. If you want to know more details, research some real systems such as Spanner or Kudu.</p><p>While sounds more understandable for developers, strong consistency trades Availability for Consistency. In the instance shown in Fig. 2, a client may need to wait for a while before it reads the value of X. If the networking fails apart (for example, Node C is partitioned from Node A and B), any write requests to Node A will fail if each value is forced to have 3 replications. In addition, if the global lock service fails, the storage service will also be unavailable. In general, a storage service with strong consistency has much higher requirements to the infrastructure in order to function well, and therefore, is more difficult to scale compared to one with eventual consistency.</p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel" target="_blank" rel="noopener">AWS S3’s consistency model</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Here is a very good explanation about eventual consistency and strong consistency. I’d like to put the two figures on that page below:&lt;/p
      
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="distributed" scheme="liqul.github.io/blog/tags/distributed/"/>
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="consistency" scheme="liqul.github.io/blog/tags/consistency/"/>
    
  </entry>
  
  <entry>
    <title>Reading &quot;State Management in Apache Flink&quot;</title>
    <link href="liqul.github.io/blog/flink/"/>
    <id>liqul.github.io/blog/flink/</id>
    <published>2018-02-05T12:47:09.000Z</published>
    <updated>2018-03-06T10:54:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Updated on 2018-02-05 </p><p>I recently read an excellent <a href="https://streaml.io/blog/exactly-once/" target="_blank" rel="noopener">blog</a> about exactly-once streaming processing. It details typical solutions for exactly-once processing used by various open source projects. No matter if the solution is based on streaming or mini-batch, exactly-once processing incurs a inevitably latency. For example in Flink, the state at each operation can only be read at each checkpoint, in order not to read something that might be rollbacked during a crash. </p><p>===</p><p>I recently read the VLDB’17 paper “State Management in Apache Flink”. In one sentence,</p><blockquote><p>The Apache Flink system is an open-source project that provides a full software stack for programming, compiling and running distributed continuous data processing pipelines.</p></blockquote><p>For me, Flink sounds yet another computation framework alternative to Spark and Mapreduce with a workflow management tool. However,</p><blockquote><p>In contrast to batch-centric job management which prioritizes reconfiguration and coordination, Flink employs a schedule-once, long-running allocation of tasks. </p></blockquote><p>How exactly does a streaming-centric framework differ from a batch-centric framework? Conceptually, there is no fundamental difference between the two. Any batch processing framework can work “like” a streaming processing framework by reducing the size of each batch to 1. However, in practice, they are indeed different. A batch-centric framework usually involve a working procedure such as </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch 1 start</span><br><span class="line">do some job</span><br><span class="line">batch 1 end</span><br><span class="line">update some state</span><br><span class="line"></span><br><span class="line">batch 2 start</span><br><span class="line">do some job</span><br><span class="line">batch 2 end</span><br><span class="line">update some state</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Note that the job is started and ended within each batch. In contrast, for a streaming-centric framework, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">start a job</span><br><span class="line"></span><br><span class="line">receiving a new data</span><br><span class="line">process the data</span><br><span class="line">update some state</span><br><span class="line">pass the data to the next job</span><br><span class="line"></span><br><span class="line">receiving a new data</span><br><span class="line">process the data</span><br><span class="line">update some state</span><br><span class="line">pass the data to the next job</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">end the job</span><br></pre></td></tr></table></figure><p>This comparison is clear. A job in the streaming-centric framework usually work continuously without being started/stopped multiple times as in a batch-centric framework. Starting and stopping a job usually incur some cost. Therefore, a batch-centric framework usually performs less efficiently compared to a streaming-centric one. Additionally, if the application is mission critical (e.g., malicious event detection), processing data in batch usually means high latency. However, if the task is batch-by-batch in nature, a batch-centric framework usually performs as efficiently as a streaming-centric one. </p><p>Another problem is about snapshotting. Snapshotting is a key capability for a processing pipeline. A snapshot is consist of both the state and data. The global state of the pipeline is composed of the sub-state of each operator. Each state is either a <em>Keyed state</em> or a <em>Operator state</em>. The former represents all type of states indexed by the key from data (e.g., count by key); the latter is more an operator-aware state (e.g., the offset of data). Snapshotting the data is tricky where Flink assumes that </p><blockquote><p>Input data streams are durably logged and indexed externally allowing dataflow sources to re-consume their input, upon recovery, from a specific logical time (offset) by restoring their state. This functionality is typically provided by file systems and message queues such as Apache Kafka</p></blockquote><p>Each operator snapshots the current state once processing a mark in the dataflow. With the marks and the snapshotted states of each operator, we can always restore the system state from the last snapshot. One should note that the keyed state is associated with an operator, and therefore, the data with the same key should be physically processed at the same node. Otherwise, there should be a scalability issue. Consequently, there should be a shuffle before such operators, or the data is already prepared to ensure data with the same key is processed at a single node.</p><p>In conclusion, Flink is great as streaming-centric frameworks have some fundamental advantages over batch-centric frameworks. However, since batch-centric frameworks such as Mapreduce and Spark are already widely applied, there should be really strong motivations to migrate existing systems to this new framework. Moreover, the implementation quality and contributor community are two very important facts for the adoption of a new born framework, while Spark has been a really popular project. Maybe, a higher level project such as the <a href="https://beam.apache.org/" target="_blank" rel="noopener">Apache Beam</a> is a good direction. Beam hides the low-level execution engine by unifying the interface. Any application written in Beam is then compiled to run on an execution engine such as Spark or Flink.</p>]]></content>
    
    <summary type="html">
    
      Understanding the concepts in Apache Flink.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="flink" scheme="liqul.github.io/blog/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Notes on MR memory issues</title>
    <link href="liqul.github.io/blog/experience_with_mr_memory_parameters/"/>
    <id>liqul.github.io/blog/experience_with_mr_memory_parameters/</id>
    <published>2018-02-05T03:29:09.000Z</published>
    <updated>2018-03-06T10:52:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>Updated on 2018-02-05</p><p>I recently encountered several OOMs from mapper tasks reading parquet files. The yarn container is killed due to running out of physical memory. Since I already set the JVM memory to 0.8 of the container size, I’m pretty sure that this is due to off-heap memory allocation issues. I found the two jira issues <a href="https://issues.apache.org/jira/browse/SPARK-4073" target="_blank" rel="noopener">here</a> and <a href="https://issues.apache.org/jira/browse/PARQUET-118" target="_blank" rel="noopener">here</a>, pointing me to the snappy codec used by parquet for decompression. There aren’t so much I can do except allocating more memory beside the JVM.  </p><p>===</p><p>I recently experienced two OOM problems running a mapreduce application. The MR application reads from a group of parquet files, shuffles the input rows, and writes into parquet files, too. </p><p>The first OOM is thrown by the mapper with error logs look like following</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2017-06-22 09:59:10.978 STDIO [ERROR] [WORKER] [129] Container [pid=14638,containerID=container_e26_1495868456939_0784_01_000066] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 1.5 GB of 2.1 GB virtual memory used. Killing container.</span><br><span class="line">Dump of the process-tree for container_e26_1495868456939_0784_01_000066 :</span><br><span class="line">    |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE</span><br><span class="line">    |- 14638 14632 14638 14638 (bash) 0 0 17096704 774 /bin/bash -c /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242 1&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stdout 2&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stderr  </span><br><span class="line">    |- 14655 14638 14638 14638 (java) 4654 290 1616650240 272880 /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242</span><br></pre></td></tr></table></figure><p>After some investigation, I realized this is due to a misconfiguration of the mapper container memory limit (mapreduce.map.memory.mb) and the mapper JVM memory limit (mapreduce.map.java.opts). Basically, the latter should be smaller than the former, because the mapper container consumes some memory itself. After setting mapreduce.map.java.opts = mapreduce.map.memory.mb * 0.8, the OOM problem is gone. I note that this also applies for the reducer, which has two corresponding parameters (mapreduce.reduce.java.opts and mapreduce.reduce.memory.mb). This <a href="https://discuss.pivotal.io/hc/en-us/articles/201462036-MapReduce-YARN-Memory-Parameters" target="_blank" rel="noopener">article</a> explains nicely.</p><p>The second OOM issue is much harder to address, which comes with the shuffle phase. I saw error logs like following</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">2017-06-21 20:22:42.294 STDIO [ERROR] [WORKER] [100] Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:514)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure><p>This is not an old problem which could be found in <a href="https://issues.apache.org/jira/browse/MAPREDUCE-6447" target="_blank" rel="noopener">here</a> and <a href="https://issues.apache.org/jira/browse/MAPREDUCE-6108" target="_blank" rel="noopener">here</a>. Most of the solutions suggest tuning the three parameters:</p><ul><li>mapreduce.reduce.shuffle.input.buffer.percent (default 0.7): how much memory shuffle can use to store data pulled from mappers for in-memory sort.</li><li>mapreduce.reduce.shuffle.memory.limit.percent (default 0.25): how much memory each shuffle thread uses for pulling data from mappers into memory. </li><li>mapreduce.reduce.shuffle.parallelcopies (default 10): the number of shuffle thread can run in parallel</li></ul><p>Some solutions claims that we should have </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent * mapreduce.reduce.shuffle.parallelcopies &lt; 1</span><br></pre></td></tr></table></figure><p>which is actually not correct. MergeManager allocates memory to shuffle threads which is used for copying mapper output into memory. Each time a shuffle thread applies for a copy action, the MergeManager determines if the application is granted by checking (1) if the appliedMemory size is more than the max memory each shuffle thread can have. This is controlled by mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent. Suppose the reducer JVM has 3.5G heap size, each shuffle can apply no more than 3500*0.7*0.25=612M with default settings. (2) if the usedMemory is more than memoryLimit. The used memory accounts for memory used by shuffles and in-memory merge. The memory limit is calculated by 3.5*0.7 = 2.45G with 3.5G JVM heap size. Now, if the usedMemory is 2.44G and appliedMemory is 612M, the real memory used by shuffle could be more than 3G !!! </p><p>This is not a bug, since there is a detailed comments in MergeManagerImpl.reserve. The comments explain why the actually used memory could be one shuffle larger than the limit. From the other side, this could cause OOM. Due to this issue, there’s no 100% safe way to fix the OOM by tuning the parameters. We can only mitigate this problem by reducing mapreduce.reduce.shuffle.input.buffer.percent and/or mapreduce.reduce.shuffle.memory.limit.percent. One should carefully calculate these parameters according to the real workload. Especially, the memory each shuffle can use limit the max size of output from each mapper. For example, if the mapper produces a 300M intermediate file, the shuffle should be able to allocate memory more than 300M. Otherwise, all sort will be done on disk. </p><p>One more thing is about the parquet format. It is a highly compressed format, and therefore the decompressed mapper output is much larger than the input split size. I think this is why OOM happens more frequently for parquet files than other file formats.</p>]]></content>
    
    <summary type="html">
    
      I recently experienced two OOM problems running a mapreduce application. The MR application reads from a group of parquet files, shuffles the input rows, and writes into parquet files, too. 
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="Mapreduce" scheme="liqul.github.io/blog/tags/Mapreduce/"/>
    
      <category term="OOM" scheme="liqul.github.io/blog/tags/OOM/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the SSD</title>
    <link href="liqul.github.io/blog/ssd/"/>
    <id>liqul.github.io/blog/ssd/</id>
    <published>2017-12-07T12:47:09.000Z</published>
    <updated>2018-03-06T10:55:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>Reading the chapter 13.5 “Arranging data on disk” in the book “DATABASE SYSTEM: IMPLEMENTATION” makes me think of a question: How data should be arranged on a SSD (Solid-State Drive)? This is indeed an old question, so after doing some research with Google, I find some very good explanations. </p><p><a href="http://site.aleratec.com/blog/2011/09/22/overview-pages-blocks-ftls-solidstate-drive-ssd/" target="_blank" rel="noopener">An Overview of Pages, Blocks and FTLs in a Solid-State Drive (SSD)</a></p><p><a href="https://www.extremetech.com/extreme/210492-extremetech-explains-how-do-ssds-work" target="_blank" rel="noopener">How Do SSDs Work?</a></p><p>The two articles above describes how SSD works differently from a HDD. Some key points to take away are:</p><ul><li>The minimum read/write unit for a SSD is a <em>page</em>. A <em>block</em> is made up of a set of pages.</li><li>A dirty page (with data) can <em>not</em> be overwritten before being erased.</li><li>The minimum erase unit for a SSD is a <em>block</em>.</li><li>Each block has a finite program/erase cycles (P/E cycles).</li></ul><p>Within a SSD, data can only be erased by block. <em>Garbage collection</em> need to run to reclaim logically deleted pages (e.g., due to update). Therefore, data in blocks with deleted pages are packed and rewrite to another empty block. A piece of data might be rewritten over and over again, which is called the <em>write amplification</em> problem. This also leads to the fact that data is moving constantly which is quite different from data stored within a HDD.</p><p><a href="https://blog.2ndquadrant.com/tables-and-indexes-vs-hdd-and-ssd/" target="_blank" rel="noopener">Tables and indexes vs. HDD and SSD</a></p><p>This article above discussed about the strategy of storing table data and indexes on HDD vs. SSD. The results are clearly shown by those charts. Also, the discussion in the comments is worthwhile for reading. </p><p><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/" target="_blank" rel="noopener">Coding for SSDs</a></p><p>Finally, I found a very interesting serial of blogs “Coding for SSDs”. The author built a key-value store optimized for SSDs. There are quite a lot of insights in these blogs. </p><p>In conclusion, SSDs outperform HDDs from almost every aspects today, except the price per bit. However, I envision that in the near future, the price could be made low enough to replace most HDDs. SSDs are almost drop-in replacement for HDDs. However, to get the best performance from SSDs, developers do need to take care about the data access characteristics of SSDs.</p>]]></content>
    
    <summary type="html">
    
      Understanding the characteristics of SSD.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="ssd" scheme="liqul.github.io/blog/tags/ssd/"/>
    
  </entry>
  
  <entry>
    <title>Things about replication in Elasticsearch</title>
    <link href="liqul.github.io/blog/things-about-replication-in-elasticsearch/"/>
    <id>liqul.github.io/blog/things-about-replication-in-elasticsearch/</id>
    <published>2017-11-10T03:33:09.000Z</published>
    <updated>2018-03-06T10:55:29.000Z</updated>
    
    <content type="html"><![CDATA[<h3><span id="updated-on-2017-11-10">Updated on 2017-11-10</span></h3><p>Elasticsearch is evolving fast in the past few years. There have been quite some discussions on data loss during node crashes, which can be found <a href="https://github.com/elastic/elasticsearch/issues/10933" target="_blank" rel="noopener">here</a> and <a href="https://github.com/elastic/elasticsearch/issues/14252" target="_blank" rel="noopener">here</a>. Most of the issues have been fixed as described <a href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html" target="_blank" rel="noopener">here</a>. However, since Elasticsearch carried out a major upgrade to version 5+, some serious issues still remain for low versions, e.g., the stale replica problem described <a href="https://github.com/elastic/elasticsearch/issues/14671" target="_blank" rel="noopener">here</a>. </p><p>I already discussed in the original article about the two node issue. I recently carried out an experiment with 3 nodes which is actually the recommended minimum size for an Elasticsearch cluster. With 3 nodes, the quorum size is 2 and the minimum master nodes is 2 (discovery.zen.minimum_master_nodes). Therefore, there is always an overlap where some nodes have the latest state. Let me explain this with an example. The nodes are A, B, and C. We go through the following test steps:</p><ol><li>Create a new index with 2 replicas, i.e., 3 copies in total;</li><li>Shut down A;</li><li>Index 1 document on index B and C successfully;</li><li>Shut down B and C;</li><li>Turn on A;</li></ol><p>What about the state for the index? The replica on A will not be allocated as the primary shard since there is only one alive node less than the minimum master nodes 2. Now, we turn on B. As B has the latest state, B propagate the latest state to A. </p><p>Most of open sourced distributed system rely on a mature consensus approach such as Raft or Zookeeper. However, Elasticsearch decided to invent its own. This actually leads to most of those serious issues. </p><p>========</p><p>Replication is a key feature for Elasticsearch from two aspects: (1) When some machines fail, the alive ones can still serve requests; (2) Replication boosts the read speed since read can retrieve data from multiple nodes simultaneously. </p><p>Elasticsearch follows a primary-secondary fashion for replication. When a write request (e.g., create, index, update, delete) arrives, it is first forward to the primary replica. The primary replica finishes the request, and then, concurrently forward the requests to all alive secondary replicas. There are a few details about this process. </p><p>First, there is a concept of write consistency level in Elasticsearch, with available options one, quorum, and all. This concept is a bit different from what we normally find for other systems such as Kafka. It barely forces the primary replica to check if there are enough alive replicas available receiving a write request. For instance, suppose we have a cluster of 3 nodes with replica number 2, i.e., each shard is with one primary replica and 2 secondary replicas. If we set the write consistency level to quorum, when the primary replica receives a index request, it checks if there are at least 2 replicas available (i.e., &gt;=replicas/2+1). If the check passes, the primary replica will start the action of index, after which it forward the request to all replicas. One should note that the consistency level is only for the check. This means there is a chance when a replica fails after the check, and right before the action.</p><p>Second, we need to answer the question: when shall the primary replica respond to the client? It turns out that there are two modes, sync and async as discussed <a href="https://discuss.elastic.co/t/es-default-async-or-sync/19654" target="_blank" rel="noopener">here</a>. The sync mode means the primary replica only responds the client if “all” secondary replicas have finished the request. Note that the “all” here, which has nothing to do with the selected write consistency level. Under the async mode, the primary replica responds to the client right after itself finishing the request. The request is then forward to other replicas in an async way. This accelerate the response timing for the client, which however may lead to overload for the Elasticsearch cluster. Mean while, since the request propagates eventually to the replicas, there will be no read-write consistency guarantee even inside the same session if the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-preference.html" target="_blank" rel="noopener">read preference</a> is not set to primary. </p><p>In normal case, there is only one primary replica for each shard. Once the primary replica fails, a secondary replica is elected to serve as primary. In some special situations, the primary replica may lose connection to other replicas, leading to multiple primary replicas in the system, which is called the split brain problem as discussed <a href="https://qbox.io/blog/split-brain-problem-elasticsearch" target="_blank" rel="noopener">here</a>. The cue to this problem is by setting the discovery.zen.minimum_master_nodes to &gt;= half of nodes + 1. For example, if you have 3 nodes, the minimum_master_nodes should be set to 2. By setting the minimum_master_nodes we ensure that the service is only available if there are more than minimum_master_nodes living nodes within one master domain. In other words, there can not be two masters in the system. </p><p>Finally, I want to discuss the problem of stale shard which I read recently from <a href="https://www.elastic.co/blog/tracking-in-sync-shard-copies" target="_blank" rel="noopener">here</a>. Let’s start by use a concrete example. Say if we have two nodes and each shard has two replicas (one primary and the other secondary). We first index 10 documents with the secondary shard node turned off. Then, we turn off the primary shard node, and bring up the secondary shard node. The question here is whether the secondary shard will be promoted to primary? If it is, how about the 10 documents we indexed before? According to this <a href="https://www.elastic.co/blog/tracking-in-sync-shard-copies" target="_blank" rel="noopener">blog</a>, with Elasticsearch v5+, the primary shard will not only do the index, but also inform the master about in-sync shards. In this case, the answer to our questions are no. Because the secondary shard is not in in-sync state after being brought up. I didn’t experiment it myself about this since I don’t have a Elasticsearch v5+ environment. I only tested this with Elasticsearch 2.4.5 where I found different answer. After secondary shard node was brought up, the secondary shard was indeed promoted to primary, and the 10 documents were lost if I then brought up the previous primary shard node. This is indeed a problem if such special situation happens, which however should be quite rare in practice especially if you have more than 2 nodes, and with quorum write consistency level.</p>]]></content>
    
    <summary type="html">
    
      Notes on what I learn about replication in Elasticsearch.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="consistency" scheme="liqul.github.io/blog/tags/consistency/"/>
    
      <category term="replication" scheme="liqul.github.io/blog/tags/replication/"/>
    
      <category term="elasticsearch" scheme="liqul.github.io/blog/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>读《洪业：清朝开国史》有感</title>
    <link href="liqul.github.io/blog/ming_lessons/"/>
    <id>liqul.github.io/blog/ming_lessons/</id>
    <published>2017-10-28T12:47:09.000Z</published>
    <updated>2018-04-11T00:11:27.878Z</updated>
    
    <content type="html"><![CDATA[<p>读<a href="https://www.amazon.cn/%E6%B4%AA%E4%B8%9A-%E6%B8%85%E6%9C%9D%E5%BC%80%E5%9B%BD%E5%8F%B2-%E9%AD%8F%E6%96%90%E5%BE%B7/dp/B06XFTC5TJ/ref=sr_1_1?ie=UTF8&amp;qid=1509194106&amp;sr=8-1&amp;keywords=%E6%B4%AA%E4%B8%9A" target="_blank" rel="noopener">《洪业：清朝开国史》</a>关于崇祯的一些感受。</p><h2><span id="魏忠贤问题">魏忠贤问题</span></h2><ul><li>上策：保持互相制衡，两方敲打，改革弊政</li><li>中策：无所作为</li><li>下策：杀魏忠贤导致文臣势力过大</li></ul><h2><span id="皇太极的问题">皇太极的问题</span></h2><ul><li>上策：联络岱善，内部瓦解后金统治阶级</li><li>中策：同意与皇太极议和，攘外必先安内，剿灭李自成</li><li>下策：同时面对两股敌人</li></ul><h2><span id="用人的问题">用人的问题</span></h2><ul><li>上策：黑猫白猫，不在意细节，以能力取人</li><li>中策：坚持用人时间长一点，不随意更替</li><li>下策：动辄得咎，反复无情，换人如流水</li></ul><h2><span id="自杀的问题">自杀的问题</span></h2><ul><li>上策：未知</li><li>中策：南下或北上。南下学宋高宗，虽末世无法与南宋比肩，但仍有一战的实力；北上联系后金，一同剿灭李自成，有崇祯在后金不那么容易南侵，何况还有吴三桂</li><li>下策：自杀身死，连太子也没有放过</li></ul>]]></content>
    
    <summary type="html">
    
      读《洪业：清朝开国史》有感。
    
    </summary>
    
      <category term="Book Reading" scheme="liqul.github.io/blog/categories/Book-Reading/"/>
    
    
      <category term="洪业" scheme="liqul.github.io/blog/tags/%E6%B4%AA%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>Quorum in Amazon Aurora</title>
    <link href="liqul.github.io/blog/quorum_in_amazon_aurora/"/>
    <id>liqul.github.io/blog/quorum_in_amazon_aurora/</id>
    <published>2017-10-27T08:12:09.000Z</published>
    <updated>2018-03-06T10:55:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>I recently read a serial of posts about the quorum mechanism in Amazon Aurora, which is a distributed relational database. These posts are: </p><ul><li><a href="/blog/assets/Amazon Aurora under the hood_ quorums and correlated failure _ AWS Database Blog.pdf">post1</a>: quorums and correlated failure.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Quorum Reads and Mutating State _ AWS Database Blog.pdf">post2</a>: quorum reads and mutating state.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Reducing Costs Using Quorum Sets _ AWS Database Blog.pdf">post3</a>: reducing costs using quorum sets.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Quorum Membership _ AWS Database Blog.pdf">post4</a>: quorum membership.</li></ul><p>Besides, there is actually a Sigmod’17 paper about Amazon Aurora which could be found <a href="http://www.allthingsdistributed.com/files/p1041-verbitski.pdf" target="_blank" rel="noopener">here</a>. I only briefly went through that paper which spends most of words talking about the basic architecture. </p><p>I like this serial of posts which is a very good tutorial if you want to learn practical usage of quorum. By definition, a quorum model is </p><blockquote><p>Formally, a quorum system that employs V copies must obey two rules. First, the read set, Vr, and the write set, Vw, must overlap on at least one copy.</p></blockquote><blockquote><p>Second, you need to ensure that the quorum used for a write overlaps with prior write quorums, which is easily done by ensuring that Vw &gt; V/2. </p></blockquote><p>At the heart of this model is that each read/write to the cluster of nodes overlaps at least one node with each other. </p><p>While it is cool to enjoy the replication benefit with the quorum model, there comes cost for both read and write. For read, a client may need to consult multiple nodes (i.e., the read set) in order to ensure reading the latest state. For write, the multiple copies need to be materialized in order to maintain the quorum model. The author introduced the basic ideas of solving these two problems in post2 and post3. Especially, for the read penalty, the master maintains a cache of the status of all successful replicas, including their latency estimations. Therefore, a client need only to find information from the master in order to read the latest information. </p><p>Membership management is discussed in post4 where they use the approach of overlapping quorums to solve the node failure problem. One nice feature is that this approach is robust given new failures happening right during the handling process. </p><p>Finally, I’d like to end up with the following sentence from the posts:</p><blockquote><p>State is often considered a dirty word in distributed systems—it is hard to manage and coordinate consistent state as you scale nodes and encounters faults. Of course, the entire purpose of database systems is to manage state, providing atomicity, consistency, isolation, and durability (ACID).</p></blockquote>]]></content>
    
    <summary type="html">
    
      Notes on the quorum mechanism in Amazon Aurora.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="quorum" scheme="liqul.github.io/blog/tags/quorum/"/>
    
      <category term="amazon aurora" scheme="liqul.github.io/blog/tags/amazon-aurora/"/>
    
  </entry>
  
  <entry>
    <title>Reading the New Apache HBase MOB Compaction Policy</title>
    <link href="liqul.github.io/blog/new_apache_hbase_mob_compaction_policy/"/>
    <id>liqul.github.io/blog/new_apache_hbase_mob_compaction_policy/</id>
    <published>2017-08-29T02:19:09.000Z</published>
    <updated>2018-03-06T10:54:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>In case you want to understand more on MOB (Moderate Object Storage), you may refer to this <a href="https://issues.apache.org/jira/browse/HBASE-11339" target="_blank" rel="noopener">issue</a>. Basically, hbase was first introduced with capability of storing mainly small objects (&lt;100k). Moderate objects stand for files from 100k to 10m. </p><p>Recently, there is a <a href="https://blog.cloudera.com/blog/2017/06/introducing-apache-hbase-medium-object-storage-mob-compaction-partition-policies/?elqTrackId=2a7ed08f6935464e84b51ad5a8f15cb2&amp;elq=896612af50b741d7b8bf576ac30276e4&amp;elqaid=4662&amp;elqat=1&amp;elqCampaignId=2850" target="_blank" rel="noopener">blog</a> introducing the new compaction policy for MOB files. The problem with the initial approach is multiple compaction. For instance, the goal is to compact the objects created in one calendar day into one big file. The compaction process starts after the first hour. The objects created in the first hour are compacted into a temporal file. Then, the objects created in the second hour, and the temporal file created for the first hour are compacted into a new temporal file…</p><p>In this way, finally, all objects created in one day is compacted into one file. However, the objects in the first hour is compacted quite a few of times, wasting IO. The new method is based on partition. For instance, we may compact the objects in each hour of day, which is the first stage. Then, the temporal files in each hour are compacted into the final file, which is the second stage. This saves a lot of IO in comparison with the initial approach. Actually, this improvement is quite straightforward. </p><p>What I found really insightful is about the compaction partitioned by the created time. Note that the creation time of each object is never changed during its life time. Therefore, suppose a set of objects is compacted into a big file which say contains objects between 2017-08-23 ~ 2017-08-24. After a while, some objects in that set may be deleted (with tombstone in hbase), or replaced with newer versioned metadata. However can we remove these objects physically? The answer is easy. We search for all objects created between 2017-08-23 ~ 2017-08-24, which should result in a subset of the original set of objects. We then extract the remain objects into a new big file, and delete the old big file. There are two other essential points to achieve the clear process described above: (1) the metadata should be 1:1 mapping with the objects. In other words, there should be no more than 1 metadata pointing to the same object. (2) the creation time and the pointer to file should be always updated atomically.</p>]]></content>
    
    <summary type="html">
    
      Notes on the new apache hbase mob compaction policy.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="hbase" scheme="liqul.github.io/blog/tags/hbase/"/>
    
      <category term="compaction" scheme="liqul.github.io/blog/tags/compaction/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Chain Replication</title>
    <link href="liqul.github.io/blog/notes-chain-replication/"/>
    <id>liqul.github.io/blog/notes-chain-replication/</id>
    <published>2017-07-28T10:05:39.000Z</published>
    <updated>2018-03-06T10:53:39.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/blog/assets/chain_replication.svg" alt="Chain Replication"></p><p>I learned the idea of chain replication from <a href="https://github.com/hibari/hibari" target="_blank" rel="noopener">hibari</a>,</p><blockquote><p>Hibari is a production-ready, distributed, ordered key-value, big data store. Hibari uses chain replication for strong consistency, high-availability, and durability. Hibari has excellent performance especially for read and large value operations.</p></blockquote><p>The term “strong consistency” indeed caught my attention as I already know a few key-value storage services with only eventually consistency, e.g., openstack swift. I read its doc to find out the key tech sitting in the core is called “chain replication”. I did some investigation about this concept which actually back to very early days in 2004 in a OSDI <a href="http://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf" target="_blank" rel="noopener">paper</a>. </p><p>The idea is actually very easy to understand. The service maintains a set of chains. Each chain is a sequence of servers, where one server is called the <em>head</em>, and one is called the <em>tail</em>; all servers in between are <em>middle</em> servers. The figure in the very beginning shows such an example with two middle servers. Each write request is directed to the head server, and the update is pipelined from the head server to the tail server though the chain. Read requests are directed to only tail servers. What a client can read from the chain is definitely replicated across all servers belonging to the chain, and therefore, strong consistency is guaranteed. </p><p>Though the idea sounds straightforward, there are few practical issues. First of all, the traffic load at tail servers is higher than other servers, since they handle both write and read traffics. A load balancing aware chain organization algorithm is needed to balance the load across all servers. For instance, one server may be middle server of one chain and meanwhile tail server of another chain (see Fig. 3 in the <a href="http://www.snookles.com/scott/publications/erlang2010-slf.pdf" target="_blank" rel="noopener">Hibari paper</a>). Another problem is failure handling. There should be a way of detecting failed servers, which turns out to be non-trivial in such distributed world. There are also plenty of issues about recovering from failures, replication, and migration. In conclusion, this “simple” idea comes with a bunch of tough issues. </p><p>There are only few open source projects based on chain replication, such as <a href="https://github.com/hibari/hibari" target="_blank" rel="noopener">Hibari</a> and <a href="https://github.com/CorfuDB/CorfuDB" target="_blank" rel="noopener">CorfuDB</a>. One fundamental reason may be the cost paid for strong consistency is too high. One killer application for object storage is handling highly massive objects such as user data in social network companies. However, the chain can never cross data centers in order for low latency. The idea of using chained servers is not really new. HDFS also use a pipeline to optimize data transfer latency while achieving strong consistency. Therefore, if the number of files is not a issue, storing them directly on HDFS might be a reasonable choice, given the advantage of naive integration with other Hadoop components.</p>]]></content>
    
    <summary type="html">
    
      A brief understanding about chain replication.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="distributed" scheme="liqul.github.io/blog/tags/distributed/"/>
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="replication" scheme="liqul.github.io/blog/tags/replication/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习笔记</title>
    <link href="liqul.github.io/blog/notes-learning-spark/"/>
    <id>liqul.github.io/blog/notes-learning-spark/</id>
    <published>2017-07-07T11:19:09.000Z</published>
    <updated>2018-03-06T10:53:45.000Z</updated>
    
    <content type="html"><![CDATA[<h2><span id="spark与scala">Spark与Scala</span></h2><p>在学习Spark之前务必对Scala有所理解，否则面对完全陌生的语法是很痛苦的。</p><p>Scala的一种入门方式是：</p><ol><li>学习<a href="https://www.coursera.org/learn/progfun1/home/welcome" target="_blank" rel="noopener">Scala 函数式程序设计原理</a>。这是Scala作者自己开的课程。没什么比语言作者更加能理解这门语言的了，是切入Scala编程的最好入门方式。课程习题参考了《计算机程序的构造和解释》一书，非常经典。</li><li>阅读《Scala in depth》一书，对一些Scala的重点概念有更加详细的讨论。</li><li>根据特定的topic，Google各种网络资料。</li></ol><h2><span id="rdd-resilient-distributed-datasets">RDD (Resilient Distributed Datasets)</span></h2><h3><span id="rdd的含义">RDD的含义</span></h3><p>RDD是spark中用于记录数据的数据结构。根据具体的RDD类型，数据有不同的组织形式。一个RDD包含多个partition，partition是并行的基本单位。RDD可能存在内存中，也可能存在硬盘里，或者两者皆有。一个RDD可以由数据源创建，也可能由其它RDD计算得到，所有参与计算RDD的RDD称为父RDD。若对mapreduce有所了解，可以把partition看作mapper的一个split。</p><h3><span id="rdd中的窄依赖narrow-dependency和宽依赖wide-dependency">RDD中的窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</span></h3><p>若一个RDD中每一条记录仅仅依赖父RDD中唯一一条记录，则其为窄依赖，否则为宽依赖。比如在map中，每一条子RDD中的记录就对应着唯一父RDD中的对应记录。而groupByKey这样的操作中，子RDD中的一条记录，我们并不知道它究竟来自哪个父RDD中的哪个partition。</p><p>利用mapreduce的概念来理解，一组连续的窄依赖操作可以用一个mapper来实现，而宽依赖操作则只能依赖reducer。正因如此，一组连续窄依赖中产生的“中间结果”（实际并不需要产生这些中间结果）是没有存在的意义的，只要知道输入、操作就能直接计算输出了。举个具体的例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resRDD = srcRDD.map(_ + <span class="number">1</span>).map(_ + <span class="number">2</span>).filter( _ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>中的transformation链可以看作mapreduce下的一个mapper，一条记录从左到右执行不依赖其它记录。若把上面例子改为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resRDD = srcRDD.map(_ + <span class="number">1</span>).distinct().filter( _ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>其中加入了distinct意味着一条记录从左到右无法利用一个mapper就完成，必须截断加入一个reducer。这里需要理解mapreduce中的一个mapper并不是等价于spark中的一个map操作，而是对应所有窄操作的组合，例如filter、flatMap、union等等。</p><blockquote><p>补充材料：<a href="https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=67043332" target="_blank" rel="noopener">why spark’s mapPartitions transformation is faster than map</a>。其中的一句话讲的非常清楚——you probably know that “narrow” transformations/tasks happen independently on each of the partitions. 即窄操作在单机即可完成，不需要依赖保存在其它主机上的partition。</p></blockquote><h3><span id="rdd中persist和checkpoint的逻辑">RDD中persist和checkpoint的逻辑</span></h3><p>persist的目的是为了加快后续对该RDD的操作；checkpoint的目的是为了减少长执行链失败带来的开销。由于目的不同，如果persist的RDD丢失了，可以重新计算一遍（这就是普通cache的基本逻辑）。反过来，如果checkpoint丢失了，则无法重新计算，因为该checkpoint之前的内容都遗忘了。cache只是persist的一个子操作，其storage level为memory_only。</p><p>persist和checkpoint都是异步操作，执行persist或checkpoint命令仅仅给对应的RDD加上一个mark，后续交给block manager完成具体的物化操作（？？？）。persist有多种storage level，包括memory, off heap memory, disk等等。在spark中，block manager负责所有的数据存储管理，包括persist、checkpoint、或shuffle产生的中间数据等。</p><p>值得一提的是关于off heap memory的<a href="http://stackoverflow.com/questions/6091615/difference-between-on-heap-and-off-heap" target="_blank" rel="noopener">概念说明</a>。简而言之，off heap memory就是不受JVM管控的一块内存空间，由于不受管控所以不存在GC的开销；另一方面由于并非JVM native环境，所以并不能识别其中存储的Java对象这样的结构，需要序列化和反序列化来支持。off heap memory的典型应用场景则是缓存一些较大的静态数据。</p><h3><span id="重要的方法">重要的方法</span></h3><h4><span id="compute">compute</span></h4><p><strong>def compute(split: Partition, context: TaskContext): Iterator[T]</strong><br>根据给定的partition计算一个interator，可以遍历该partition下的所有记录。有意思的是partition的名字为split，与mapreduce下mapper的处理单位名字一样。</p><h3><span id="rdd中的基础transformation">RDD中的基础transformation</span></h3><h4><span id="map">map</span></h4><p><strong>def map[U: ClassTag](f: T =&gt; U): RDD[U]</strong><br>返回的RDD为MapPartitionsRDD类型，其compute方法会对其父RDD中的记录执行f映射。</p><h4><span id="mappartitions">mapPartitions</span></h4><p><strong>def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]</strong><br>与map的区别在于映射f的作用对象是整个partition，而不是一条partition中的记录。在一些初始化代价较高的场景下，mapPartition比map更加合理和高效。</p><blockquote><p>补充材料：<a href="https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=67043332" target="_blank" rel="noopener">why spark’s mapPartitions transformation is faster than map</a>。</p></blockquote><h4><span id="flatmap">flatMap</span></h4><p><strong>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</strong><br>与map类似，仅仅将对iterator的map操作换成flatMap操作。这里f映射的输出类型为TraversableOnce，表示只要能完成单次遍历即可，可以是Traversable或Iterable。</p><h4><span id="filter">filter</span></h4><p><strong>def filter(f: T =&gt; Boolean): RDD[T]</strong><br>与map类似，仅仅将对iterator的map操作换作filter操作。</p><h4><span id="distinct">distinct</span></h4><p><strong>def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</strong><br>首先这个方法存在一个implicit的参数ord，类型为scala.math.Ordering。Ordering中实现了各种基础类型（Int, Long, Double, String等）的比较方法，这意味着如果T是一种基础类型则无须实现自己的比较方法，只需要import scala.math.Ordering即可。</p><p>与前几种transformation最大的不同在于distinct依赖reduce，即它是一种宽依赖操作。其具体实现代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(x =&gt; (x, <span class="literal">null</span>)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br></pre></td></tr></table></figure><p>可见其首先将一条记录映射为一个pair，然后执行reduceByKey的操作。这里reduceByKey方法并非RDD所有，之所以可以调用是因为object RDD里定义了从RDD转换为PairRDDFunctions的implicit方法。这种针对特定情况下的RDD增加操作的抽象方式可以学习。reduceByKey中给出了合并两个value的方式，即把相同的key的alue合并为一个（在此为null），然后根据给定的numPartitions数量进行hash partition。最终结果通过map仅保留key即可。</p><p>与mapreduce一致，这里的合并会发生在本地和reducer处，类似mapreduce中的combiner。在调用reduceByKey后的调用逻辑为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey((x, y) =&gt; x, numPartitions)</span><br><span class="line">combineByKeyWithClassTag(x=&gt;x, (x,y)=&gt;x, (x,y)=&gt;x, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br></pre></td></tr></table></figure><p>在combineByKeyWithClassTag中会根据传入的三个映射分别创建createCombiner、mergeValue和mergeCombiner。其中，createCombiner用于产生合并的初始值；mergeValue用于合并两条记录；mergeCombiner用于将mergeValue得到的结果再次合并。上述三者组成一个Aggregator对象。</p><h4><span id="coalesce">coalesce</span></h4><p><strong>def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null) : RDD[T]</strong><br>连接的作用是重新整理原有的RDD。有两种情况：（1）若shuffle\=\=false，表示一种虚拟的RDD分区变化，此时numPartitions应该比原来的少，否则无意义。注意此时是不会发生真实的IO的；（2）若shuffle\=\=true，表示要做一次真实的shuffle，即会带有真实的数据IO。对于第二种情况，在coalesce方法内部会做一次随机的mapping操作，把每个元素与结果RDD中的partition做一次mapping。在第二种情况下，numPartitions可以比父RDD的分区数量更多。</p><p>虽然前一种情况只是虚拟的分区变化，但究竟把哪些父partition分入同一个子partition是可以考虑locality因素的，CoalescedRDD的balanceSlack参数用来控制locality在分配父partition时起的权重。</p><p>看代码中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// include a shuffle step so that our upstream tasks are still distributed</span><br><span class="line">      new CoalescedRDD(</span><br><span class="line">        new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        new HashPartitioner(numPartitions)),</span><br><span class="line">        numPartitions).values</span><br></pre></td></tr></table></figure></p><p>这段话比较难懂，而实际上是做了几件事：首先，在ShuffledRDD中根据随机生成的key将父RDD各partiton中的数据分散到子RDD的各partiton中；然后，隐式转换为PairRDDFunctions的values方法转换成普通的RDD。</p><h4><span id="sample">sample</span></h4><p><strong>def sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = withScope</strong><br>对当前RDD的每个partition进行一次sample。withReplacement用于控制是否可出现重复sample，fraction控制sample的比例，seed即随机种子。</p><h4><span id="randomsplit">randomSplit</span></h4><p><strong>def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]</strong><br>给定一组weights，例如Array(2.0,3.0,5.0)，将父RDD按这样的比例划分，得到一个子RDD数组。<br>示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.makeRDD(1 to 10,10)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res1: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) </span><br><span class="line"></span><br><span class="line">scala&gt; val randomSplittedRDD = rdd.randomSplit(Array(2.0, 3.0, 5.0))</span><br><span class="line">randomSplittedRDD: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[12] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[13] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[14] at randomSplit at &lt;console&gt;:29)</span><br><span class="line"></span><br><span class="line">scala&gt; randomSplittedRDD.foreach(x =&gt; println(x.collect.mkString(&quot; &quot;)))</span><br><span class="line">9 10</span><br><span class="line">2 4 8</span><br><span class="line">1 3 5 6 7</span><br></pre></td></tr></table></figure></p><p>其内部实现实际上是利用了BernoulliCellSampler完成的，每次把父RDD的某个partition做一次sample得到一个子partition，通过一个MapPartitionsRDD实现从父RDD到子RDD的映射。但由于产生的是一组子RDD，因此每多一个子RDD就需要把父RDD做一次sample。由于每次调用时random seed是在内部保持不变的，所以即使多次sample，也不会导致某个元素被分到不同的子RDD里去。这一点是开始一直想不通的，因为我一直以为只需要sample一遍就能完成整个过程。</p><h4><span id="takesample">takeSample</span></h4><p><strong>def takeSample(withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T]</strong><br>返回指定数量的sample。</p><h4><span id="union同">union（同++）</span></h4><p><strong>def union(other: RDD[T]): RDD[T]</strong><br>获取两个RDD的并集，若一个元素出现多次，并不会通过union操作去重，因此union本身属于窄依赖。根据partitioner的情况，分两种情况处理：（1）如果两个RDD的partitioner都定义了且相同，那两RDD的partition数量一样，得到的并集RDD也有相同数量的partition。在考虑locality时，会按照多数原则处理，即如果大多数属于某个并集partition的父partition都倾向某个locality选择，那么就以此多数为准；（2）如果不满足（1）的情况，则并集RDD的partition数量为两父RDD的数量之和，即简单的合并关系。</p><h4><span id="keyby">keyBy</span></h4><p><strong>def keyBy[K](f: T =&gt; K): RDD[(K, T)]</strong><br>根据映射f抽取原RDD中每条记录的key，使结果RDD中每条记录为一个kv二元组。</p><h4><span id="sortby">sortBy</span></h4><p><strong>def sortBy[K](f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</strong><br>对RDD排序，key由映射f抽取。这个方法的实现比较有趣，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this.keyBy[K](f)  //生成一个基于kv二元组的RDD</span><br><span class="line">        .sortByKey(ascending, numPartitions)  //sortByKey是OrderedRDDFunctions中的方法，由隐式转换rddToOrderedRDDFunctions支持</span><br><span class="line">        .values //排好序的RDD再退化由原来的元素组成，也是隐式转换支持</span><br></pre></td></tr></table></figure></p><p>实现过程经过两次隐式转换，非常有scala的特色，这种隐式转换往往发生在特殊的RDD之上。排序的具体过程参考Shuffle一节。</p><h4><span id="intersection">intersection</span></h4><p><strong>def intersection(other: RDD[T]): RDD[T]</strong><br>计算两个父RDD的交集，得到子RDD，交集元素无重复。实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null))) //map成kv二元组后，隐式转换PairRDDFunctions调用其cogroup方法得到(k, (v1, v2))的结构</span><br><span class="line">        .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;  //把两边都不是空的情况筛选出来</span><br><span class="line">        .keys //退化为普通的RDD</span><br></pre></td></tr></table></figure></p><p>其中cogroup依赖shuffle，所以是宽依赖操作。intersection操作还有一些重载，但基本实现是相同的。</p><h4><span id="glom">glom</span></h4><p><strong>def glom(): RDD[Array[T]]</strong><br>将原来的RDD变成新的RDD，其原有的每个partition变成一个数组。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line"></span><br><span class="line">scala&gt; a.glom.collect</span><br><span class="line">res66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9))</span><br></pre></td></tr></table></figure></p><p>这篇<a href="http://blog.madhukaraphatak.com/glom-in-spark/" target="_blank" rel="noopener">文章</a>把glom的作用讲的非常清楚。其中的例1和例2都是在处理一个数组要比挨个处理每个元素好很多的时候。当然，这消耗的内存要更大（<strong>TODO</strong>: 具体使用情况如何？是否会导致OOM？），是一个折衷。</p><h4><span id="cartesian">cartesian</span></h4><p><strong>def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]</strong><br>生成当前RDD与另一个RDD的笛卡尔积，即列举所有a in this和b in other而组成的(a,b)的集合。生成的新RDD的partition数量等于原两个RDD各自的partition数量的乘积。</p><h4><span id="groupby">groupBy</span></h4><p><strong>def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])]</strong><br>将当前RDD中的元素按f映射的key做group操作，结果RDD可根据传入的partitioner来进行分区。源代码中有如下注释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">* Note: This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]</span><br><span class="line">* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.</span><br><span class="line">*</span><br><span class="line">* Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span><br><span class="line">* key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]].</span><br></pre></td></tr></table></figure></p><p>其中指出当前实现中一个key的所有value会需要保存在内存中，从而可能导致OOM，这可能是combine的过程中必须将所有value保存在内存中有关（推测）。另外，聚合或reduce可以解决大部分问题，而不需要groupBy，依此推测这个操作仅用于一些value较少又不得不获取这个中间结果的场景。</p><p>这篇<a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html" target="_blank" rel="noopener">文章</a>很好的讲述了groupBy引入的内存问题的原因。</p><h4><span id="pipe">pipe</span></h4><p><strong>def pipe(command: String): RDD[String]</strong><br>pipe类似于mapreduce中的streaming，即能通过stdin来把数据发往外部进程，在通过stdout把结果读回来。这篇<a href="http://blog.madhukaraphatak.com/pipe-in-spark/" target="_blank" rel="noopener">文章</a>讲的非常清楚。但是这似乎只是map的过程，并不能包括reduce。</p><p>其内部实现实际上就是把参数中包含的command启动一个进程，然后通过stdin/out来完成上述算子操作过程。</p><h4><span id="zip">zip</span></h4><p><strong>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]</strong><br>将当前RDD与other组合成一个新的包含二元组的RDD，要求两个RDD包含相同数量的partition，且每对partition包含相同数量的元素。</p><h4><span id="zippartitions">zipPartitions</span></h4><p><strong>def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V]</strong><br>与zip的关系类似map与mapPartitions的关系，但又不完全一样。zip要求对应的partition里包含的元素数量也完全一样，但这里f映射并不需要两个partiton里元素数量相同。但显然可以利用zipPartitions来实现zip的功能，且与zip比较起来应该有更好的效率。</p><h4><span id="subtract">subtract</span></h4><p><strong>def subtract(other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]</strong><br>得到在当前RDD中且不在other中的元素组成的RDD，由于需要按元素做key，属于宽依赖。</p><h4><span id="dataframerepartion-vs-dataframewriterpartitionby">DataFrame.repartion vs. DataFrameWriter.partitionBy</span></h4><p><strong>def repartition(numPartitions: Int, partitionExprs: Column*): DataFrame</strong><br><strong>def partitionBy(colNames: String*): DataFrameWriter</strong><br>这里的<a href="https://stackoverflow.com/questions/40416357/spark-sql-difference-between-df-repartition-and-dataframewriter-partitionby" target="_blank" rel="noopener">讨论</a>非常清楚。repartition的参数是numPartitions和partitionExprs，partitionExprs将指定的列做hash后对numPartitions求模，得到对应的partition的index。这样得到的最终分区数量是numPartitions，但实际上如果numPartitons大于分组数量，可能有一些partition是空的；反之，如果numPartitions小于分组数量，有一些partiton里包含多个分组。partitionBy是把每个partition按照指定的列拆分为一到多个文件。</p><p>一个应用实力：如果希望输出的文件里，每个文件有且仅有一个分组，那么就可以dataframe.repartiton(n, columns).write.partitionBy(columns).csv(xxx)。其中n可以控制并发的数量，跟实际的数据分布有关。</p><h4><span id="zipwithuniqueid">zipWithUniqueId</span></h4><p><strong>def zipWithUniqueId(): RDD[(T, Long)]</strong><br>为了解决zipWithIndex带来的性能问题，这里放松了条件，只要求id是唯一的。zipWithUniqueId只是个算子，第k个partition的元素对应的id分别为k, k+n, k+2n, …，这里的n是partition的数量。</p><h3><span id="rdd中的actions">RDD中的actions</span></h3><h4><span id="foreach">foreach</span></h4><p><strong>def foreach(f: T =&gt; Unit): Unit</strong><br>将映射f应用到每个元素上。</p><h4><span id="foreachpartition">foreachPartition</span></h4><p><strong>def foreachPartition(f: Iterator[T] =&gt; Unit): Unit</strong><br>将映射f应用到每个partition上。</p><h4><span id="collect">collect</span></h4><p><strong>def collect(): Array[T]</strong><br>将RDD中所有元素作为一个数组返回。<strong>注意不要将collect作用于一个过大的RDD，否则会抛出内存异常，可先利用take和takeSample只取一个子集</strong>。</p><h4><span id="reduce">reduce</span></h4><p><strong>def reduce(f: (T, T) =&gt; T): T</strong><br>执行映射f对应的reduce操作。其操作基本步骤是：（1）每个partition执行f映射对应的reduce过程；（2）在driver的host机器上执行基于f映射的reduce过程，输入来自各个partition的输出。步骤（2）的复杂度与partition的数量呈线性增加。</p><h4><span id="treereduce">treeReduce</span></h4><p><strong>def treeReduce(f: (T, T) =&gt; T, depth: Int = 2): T</strong><br>为了改进reduce里步骤（2）的瓶颈问题，对各partition的输出先逐层聚合，最后再到driver处生成最终结果，类似一棵树的聚合过程。在<a href="https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html" target="_blank" rel="noopener">文章</a>里有详细的描述。reduce和treeReduce的关系类似aggregate和treeAggregate的关系。</p><h4><span id="fold">fold</span></h4><p><strong>def fold(zeroValue: T)(op: (T, T) =&gt; T): T</strong><br>将映射op应用到每对元素上面。在实现过程中，spark不限定元素之间的执行顺序，实际上是先在partition内部做，然后再在partition之间，所以不能保证一个预先设定好的顺序来执行。因此，fold算子适用于那种不需要考虑左右操作元素的顺序，例如max。</p><h4><span id="aggregate">aggregate</span></h4><p><strong>def aggregate<a href="zeroValue: U" target="_blank" rel="noopener">U: ClassTag\</a>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</strong><br>与fold的不同在于aggregate可以返回一个新的类型U，而不是原来的类型Ｔ。从定义的角度，fold是aggregate的一种特例。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; a.fold(0)&#123; _ + _ &#125;</span><br><span class="line">res0: Int = 45</span><br><span class="line"></span><br><span class="line">scala&gt; a.aggregate(0) ( _ + _, _ + _ )</span><br><span class="line">res1: Int = 45</span><br></pre></td></tr></table></figure><h4><span id="treeaggregate">treeAggregate</span></h4><p><strong>def treeAggregate<a href="zeroValue: U" target="_blank" rel="noopener">U: ClassTag</a>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U, depth: Int = 2): U</strong><br>aggregate与treeAggregate和reduce与treeReduce的关系类似。</p><h4><span id="count">count</span></h4><p><strong>def count(): Long</strong><br>计算整个RDD中元素的个数。</p><h4><span id="countapprox">countApprox</span></h4><p><strong>countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]</strong></p><p>在给定timeout期限的情况下，返回RDD中元素个数的估计。其中confidence是认为评估结果符合高斯分布的假设条件下估算的置信度，而<strong>不是结果的可信度</strong>。其核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">override def currentResult(): BoundedDouble = &#123;</span><br><span class="line">    if (outputsMerged == totalOutputs) &#123;</span><br><span class="line">      new BoundedDouble(sum, 1.0, sum, sum)</span><br><span class="line">    &#125; else if (outputsMerged == 0) &#123;</span><br><span class="line">      new BoundedDouble(0, 0.0, Double.NegativeInfinity, Double.PositiveInfinity)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      val p = outputsMerged.toDouble / totalOutputs</span><br><span class="line">      val mean = (sum + 1 - p) / p</span><br><span class="line">      val variance = (sum + 1) * (1 - p) / (p * p)</span><br><span class="line">      val stdev = math.sqrt(variance)</span><br><span class="line">      val confFactor = new NormalDistribution().</span><br><span class="line">        inverseCumulativeProbability(1 - (1 - confidence) / 2)</span><br><span class="line">      val low = mean - confFactor * stdev</span><br><span class="line">      val high = mean + confFactor * stdev</span><br><span class="line">      new BoundedDouble(mean, confidence, low, high)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>其中totalOutputs是partition的个数。上面代码的逻辑是：如果已经计算了所有partition，则返回的结果是100%准确的；如果一个partition都未完成，那么结果完全不可信；否则，按比例计算mean，variance跟已返回比例有关，越多则variance越小，其low/high都是根据confidence和mean算出来的。</p><h4><span id="countbyvalue">countByValue</span></h4><p><strong>def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long]</strong><br>实际上就是一个map + reduce的过程，而所得结果因为需要转化为Map，需要把所得内容完全载入driver的内存，所以只适合不同的value的数量比较小的情况。</p><h4><span id="countbyvalueapprox">countByValueApprox</span></h4><p><strong>def countByValueApprox(timeout: Long, confidence: Double = 0.95)(implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]]</strong><br>与前面提到的countApprox实现类似。</p><h4><span id="zipwithindex">zipWithIndex</span></h4><p><strong>def zipWithIndex(): RDD[(T, Long)]</strong><br>获得一个新的RDD，其中每个元素都是一个二元组，其中value是元素所在RDD中的全局index。该操作不保证重复时index的顺序不变。这个操作表面上是一个算子，但实际上会触发一个spark job，因为在执行之前需要知道每个partition的起始index，而这只能通过count每个partition来得到。</p><h4><span id="take">take</span></h4><p><strong>def take(num: Int): Array[T]</strong><br>take的作用是从一个RDD中获取给定数量num个数的元素，得到一个数组。实现的基本思路是，首先尝试读一个partition，然后根据得到的元素数量与num的比较决定是否需要再探索其它的partition，以及探索的partition数量。这个探索数量的策略似乎比较heuristic，大体上是每次探索的partition数量小于等于已探索的4倍，而具体的值跟已探索到的元素数量与num的关系来定。从实现上看，take返回的所有元素都保存在一个数组内，所以如果num数量过大会引起内存问题。</p><h4><span id="takeordered">takeOrdered</span></h4><p><strong>def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</strong><br>takeOrdered除了获取num个元素外，还要求这些元素按照ord给出的排序方式排序。其实现的核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val mapRDDs = mapPartitions &#123; items =&gt;</span><br><span class="line">        // Priority keeps the largest elements, so let&apos;s reverse the ordering.</span><br><span class="line">        val queue = new BoundedPriorityQueue[T](num)(ord.reverse)</span><br><span class="line">        queue ++= util.collection.Utils.takeOrdered(items, num)(ord)</span><br><span class="line">        Iterator.single(queue)</span><br><span class="line">      &#125;</span><br><span class="line">      if (mapRDDs.partitions.length == 0) &#123;</span><br><span class="line">        Array.empty</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        mapRDDs.reduce &#123; (queue1, queue2) =&gt;</span><br><span class="line">          queue1 ++= queue2</span><br><span class="line">          queue1</span><br><span class="line">        &#125;.toArray.sorted(ord)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p>首先，对每个partition需要得到一个BoundedPriorityQueue，其大小固定为num。若partition内元素少于num个，则queue不满。随后，在一个reduce中，把每个partition得到的queue拼接为一个queue。BoundedPriorityQueue的拼接会按照每个元素插入队列。根据这个实现，每次takeOrdered或top操作都需要对所有partition排序，然后在结果里拼出一个大小为num的队列，代价是比较大的。</p><h3><span id="常见的rdd派生类">常见的RDD派生类</span></h3><h2><span id="spark-architecture">Spark Architecture</span></h2><blockquote><p><a href="http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/" target="_blank" rel="noopener">http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/</a></p></blockquote><h2><span id="shuffle">Shuffle</span></h2><p>Shuffle的目的是把key相同的记录发送到相同的parition以供后续处理。Mapreduce中同样存在shuffle阶段。回顾mapreduce中shuffle的过程：（1）mapper将数据分为多个partition，然后parition内按照key排序（实际分两步完成），这些partition一部分写入磁盘，一部分缓存在内存里；（2）mapper输出的partition分发到对应的reducer；（3）reducer对已经排好续的记录再次进行合并排序；（4）key相同的记录被group为一个iterable交给reduce方法处理。</p><blockquote><p>补充材料：《Hadoop: The Definitive Guide》英文版，197页</p></blockquote><h3><span id="shuffle的两种方法">Shuffle的两种方法</span></h3><p>Spark中shuffle“目前”有两种实现，分别是基于hash和sort。</p><p>基于hash的方式在spark 1.2.0以前是默认的方式。其实现思路非常简单，对于任意输入RDD中的partition，根据hash结果产生N个文件。N表示“reducer”的数量。由于没有排序，每条记录经过hash后直接写入文件，因此速度较快。对于后续处理不需要排序的情况，基于hash的shuffle性能较好。其缺陷是产生的文件数量较大。</p><p>基于sort的方式达到的效果与mapreduce里的shuffle一样，但实现上有较大的差异。首先，从“mapper”写出的数据是不做本地排序的，只有在“reducer”从远端获取数据时才会触发排序过程。这里需要了解spark中的AppendOnlyMap的数据结构。简单来说，在数据量足够小的情况下，“mapper”输出的数据会保存在内存一个AppendOnlyMap中。如果数据较多，则会将AppendOnlyMap变换为一个priority queue，按key排序后保存到外部文件中。这样一来，一次map操作的所有数据会保存在一个内存里的AppendOnlyMap加若干外部的文件。当“reducer”请求数据的时候，这些数据分片会被组织成一个最小堆，每次读取一个key最小的记录，从而实现了排序的功能。“Reducer“拿到各个数据分片后，采用TimSort来对所有数据排序，而不是mapreduce中的合并排序。</p><blockquote><p>补充材料：<br><a href="https://0x0fff.com/spark-architecture-shuffle/" target="_blank" rel="noopener">Spark Architecture: Shuffle</a><br><a href="http://dataknocker.github.io/2014/07/23/spark-appendonlymap/" target="_blank" rel="noopener">spark的外排:AppendOnlyMap与ExternalAppendOnlyMap</a></p></blockquote><h2><span id="block-manager">Block Manager</span></h2><p>Block Manager在spark中作为一层存储抽象层存在。RDD的iterator方法里有读取缓存的partition的入口getOrCompute，其中block的id定义为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val key = RDDBlockId(rdd.id, partition.index)</span><br><span class="line"></span><br><span class="line">case class RDDBlockId(rddId: Int, splitIndex: Int) extends BlockId &#123;</span><br><span class="line">  override def name: String = &quot;rdd_&quot; + rddId + &quot;_&quot; + splitIndex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从实现上看每个RDD的partition都有一个唯一的key，用于blockmanager存储的键值。一个partition应该与一个block一一对应的。Block的存储完全由block manager来管理。</p><p><a href="https://issues.apache.org/jira/browse/SPARK-6235" target="_blank" rel="noopener">关于block size不能超过2g限制的issue tracker</a></p><p>不错的参考资料<br><a href="http://jerryshao.me/architecture/2013/10/08/spark-storage-module-analysis/" target="_blank" rel="noopener">Spark源码分析之-Storage模块</a><br><a href="http://cholerae.com/2015/03/06/Spark%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">Spark缓存机制分析</a><br><a href="http://www.cnblogs.com/hseagle/p/3673138.html" target="_blank" rel="noopener">Apache Spark源码走读之6 – 存储子系统分析</a></p><h2><span id="dag">DAG</span></h2><p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-dagscheduler.html" target="_blank" rel="noopener">DAGScheduler</a><br><a href="https://issues.apache.org/jira/browse/SPARK-9850" target="_blank" rel="noopener">Adaptive execution in Spark</a></p><h2><span id="dataframe">DataFrame</span></h2><p>可以反复学习的blog<br><a href="http://dataknocker.github.io" target="_blank" rel="noopener">http://dataknocker.github.io</a></p>]]></content>
    
    <summary type="html">
    
      My notes taken for learning spark.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="spark" scheme="liqul.github.io/blog/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Two-phase Commit</title>
    <link href="liqul.github.io/blog/two-phase-commit/"/>
    <id>liqul.github.io/blog/two-phase-commit/</id>
    <published>2017-06-09T06:32:00.000Z</published>
    <updated>2018-03-06T10:52:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>I recently came across a good description of two-phase commit from actordb’s document. I decide to borrow it as a note. The following is copied from <a href="http://www.actordb.com/docs-howitworks.html#h_323" target="_blank" rel="noopener">actordb’s document</a>:</p><p>3.2.3 Multi-actor transactions<br>Multi-actor transactions need to be ACID compliant. They are executed by a transaction manager. The manager is itself an actor. It has name and a transaction number that is incremented for every transaction.</p><p>Sequence of events from the transaction manager point of view:</p><ol><li>Start transaction by writing the number and state (uncommitted) to transaction table of transaction manager actor.</li><li>Go through all actors in the transaction and execute their belonging SQL to check if it can execute, but do not commit it. If actor successfully executes SQL it will lock itself (queue all reads and writes).</li><li>All actors returned success. Change state in transaction table for transaction to committed.</li><li>Inform all actors that they should commit.</li></ol><p>Sequence of events from an actors point of view:</p><ol><li>Actor receives SQL with a transaction ID, transaction number and which node transaction manager belongs to.</li><li>Store the actual SQL statement with transaction info to a transaction table (not execute it).</li><li>Once it is stored, the SQL will be executed but not committed. If there was no error, return success.</li><li>Actor waits for confirm or abort from transaction manager. It will also periodically check back with the transaction manager in case the node where it was running from went down and confirmation message is lost.</li><li>Once it has a confirmation or abort message it executes it and unlocks itself.</li></ol><p>Problem scenarios:</p><ol><li>Node where transaction manager lives goes down before committing transaction: Actors will be checking back to see what state a transaction is in. If transaction manager actor resumes on another node and sees an uncommitted transaction, it will mark it as aborted. Actors will in turn abort the transaction as well.</li><li>Node where transaction manager lives goes down after committing transaction to local state, but before informing actors that transaction was confirmed. Actors checking back will detect a confirmed transaction and commit it.</li><li>Node where one or more actors live goes down after confirming that they can execute transaction. The actual SQL statements are stored in their databases. The next time actors start up, they will notice that transaction. Check back with the transaction manager and either commit or abort it.</li></ol>]]></content>
    
    <summary type="html">
    
      I recently came across a good description of two-phase commit from actordb&#39;s document. I decide to borrow it as a note. The following is copied from actordb&#39;s document
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="transaction" scheme="liqul.github.io/blog/tags/transaction/"/>
    
  </entry>
  
  <entry>
    <title>摄影笔记</title>
    <link href="liqul.github.io/blog/notes-on-photography/"/>
    <id>liqul.github.io/blog/notes-on-photography/</id>
    <published>2017-05-04T01:58:00.000Z</published>
    <updated>2018-03-06T10:52:25.000Z</updated>
    
    <content type="html"><![CDATA[<h3><span id="焦段选择的一些感想">焦段选择的一些感想：</span></h3><p><strong>广角（&lt;35mm)</strong></p><ul><li>场面干净：由于广角会摄入较广的场景，所以必须保证其中不要有不希望被包括的主体</li><li>中心突出：没有中心的广角构图是非常失败的，这比其它焦段更加要求中心突出</li><li>线条整齐对称：没有细密整齐的线条，广角会非常乏味，这些线条可以是建筑、地面的纹路、天际线等等</li><li>身临其境：广角照片给人的印象是身历其境，所以角度一般不能太平庸，要么居高临下，要么自底向上</li><li>多元素：元素可以多一点但最好是能够相互呼应的</li></ul><p><strong>中焦(35mm~70mm)</strong></p><ul><li>现实感：由于其呈现的效果更加接近人眼所以能给人一种“旁观”的感觉，更加适合拍摄纪实的题材，其带来的震撼感要高于其它焦段</li><li>距离变化：在这个焦段范围中，一点点变化都能对拍摄距离产生较大影响</li></ul><p><strong>长焦(&gt;70mm)</strong></p><ul><li>微距：把较远处的主体拍到眼前是长焦的主要作用之一</li><li>压缩场景：由于长焦会把多个主体间的距离弱化，很像中国画的感觉，体现的是一种平面的美感</li><li>少元素：元素尽量少一点，画面简单一点，弱水三千只取一瓢</li><li>虚化加成：由于长焦带来的虚化加成，在稍微大一点的光圈下能达到所谓“空气切割”的感觉</li></ul><h3><span id="场景-vs-焦段">场景 vs. 焦段</span></h3><ul><li>苏州园林：原本是为了人眼优化的布景，更加适合中焦和长焦</li><li>城市建筑：广角更能呈现出震撼的感觉，加上建筑的线条在广角中更具有表现力；一些广场上的建筑由于没有遮挡，在没人的时候也可以用长一点的焦段</li><li>人像：跟场景有关，在场景杂乱的地方就老实用中长焦大光圈虚化；在户外视场景而定广角可以突出人与宏达场景的相映，中焦更接近生活，长焦可以捕捉一些在无干扰情况下的活动，总而言之还是跟背景有关系</li></ul><h3><span id="一些原则">一些原则</span></h3><ul><li>色彩尽量少一点，不要给人一种杂乱的感觉</li><li>一定要有主体，不然没有着眼点</li><li>场景中的元素除非必要尽量不要包括进来</li></ul>]]></content>
    
    <summary type="html">
    
      My naive thoughts on photography...
    
    </summary>
    
      <category term="Photography" scheme="liqul.github.io/blog/categories/Photography/"/>
    
    
      <category term="photography" scheme="liqul.github.io/blog/tags/photography/"/>
    
  </entry>
  
  <entry>
    <title>Setup SBT Development Environment</title>
    <link href="liqul.github.io/blog/setup-sbt-development-environment/"/>
    <id>liqul.github.io/blog/setup-sbt-development-environment/</id>
    <published>2017-04-10T02:52:39.000Z</published>
    <updated>2018-03-06T10:52:05.000Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>Setup JDK following Oracle guidance.</p></li><li><p>Setup SBT</p></li></ol><p>No matter which platform you are on. I recommend downloading the <a href="https://dl.bintray.com/sbt/native-packages/sbt/0.13.15/sbt-0.13.15.zip" target="_blank" rel="noopener">zip</a> archive directly.</p><p>Put the following into <code>~/.sbt/repositories</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line"><span class="comment">#local</span></span><br><span class="line">public: http://maven.aliyun.com/nexus/content/groups/public/</span><br><span class="line">typesafe:http://dl.bintray.com/typesafe/ivy-releases/ , [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[<span class="built_in">type</span>]s/[artifact](-[classifier]).[ext], bootOnly</span><br><span class="line">ivy-sbt-plugin:http://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[<span class="built_in">type</span>]s/[artifact](-[classifier]).[ext]</span><br><span class="line">sonatype-oss-releases</span><br><span class="line"></span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure><p>Run <code>sbt</code> and <code>sbt console</code>. If you see all downloads from aliyun, you’ve setup it successfully. Test creating a new SBT project in intellij to see if everything ok.</p>]]></content>
    
    <summary type="html">
    
      Deploying SBT is not easy...
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="sbt" scheme="liqul.github.io/blog/tags/sbt/"/>
    
      <category term="scala" scheme="liqul.github.io/blog/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Implementing A Lock System</title>
    <link href="liqul.github.io/blog/implementing-a-lock-system/"/>
    <id>liqul.github.io/blog/implementing-a-lock-system/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-03-06T10:51:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently, I got the job of building a locking utility. I started by learning from existing locking services, i.e., zookeeper and mysql, since they already have been widely accepted. Zookeeper has this <a href="https://zookeeper.apache.org/doc/r3.1.2/recipes.html" target="_blank" rel="noopener">page</a> introducing how to build locks. I also find this <a href="https://blogs.oracle.com/mysqlinnodb/entry/introduction_to_transaction_locks_in" target="_blank" rel="noopener">page</a> details how lock implemented for mysql innodb table rows. After reading these materials, as well as some others. I found a locking utility could be simple, and also could be quite complicated. So I decided to make a note here to summarize my understandings. </p><p>A lock is essentially a flag claiming the right of a deterministic resource. In mysql, the locking of a range is mapped to a set of data pages, which is thus deterministic as well. The simplest form of lock is a single write lock of one resource, e.g., a file. One could implement this based on zookeeper by creating a node with a certain name. For example, if I want to lock the file “/home/liqul/file1”, I’d put a node with the same name of the file into zookeeper. If the creation returns with success, I got the lock for the file. In contrast, if I failed in creating the node, it means the lock of the file is already held by someone else. If I cannot obtain the lock, I may backoff for a while and retry afterwards. The one holding the lock should release it explicitly. Otherwise, the file can never be used by others any more. </p><p>I note that this is much simpler compared with the lock recipe in this <a href="https://zookeeper.apache.org/doc/r3.1.2/recipes.html" target="_blank" rel="noopener">page</a>. The zookeeper lock recipe brings two useful features: blocking and session-aware. Blocking means that if I failed to obtain the lock now, I will wait until the lock being released by all peers queuing before me. Session-aware means that if I crash, my application for the lock also disappears. The latter is useful to avoid forgetting to release the lock as discussed in the previous paragraph. To implement these two features, one should setup a client-server architecture. Also, to achieve blocking lock, one need a queue for each resource. Sometimes, we however prefer non-blocking locks, which is not discussed in the zookeeper recipe. </p><p>Read-write lock is an extension of the write-only lock. Zookeeper also has a dedicated recipe for it <a href="https://zookeeper.apache.org/doc/r3.1.2/recipes.html" target="_blank" rel="noopener">here</a>. Implementing a read-write lock is non-trivial. Let’s explain this problem with a concrete example. Suppose <em>A</em> wants to acquire a read lock of a file <em>F</em>, while <em>B</em> wants a write lock to <em>F</em> too. A naive implementation may be as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1 *A* checks if there&apos;s any write locks to *F*</span><br><span class="line">2 *B* checks if there&apos;s any read locks to *F*</span><br><span class="line">3 *A* finds no write locks to *F*</span><br><span class="line">4 *B* finds no read locks to *F*</span><br><span class="line">5 *A* put a read lock of *F*</span><br><span class="line">6 *B* put a write lock of *F*</span><br></pre></td></tr></table></figure><p>The “check and then lock” pattern simply doesn’t work correctly. In zookeeper, they rely on sequential nodes and watcher to work around this problem, where each peer always first inserts a node and then check if itself obtains the lock. If not, the peer put a watcher onto the current holder of the lock. Another workaround is to first obtain a global write pre-lock before any operation. With a pre-lock the above procedure becomes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1 *A* acquires the pre-lock</span><br><span class="line">2 *A* obtains the pre-lock</span><br><span class="line">3 *A* checks if there&apos;s any write locks to *F*</span><br><span class="line">4 *B* acquires the pre-lock</span><br><span class="line">5 *B* failed to obtain the pre-lock</span><br><span class="line">6 *A* finds no write locks to *F*</span><br><span class="line">7 *A* put a read lock of *F*</span><br><span class="line">8 *A* release the pre-lock</span><br></pre></td></tr></table></figure><p>One obvious drawback of the second workaround is that even two locks have no conflict, they still need to perform one after another. To avoid this problem, the lock utility need to maintain a conflict matrix for each pair of locks being processed or pending (should be marked separately). If a pending lock is not conflicting with any lock processed, it obtains the pre-lock right away. Otherwise, it is put into a queue waiting for all conflicting locks being clear. A short version is to only consider read and write locks separately. </p><p>Another extension is to achieve atomicity for a set of locks. For this purpose, one need to treat the whole set as “one” lock. And the handling of it is quite similar with what we have discussed above. For instance, if you want to implement with zookeeper, you may need to insert all the locks and then set watchers for a set of conflicting locks. Only after all conflicting locks being clear, you obtain the locks. Without zookeeper, one can also use the pre-lock solution as described above. A conflict matrix is necessary to avoid deadlock if you want to process the sets of locks in parallel.</p><p>In general, zookeeper is quite ready for customizing into your own locking service. However, it does has its own drawbacks. For example, it is not clear how to implement non-blocking read-write locks. If you have metadata for your locks, and you want to search in the metadata, zookeeper may be painful. At this time, using a mysql database may be a good choice, though you need to avoid some pitfalls discussed in this article.</p>]]></content>
    
    <summary type="html">
    
      Recently, I got the job of building a locking utility.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="lock" scheme="liqul.github.io/blog/tags/lock/"/>
    
  </entry>
  
  <entry>
    <title>Notes on The Raft Consensus Algorithm</title>
    <link href="liqul.github.io/blog/the-raft-consensus-algorithm/"/>
    <id>liqul.github.io/blog/the-raft-consensus-algorithm/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-03-15T02:34:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>What’s consensus?</p><blockquote><p>It allows a collection of machines to work as a coherent group that can survive the failures of some of its members.</p></blockquote><p>It means not only a group of machines reach a final decision for a request, but also the state machine is replicated across these machines, so that some failures do not affect the functioning. Raft is a consensus algorithm seeking to be correct, implementable, and understandable. </p><p>The <a href="https://ramcloud.stanford.edu/~ongaro/thesis.pdf" target="_blank" rel="noopener">thesis</a> is very well written. It is much more comprehensive compared to the NSDI paper. Implementing Raft based on the thesis shouldn’t be too difficult (of course, also not trivial). The author also built a <a href="https://raft.github.io/" target="_blank" rel="noopener">website</a> putting all kinds of helping things there. I read the paper and decide to take some notes here.</p><p><img src="/blog/assets/state.png" width="800"></p><p>There are two key parts sitting in the core of the algorithm:</p><p><strong>Leader election</strong></p><p>The election is triggered by a timeout. If a server failed to detect heartbeats from the current leader, it start a new <em>term</em> of election. During the term, it broadcast requests to collect votes from other servers. If equal or more than majority of servers reply with a vote, the server becomes the leader of this term. The “term” here is a monotonically increasing logic time. From the perspective of a server receiving the vote request, it decides whether to give the vote based on a few considerations. First of all, if the sender even falls behind the receiver in terms of log index, the receiver should not vote for it. Also, if the receiver can still hear the heartbeats from current leader, it should not vote too. In this case, the requester might be a <em>disruptive server</em>. In other cases, the receiver should vote for the sender. </p><p><img src="/blog/assets/leader election.png" width="800"></p><p><strong>Log replication</strong></p><p>Once a server becomes the leader, it’s mission is simply replicate it’s log to every other follower. The replication means make the log of a follower <em>exactly</em> the same as the leader. For each pair of leader and follower, the leader first identify the highest index where they reach an agreement. Starting from there, the leader overwrite its log to the follower. The leader handles all requests from clients. Once it receives a new request, it first put the request into its own log. Then, it replicate the request to all followers. If equal or more than majority followers (including the leader itself) answer the replication request with success, the leader apply the request into its state machine (this is called commit). The leader put the new log index into its heartbeats, so followers know if the request has been committed, after which each follower commit the request too.</p><p><img src="/blog/assets/log replication.png" width="800"></p><p>More formal introduction of the core Raft could be found in Fig. 3.1 in the thesis paper. There are also a few extensions to make the algorithm practical to be used in production systems, such as the group management. I also found Fig. 10.1 a very good reference of architecture. </p><p>There are quite a lot of implementations of Raft, which could be found <a href="https://raft.github.io/" target="_blank" rel="noopener">here</a>. I also find a project named Copycat, with code <a href="https://github.com/atomix/copycat" target="_blank" rel="noopener">here</a> and document <a href="http://atomix.io/copycat/" target="_blank" rel="noopener">here</a>. Copycat is a full featured implementation of Raft in java. Building your own application based on Copycat shouldn’t be too difficult. They provide an example of implementing a KV store based on Copycat in their source code <a href="https://github.com/atomix/copycat/tree/master/examples" target="_blank" rel="noopener">here</a>, which is used as the “<a href="http://atomix.io/copycat/docs/getting-started/" target="_blank" rel="noopener">Get Started</a>“ tutorial. Another very important reason, why I think Copycat a good reference, is that it emphases the abstraction of state machine, client, server, and operations. Therefore, going through it’s document enhanced my understanding of Raft. </p><p>If you don’t want to build your own Raft, may be Copycat is worthwhile a try, though I haven’t any real experience beyond a toy project.</p><p>The annotated thesis could be found <a href="/blog/assets/raft-thesis.pdf">here</a>.</p><p><strong>A go-through case for understanding</strong></p><p>A typical request handling process is as follows:</p><ol><li>The client sends a request to the cluster;</li><li>The leader handles the request by putting it to a WAL;</li><li>The leader sends the request to all followers;</li><li>Each follower puts the received request to its WAL, and responds to the leader;</li><li>Once the leader has heard a majority number of responses from its followers, the leader commit the request by applying the WAL to its state machine;</li><li>The leader inform the client that the request has been handled properly, and then, put the index of the request into its heartbeat to let all followers know the status of each request;</li><li>Once the follower knows that the request has been committed by the leader, the follower commit the request too by applying it to its own state machine. </li></ol><p>There are a few key points to understand in the process above:</p><p>1.Does the client always know if its request has been handled properly?</p><p>No. If the leader commits the request and then crashes, the client will not know if the request has been actually successfully handled. In some cases, the client will resend the request which may lead to duplicated data. It leaves for the client to avoid such kind of duplication. </p><p>2.How about the leader crashes before inform its followers that the request has been committed?</p><p>If the leader crashes, a follower will be elected to be the next leader. The follower must have the latest state according to the mechanism of Raft. Therefore, the next leader definitely has the WAL for the request, and the request has definitely been replicated across a majority number of hosts. Therefore, it is safe to replicate its state to all followers. </p><p>3.Key feature of a consensus algorithm (or strong consistency)?</p><p>Under normal situations, if there’s a state change, the key step changing the state should be always handled by a certain node. The state changing should be replicated to a majority number of followers before informing the requester a success. Each read request goes to that certain node as well. Once there’s node failures or networking partitions, the service stop working until returning to the normal situation again.</p>]]></content>
    
    <summary type="html">
    
      It allows a collection of machines to work as a coherent group that can survive the failures of some of its members
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="Raft" scheme="liqul.github.io/blog/tags/Raft/"/>
    
      <category term="Consensus" scheme="liqul.github.io/blog/tags/Consensus/"/>
    
      <category term="Copycat" scheme="liqul.github.io/blog/tags/Copycat/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Using &quot;Select ... For Update&quot; for Uniqueness in Mysql</title>
    <link href="liqul.github.io/blog/using-select-for-update-for-uniqueness-in-mysql/"/>
    <id>liqul.github.io/blog/using-select-for-update-for-uniqueness-in-mysql/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-03-06T10:51:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>I encountered a deadlock recently. Similar questions have been asked on StackOverflow, e.g., <a href="http://stackoverflow.com/questions/21851119/deadlock-using-select-for-update-in-mysql" target="_blank" rel="noopener">this</a> and <a href="http://stackoverflow.com/questions/43251975/mysql-select-for-update-blocks-1st-insert-if-using-non-primary-key-in-where-clau" target="_blank" rel="noopener">this</a>. But the answers didn’t really explain why this happens. </p><p>The situation is quite easy to reproduce @ Mysql 5.7.17 (also tested on other versions in 5.5 or 5.6):</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`test`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`val`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>),</span><br><span class="line">  <span class="keyword">KEY</span> <span class="string">`search`</span> (<span class="string">`val`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">4</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'pre-lock'</span>;</span><br></pre></td></tr></table></figure><p><strong>session1</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session2</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session1</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'/a/b/c'</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session2</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1213 (40001): Deadlock found when trying to get <span class="keyword">lock</span>; try restarting transaction</span><br></pre></td></tr></table></figure></p><p>The result of show engine innodb status:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">LATEST DETECTED DEADLOCK</span><br><span class="line"><span class="comment">------------------------</span></span><br><span class="line">2017-04-06 23:54:03 0x7000057db000</span><br><span class="line">*** (1) TRANSACTION:</span><br><span class="line">TRANSACTION 1333, ACTIVE 18 sec starting index read</span><br><span class="line">mysql tables in <span class="keyword">use</span> <span class="number">1</span>, <span class="keyword">locked</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">LOCK</span> <span class="keyword">WAIT</span> <span class="number">2</span> <span class="keyword">lock</span> <span class="keyword">struct</span>(s), <span class="keyword">heap</span> <span class="keyword">size</span> <span class="number">1136</span>, <span class="number">1</span> <span class="keyword">row</span> <span class="keyword">lock</span>(s)</span><br><span class="line">MySQL <span class="keyword">thread</span> <span class="keyword">id</span> <span class="number">5</span>, OS <span class="keyword">thread</span> handle <span class="number">123145394155520</span>, <span class="keyword">query</span> <span class="keyword">id</span> <span class="number">62</span> localhost root Sending <span class="keyword">data</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span></span><br><span class="line">*** (<span class="number">1</span>) WAITING <span class="keyword">FOR</span> THIS <span class="keyword">LOCK</span> <span class="keyword">TO</span> BE GRANTED:</span><br><span class="line"><span class="built_in">RECORD</span> LOCKS <span class="keyword">space</span> <span class="keyword">id</span> <span class="number">24</span> page <span class="keyword">no</span> <span class="number">4</span> n bits <span class="number">72</span> <span class="keyword">index</span> <span class="keyword">search</span> <span class="keyword">of</span> <span class="keyword">table</span> <span class="string">`test_tnx`</span>.<span class="string">`test`</span> trx <span class="keyword">id</span> <span class="number">1333</span> lock_mode X waiting</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** (2) TRANSACTION:</span><br><span class="line">TRANSACTION 1332, ACTIVE 29 sec inserting</span><br><span class="line">mysql tables in <span class="keyword">use</span> <span class="number">1</span>, <span class="keyword">locked</span> <span class="number">1</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">lock</span> <span class="keyword">struct</span>(s), <span class="keyword">heap</span> <span class="keyword">size</span> <span class="number">1136</span>, <span class="number">4</span> <span class="keyword">row</span> <span class="keyword">lock</span>(s), <span class="keyword">undo</span> <span class="keyword">log</span> entries <span class="number">1</span></span><br><span class="line">MySQL <span class="keyword">thread</span> <span class="keyword">id</span> <span class="number">62</span>, OS <span class="keyword">thread</span> handle <span class="number">123145394434048</span>, <span class="keyword">query</span> <span class="keyword">id</span> <span class="number">63</span> localhost root <span class="keyword">update</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'/a/b/c'</span></span><br><span class="line">*** (<span class="number">2</span>) HOLDS THE <span class="keyword">LOCK</span>(S):</span><br><span class="line"><span class="built_in">RECORD</span> LOCKS <span class="keyword">space</span> <span class="keyword">id</span> <span class="number">24</span> page <span class="keyword">no</span> <span class="number">4</span> n bits <span class="number">72</span> <span class="keyword">index</span> <span class="keyword">search</span> <span class="keyword">of</span> <span class="keyword">table</span> <span class="string">`test_tnx`</span>.<span class="string">`test`</span> trx <span class="keyword">id</span> <span class="number">1332</span> lock_mode X</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">1</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">1</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 73757072656d756d; asc supremum;;</span><br><span class="line"></span><br><span class="line">Record <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** (2) WAITING FOR THIS LOCK TO BE GRANTED:</span><br><span class="line">RECORD LOCKS space id 24 page no 4 n bits 72 index search of table `test_tnx`.`test` trx id 1332 lock_mode X locks gap before rec <span class="keyword">insert</span> intention waiting</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** WE ROLL BACK TRANSACTION (1)</span><br></pre></td></tr></table></figure><p>My objective is to use <code>select ... for update</code> as a uniqueness check for a following sequence of insertions. I expected that Tnx 2 would wait until Tnx 1 released the lock, and then continue its own business. However, Tnx 2 is rolled back due to deadlock. The innodb status looks quite confusing. Tnx 1 is holding and waiting for the same lock. </p><p>After some research, though I still cannot figure out the root cause, my perception is that the insertion in Tnx 1 acquires a gap lock which is somehow overlapping with the gap lock by the <code>select ... for update</code>. And therefore, this create a deadlock where Tnx 1 waits for Tnx 2 and Tnx 2 waits for Tnx 1. </p><p>During my research, I found that the right <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-next-key-locking.html" target="_blank" rel="noopener">use case</a> for <code>select ... for update</code> is as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> [<span class="keyword">table</span>] <span class="keyword">where</span> [condition] <span class="keyword">for</span> <span class="keyword">update</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> [<span class="keyword">table</span>] <span class="keyword">values</span> [belongs <span class="keyword">to</span> condition];</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> [<span class="keyword">table</span>] <span class="keyword">where</span> [belongs <span class="keyword">to</span> condition];</span><br></pre></td></tr></table></figure><p>The rows being mutated should be explicitly locked by the <code>select ... for update</code>. Also, the condition should be as clear as possible. For example, put only an unique key in the condition. This is to make the gap lock with a simple and clear range, in order not to cause deadlocks.</p><p>Generally, using <code>select ... for update</code> is non-trivial since the underlying locking mechanism seems quite complicated. For my scenario, I got two workarounds:</p><ol><li>Disable gap locks by setting the <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html" target="_blank" rel="noopener">isolation level</a> to <code>READ COMMITTED</code>.</li><li>Apply <code>select ... for update</code> on a row from another table, which avoid possible lock overlap.</li></ol>]]></content>
    
    <summary type="html">
    
      I encountered a deadlock recently. Similar questions have been asked on StackOverflow, e.g., this and this. But the answers didn&#39;t really explain why this happens.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="lock" scheme="liqul.github.io/blog/tags/lock/"/>
    
      <category term="Mysql" scheme="liqul.github.io/blog/tags/Mysql/"/>
    
      <category term="Deadlock" scheme="liqul.github.io/blog/tags/Deadlock/"/>
    
      <category term="Uniqueness" scheme="liqul.github.io/blog/tags/Uniqueness/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Multi-versioned Storage</title>
    <link href="liqul.github.io/blog/non-blocking-read/"/>
    <id>liqul.github.io/blog/non-blocking-read/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-03-06T10:51:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>I recently read the <a href="https://research.google.com/archive/spanner.html" target="_blank" rel="noopener">Spanner</a> paper. I realized that I cannot understand the idea of TrueTime and Non-blocking read well. Therefore, I did some research by googling the concept of non-blocking read, and came across this mysql <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html" target="_blank" rel="noopener">document</a>. After reading it, I realized that my understanding of multi-versioned storage is incorrect. So I decide to put some notes here.</p><p>The key points of multi-versioned storage are:  </p><ol><li>version -&gt; the creation wall clock time of the object (or a vector time)</li><li>timestamp -&gt; the query time of the object</li><li>the association between the metadata and content should never be changed</li></ol><p>Each object is associated with a set of metadata. The metadata contains a timestamp field indicating the version of this object. For a concrete example, let’s define a storage model as follows:</p><p><img src="/blog/assets/objstore1.png" width="500"></p><p>Each metadata contains three fields</p><table><thead><tr><th>Field</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td>name</td><td style="text-align:center">name of the object. objects with the same name are deem to be the same object</td></tr><tr><td>ctime</td><td style="text-align:center">creation time</td></tr><tr><td>deleted</td><td style="text-align:center">1 means a tombstone</td></tr><tr><td>extra</td><td style="text-align:center">some extra metadata for this object</td></tr></tbody></table><p>Let’s use typical operations to clarify the usage of this data model. </p><p><strong>Put</strong><br>Put is adding a new versioned object into the storage. In the figure above, the first “obj1” is inserted at 2017-04-01 11:30:12. Inserting another object with the same name is simply adding a new entry to the table pointing to the new content. When an object is requested from client, only the one with the latest timestamp is returned. Therefore, from 2017-04-01 11:30:12 to 2017-04-02 11:31:25, the first obj1 is visible. After 2017-04-02 11:31:25, the client see only the second obj1. With this data model, there is no need to block writes during reading this object, since each operation is based solely on its timestamp. </p><p><strong>Delete</strong><br>Delete is simply by putting a tombstone for a certain object. Like the third obj1 in the example. No content is presented for this entry. The objects with a tombstone as the latest entry will be filtered out if requested from clients.</p><p><strong>Update</strong><br>In this context, update is different from putting new contents into the object, but altering the object’s metadata (e.g., the extra field in the table). Updating the fields beyond the unique key is less a problem. If we need to update even the name of the object, we need to perform two steps: (1) delete the original object; (2) put a new object with the new metadata. One need to keep these two steps in an atomic transaction.</p><p><strong>Search</strong><br>Search is usually based on metadata to find a set of objects. The difference with multi-versioned storage is that we only return an object with the latest timestamp. This could be achieved with a select clause like </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT t.* FROM (SELECT * FROM table WHERE xxx AND ctime &lt; NOW() ORDER BY ctime DESC) t GROUP BY t.name</span><br></pre></td></tr></table></figure><p>We then filter out tombstones reside in the result set.</p><p><strong>Get</strong><br>Get is first searching for the latest object before the operation time. Then, the pointer of content is handed over to the client. The client should read the content as soon as possible. Otherwise, the content may be deleted due to recycling.  </p><p><strong>Compact</strong><br>Compact refers to merge a set of contents into a big one. This is useful for example for HDFS to relief the name node’s burden of storing a large number of small files. The implementation of compaction is a bit tricky. One first merge the target set of contents into a big file. Then, insert a new entry for each  related object within an atomic transaction. For clarification, let’s suppose we need to compact obj2 and obj3 in our example. We first create a new content with both contents from the original obj2 and obj3. Then, we add new entries for obj2 and obj3, respectively. The tricky part is that the ctime for these two entries are just 1 second larger than the original two. </p><p><img src="/blog/assets/objstore2.png" width="600"></p><p>This may sounds weired at first. But think about the situation that a new entry for obj2 is put after we started but not finished compaction. In this case, the new content could be covered by the compaction if the ctime is larger than the new entry. We add 1 to the ctime. So, the new entry will either be later, or conflict with the compaction, leading to a failure. Upon such failure, the client simply retry to submit the new content. This should not be a real problem since compaction is a rare operation. </p><p>One may wonder why not simply updating the pointers in the original entries for obj2 and obj3. This actually breaks the third key points we mentioned at the beginning. It is important not changing the association. For example, if we want to get an object during compaction, reading may fail since the old contents may be deleted. Also, stale contents may be produced. More importantly, we may need very complicated transaction controls. </p><p><strong>Recycle</strong><br>Recycle is used to delete all deleted objects. The deleted objects could be find by searching for tombstones in the table. If a tombstone is detected, all entries before that can be deleted physically, including the tombstone itself. Delete a single content is straightforward. However, if a content is merged into a big file, we can carry out a similar process like compaction to delete the content physically. Old-versioned entries can also be recycled. Both recycling for deleted and old-versioned entries follows a certain policy. We can delete an entry once it has been out dated for a few days. This duration shouldn’t be too soon. Otherwise, we may recycle content being or to be read. </p><p>One can see that, with this multi-versioned storage model, all operations are much simpler without dependency to a locking system. We even do not rely on transaction, if compaction is not necessary. Let’s return to the three key parts at the beginning of this note. With a timestamp field, we already get the sense of version. As discussed above, the operation timestamp is critical as we need it to determine which object should be put into the result set. If data is stored across multiple machines, we need to synchronize their clock precisely. TrueTime is a API expose the uncertainty of time among different machines, and thus is critical for such large scale storage implementation. Finally, the association should not be broken, otherwise, we need complicated mechanisms to fix the issues it incurs.</p>]]></content>
    
    <summary type="html">
    
      I recently read the Spanner paper. I realized that I cannot understand the idea of TrueTime and Non-blocking read well. Therefore, I did some research by googling the concept of non-blocking read, and came across this mysql document. After reading it, I realized that my understanding of multi-versioned storage is incorrect. So I decide to put some notes here.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="storage" scheme="liqul.github.io/blog/tags/storage/"/>
    
      <category term="MVCC" scheme="liqul.github.io/blog/tags/MVCC/"/>
    
      <category term="non-blocking" scheme="liqul.github.io/blog/tags/non-blocking/"/>
    
  </entry>
  
  <entry>
    <title>An Algorithm Deduplicating File Locks</title>
    <link href="liqul.github.io/blog/an-algorithm-deduplicate-file-locks/"/>
    <id>liqul.github.io/blog/an-algorithm-deduplicate-file-locks/</id>
    <published>2017-03-26T13:02:44.000Z</published>
    <updated>2018-03-06T10:50:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>In one of my recent project, I need to implement a lock service for files. It is quite similar to the lock service of existing file systems. I need to support two modes: write (exclusive) and read (shard). For instance, “R/a/b/c” means a read lock for the file with path “/a/b/c”. Likewise, “W/a/b/c” stands for a write lock. </p><p>The interface is pretty straightforward. The user provide a set of locks, e.g., {R/a/b/c, R/a/b/d}, to be locked by our service. One challenge we face is to deduplicate the locks given by the user. For instance, the user may provide a set of locks {R/a/b/c, W/a/b/c}. In this example, we know the write lock is stronger than the read lock, and therefore, the set is equivalent to {W/a/b/c}. We tend to reduce the number of locks, since it is beneficial for checking less conflicts in the steps afterwards. </p><p>So far, the deduplication problem sounds quite simple, since we only need to consider locks with exactly the same path. However, we also need to support a wildcard notation, because we have a large number of files under one directory. If we want to lock all files in this directory, we need to generate a huge list of locks, each for a file. With a wildcard, we lock all files under “/a/b/“ with “R/a/b/*”. The wildcard makes the deduplication problem much more complicated. </p><p>We first clarify the coverage of locks by a concrete example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W/a/b/* &gt; W/a/b/c &gt; R/a/b/c</span><br></pre></td></tr></table></figure><p>Given a large set of locks, a very simple algorithm is that we compare each pair of locks. If one is stronger than the other, the weaker one is throw away. The result set contains only locks where no one is stronger than other locks. This algorithm’s complexity is O(n*n) which is slow if the given set is large. So, I try to find a faster algorithm. </p><p>After some thoughts, I developed an algorithm with complexity O(n), which constructs a tree on the set of locks. Each tree node has the following attributes:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Node &#123;</span><br><span class="line">    String name; //the name of the node</span><br><span class="line">    String mode; //&quot;W&quot; or &quot;R&quot;</span><br><span class="line">    Node parent; //the parent of the node</span><br><span class="line">    boolean mark = false; //mark=true if there&apos;s at least one W node in its subtree</span><br><span class="line">    Map&lt;String, Node&gt; children = new HashMap&lt;&gt;(); //the children of this node</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We now explain how to construct a lock tree with a concrete example.</p><p><img src="/blog/assets/algorithm.png"></p><p>In the figure above, we show inserting a set of locks into an empty lock tree. Each node is represented with a string “mode:name:mark”. </p><p><em>Step 1~3</em>: Inserting the first three locks {R/a/b/c, R/a/b/d, R/a/e/f} is straightforward.<br><em>Step 4:</em> Then, inserting the 4th lock, i.e., “W/a/b/c”, has two effects: (1) upgrade the original “R” lock of /a/b/c to “W” mode; (2) mark the path of “/a/b/c” from 0 to 1.<br><em>Step 5:</em> The 5th lock “R/a/b/e” is the same with the first three.<br><em>Step 6~7:</em> Then, the 6th lock “R/a/b/*“ contains a wildcard. Having a “R” wildcard means replacing all unmarked nodes (“R:e:0” and “R:d:0”) with a “R:*:0” node, in its parent’s subtree (the parent is “R:b:1” in this case). Similarly, inserting “R/a/*/*“ first removes all unmarked nodes in “R:a:1”‘s subtree, i.e., “R:e:0”, “R:*:0”, and “R:f:0”, and then insert new nodes with wildcard.<br><em>Step 8:</em> Finally, inserting a “W” mode lock with wildcard means deleting all nodes in the subtree of “R:a:1” (the wildcard node’s parent) and then insert the new nodes. </p><p>After constructing the lock tree, each path from root to leaf is a lock in our deduplicated result set. In practice, a hashmap may already solve most duplicated cases. We rarely encounter extreme cases as shown in the example above. The algorithm briefly described above is only for fun :). I didn’t mention paths with different lengths since they could be put into different trees, which is therefore not a real problem. </p><p>I didn’t elaborate all cases in the example above, which is more complicated. A sample implementation of the algorithm could be find <a href="/blog/assets/LockTree.java">here</a>. The output is as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">R/a/b/c</span><br><span class="line">ROOT: abc[R]</span><br><span class="line">R/a/b/d</span><br><span class="line">ROOT: abc[R]</span><br><span class="line">ROOT: abd[R]</span><br><span class="line">R/a/e/f</span><br><span class="line">ROOT: abc[R]</span><br><span class="line">ROOT: abd[R]</span><br><span class="line">ROOT: aef[R]</span><br><span class="line">W/a/b/c</span><br><span class="line">ROOT: abc[W]</span><br><span class="line">ROOT: abd[R]</span><br><span class="line">ROOT: aef[R]</span><br><span class="line">R/a/b/e</span><br><span class="line">ROOT: abc[W]</span><br><span class="line">ROOT: abd[R]</span><br><span class="line">ROOT: abe[R]</span><br><span class="line">ROOT: aef[R]</span><br><span class="line">R/a/b/*</span><br><span class="line">ROOT: abc[W]</span><br><span class="line">ROOT: ab*[R]</span><br><span class="line">ROOT: aef[R]</span><br><span class="line">R/a/*/*</span><br><span class="line">ROOT: abc[W]</span><br><span class="line">ROOT: a**[R]</span><br><span class="line">W/a/*/*</span><br><span class="line">ROOT: a**[W]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      In one of my recent project, I need to implement a lock service for files. It is quite similar to the lock service of existing file systems. 
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="lock" scheme="liqul.github.io/blog/tags/lock/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Designing an Admission Control System</title>
    <link href="liqul.github.io/blog/a-role-based-admission-control-system/"/>
    <id>liqul.github.io/blog/a-role-based-admission-control-system/</id>
    <published>2017-03-07T07:50:03.000Z</published>
    <updated>2018-03-06T10:50:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>An admission control system takes care of two parts: <em>action</em> and <em>resource</em>. As a concrete example, let’s assume you are a sales manager. A typical business is viewing the orders made by all sales man in your group. Here, viewing the orders is the “action” part, while the orders made by your group is the “resource” part.</p><p>Given a user account, an admission control system tells which actions the user could take on which resources. For a website, the action is usually represented with a URI. For instance, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;/orders/view&quot; -&gt; view the orders</span><br><span class="line">&quot;/user/delete&quot; -&gt; delete a user</span><br><span class="line">&quot;/user/update&quot; -&gt; update a user&apos;s profile</span><br></pre></td></tr></table></figure><p>A typical way of managing the actions is through a layered representation of users, roles, and permissions. A user belongs to a role, and a role is composed of a set of permissions. Each permission is linked to a URI for instance. For convenience reasons, people usually add additional layers to the three-layer structure. For example, a “permission group” layer makes it easier for the manager to assign permissions when the number of permissions is too large. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user -&gt; role -&gt; permission</span><br></pre></td></tr></table></figure><p>Representing the resource is nontrivial. Before that, you need domain knowledge to understand the structure of the resource. In our example of sales manager, the resource are orders which could be managed in a tree. The manager can see all orders within her subtree, while a sales man cannot see orders from her siblings. In this case, the resource is put into a tree as follows.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">manager -- sales man 1</span><br><span class="line">        |</span><br><span class="line">        -- sales man 2</span><br><span class="line">        |</span><br><span class="line">        -- sales man 3 -- agent 1</span><br><span class="line">                       |</span><br><span class="line">                       -- agent 2</span><br></pre></td></tr></table></figure><p>You may maintain multiple trees, each for a logic organization. For instance, one tree for sales and another for operators. Each order is linked to a node on the tree. When a action arrives, e.g., “/orders/view/1” where “1” is the order id, we check if the linked node of this order entry in DB. If the node is within the user’s subtree, the admission is granted, otherwise denied. </p><p>Last but not least, be sure to use a white list for the super user. In other words, the super user should <em>not</em> go through the admission control system. Otherwise, when the admission control system is down, you can do nothing at all without permission.</p>]]></content>
    
    <summary type="html">
    
      An admission control system takes care of two parts: *action* and *resource*. 
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="admission control" scheme="liqul.github.io/blog/tags/admission-control/"/>
    
  </entry>
  
  <entry>
    <title>乌合之众</title>
    <link href="liqul.github.io/blog/the-crowd/"/>
    <id>liqul.github.io/blog/the-crowd/</id>
    <published>2017-02-25T14:31:47.000Z</published>
    <updated>2018-03-06T10:50:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>准确的说《乌合之众》是指“The Crowd——A Study of the Popular Mind”，作者是Gustave Le Bon，一位1931年离世的法国作家。我所读的版本是由广西师范大学出版社出版的，由冯克利翻译的版本。有意思的是这本书正文只有32开的183页，而其中1-29页是Robert K. Merton撰写的读后感，所以在阅读正文之前已经有非常详尽的对于书的评论了。</p><p>读后感里对《乌合之众》的评论是非常纠结的，既赞扬了作者深刻的观察力，也批评了作者受其根深蒂固的观念的影响。相信赞扬的成分还是要比批评的成分大得多的，其实一部书里哪怕揭示了一点潜藏的知识，那这部书就已经很了不起了，哪怕附带许多明显的错误。《乌合之众》就是这样，在短短的一百多页里，作者其实只想传达一个观点——“群体的思维和个人是不同的”。更准确的说，是群体的思维是<strong>混乱的、富含想象力的、可传染的、极端的、低俗简单的、扩大化的、矛盾的，跟文明社会里的个人相比，群体就好比是来自野蛮社会的个体。</strong></p><p>《乌合之众》里有非常多关于上面我列举的各个形容词的描述，例如：“打动群体心灵的是神话中的英雄，而不是一时的真实英雄”、“群体感情的一致倾向会立刻变成一个既定事实”、“群体总是落后于博学之士和哲学家好几代人”。尤其经典的是“群体推理的特点，是把彼此不同、只在表面上相似的事物搅在一起，并且立刻把具体的事物普遍化”，比如“受雇主剥削的苦力，立刻便认为天下所有的雇主都是剥削他们的人”。</p><p>这本书两个世纪前就已经完成了，但它所描绘的现象却不断重现着。近代近两百年来中国发生的各项运动，以及参与这些运动的农民、学生、工人、商人等等，无不是《乌合之众》里描绘的角色。他们富有激情，时刻以民族、道德为口号党同伐异，采用最野蛮和暴力的方式。他们看似有无穷的力量，推动社会和历史前进，而实际上他们不过是任人摆布的玩偶，为达到某些目的而布施的棋子。</p><p>可恨的是直到今天这样的场景仍然此起彼伏，人们并没有比一两百年前的先辈更加冷静和明白，而一些野心家还试图利用群体的这种特点，一旦风吹草动人们仍然会像暴民一样无差别摧毁一切（很多时候包括这些活动的制造者）。作为个人，在群体面前显得太势单力薄了，我们无法改变什么，甚至无法保护自己，我们能够做的，仅仅是躲避群体带来的灾难——这种灾难是自然灾难无法比拟的！</p>]]></content>
    
    <summary type="html">
    
      可恨的是直到今天这样的场景仍然此起彼伏，人们并没有比一两百年前的先辈更加冷静和明白，而一些野心家还试图利用群体的这种特点，一旦风吹草动人们仍然会像暴民一样无差别摧毁一切
    
    </summary>
    
      <category term="Book Reading" scheme="liqul.github.io/blog/categories/Book-Reading/"/>
    
    
  </entry>
  
</feed>
