{"meta":{"title":"Bucket & Hammer","subtitle":"A place for knowledge","description":null,"author":"Liqun Li","url":"liqul.github.io/blog","root":"/blog/"},"pages":[{"title":"categories","date":"2019-03-07T03:13:12.000Z","updated":"2019-03-07T03:13:31.000Z","comments":true,"path":"categories/index.html","permalink":"liqul.github.io/blog/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-03-07T03:08:40.000Z","updated":"2019-03-07T03:12:30.000Z","comments":true,"path":"tags/index.html","permalink":"liqul.github.io/blog/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-03-07T13:15:26.097Z","updated":"2019-03-07T13:15:26.097Z","comments":true,"path":"about/index.html","permalink":"liqul.github.io/blog/about/index.html","excerpt":"","text":"I am a system engineer with interests across IOT, Big Data, and Machine Learning. I received my Ph.D on computer science from Institute of software Chinese academy of sciences in 2012, and B.E. from department of Computer science and technology at Tsinghua University in 2006. I visited Michigan State University in 2009 as a visiting scholar. I like photography and reading books. To find some of my photos, please visit my 500px. Feel free to reach me at liqul (at) outlook.com, though I may not be very responsive. Work Experience 2016.11 - now Architect at K2Data. 2016.5 - 2016.11 Engineer at 高德地图. 2014.11 - 2016.2 Engineer at PPzuche.com. 2012.8 - 2014.11 Researcher in Microsoft Research Asia (MSRA). Selected ProjectsData Management for Industrial IOT Time series is the first class citizen in industrial scenarios. Machines with hundreds of sensors generate tons of time series data that need to be stored and analyzed, demanding for a scalable and reliable storage service. However, existing solutions (e.g., OpenTSDB, Influxdb, and Timescaledb) either adopt a single node deployment, or luck of an aproperiate data model. Therefore, we developed a new time series storage service, leveraging existing open sourced projects such as Hadoop, Kafka, Zookeeper, and Parquet. We built key building blocks to enable atomic data ingestion, partitioning, and compaction. Time series data can be directly read, processed, and analyzed by parallel computing frameworks such as Map-reduce or Spark. Machines not only generate time series, but also a huge number of objects. Storing those files sounds trivial, while analyzing them incurs challenges, especially on a huge number of files. Our goal is to build an object storage service which is capable of holding a huge number of files, as well as offering easy interfaces for data analysis. For this purpose, we adopt the data model of object = metadata + file. The metadata is indexed by Elasticsearch, and we implement atomic CURD. We keep the file in HDFS, where Map-reduce or Spark programs can directly read the files. To prevent from the “small-file-problem”, a background housekeeper process continuously compacts small files. Transportation Activity Recognition on Smartphones In data crowdsourcing, it is useful to know your user’s transportation status. For instance, for a map maintainer, knowing the transportation status helps to distinguish if the data is collected on road, sidewalk, or in building. However, it is nontrivial to approach high accuracy. Practical challenges include unknown phone gesture, random noises, and achieving high energy efficiency. I designed a framework combining both inputs from the inertial sensors (accelerometer and gyroscope) and various contextual informations. Extraneous events incurred by user random activities are filtered with intuitive rules discovered through our training data from tens of people. We finally achieved a significantly better accuracy compared with existing frameworks from Google and Samsung. Indoor localizationLocalization in indoor areas is nontrivial due to the fact that GPS signals cannot penetrate walls. To tackle this challenge, we leverage various signals, e.g., Wi-Fi, Bluetooth, Geomagnetic field, IMU sensors, and even visible light, which are commonly found in indoor environment. Specifically, we proposed a Wi-Fi based positioning system called Modellet. Modellet takes advantage of both fingerprint-based and model-based approaches, and is able to adapt to environmental locality and training data density. We evaluate Modellet with data collected from venues (office, airport, and shopping mall) across China, Germany, and the U.S. We show that Modellet outperforms Radar and EZPerfect. Secondly, we build a system called Magicol adding more modalities, i.e., Geomagnetic field and IMU sensors, to the system to improve localization accuracy. We leverage the particle filter framework to fuse the signals. Evaluation results show that Magicol achieves around 2-meter accuracy in office and shopping mall areas. Finally, we explore the possibility of positioning with visible light emitted from LEDs. LED bulbs are modified to blink in unique patterns (invisible to human eyes). Any device with a light sensor can decode and estimate its own postion based on the light energy propagation model. The system is called Epsilon which achieves submeter-level accuracy. Wireless Sensor Network Wireless sensor network (WSN) typically refers to a large number of networked embedded devices, called sensor nodes. In WSNs, data is transmitted from one node to another in a multi-hop minor. WSNs are usually deployed in harsh environments like in forest or around volcano, and therefore the nodes face frequent failures. I studied several issues raised from WSNs. Specifically, I develop voice-streaming systems (namely QVS and ASM) which are aware of the voice quality. These systems prevent the problem of network congestion with an admission control protocol. I also investigate the time synchronization problem where we need to maintain accurate relative time between sensor nodes. I exploit the regular pattern of the RDS data carried by the FM radio signal for energy efficient millisecond-level time synchronization in city-scale sensor networks. PublicationsIndoor Localization Hu, Pan; Li, Liqun; Peng, Chunyi; Shen, Guobin; Zhao, Feng; Pharos: enable physical analytics through visible light based indoor localization, HotNets‘13 Li, Liqun; Hu, Pan; Shen, Guobin; Peng, Chunyi; Zhao, Feng; Epsilon: Visible Light Based Positioning System, NSDI‘13 Zheng, Yuanqing; Shen, Guobin; Li, Liqun; Zhao, Chunshui; Li, Mo; Zhao, Feng; Travi-navi: Self-deployable indoor navigation system, Mobicom‘14 Li, Liqun; Shen, Guobin; Zhao, Chunshui; Moscibroda, Thomas; Lin, Jyh-Han; Zhao, Feng; Experiencing and Handling the Diversity in Data Density and Environmental Locality in an Indoor Positioning Service, Mobicom‘14 Yuanchao Shu, Cheng Bo, Guobin Shen, Chunshui Zhao, Liqun Li and Feng Zhao; Magicol: Indoor Localization Using Pervasive Magnetic Field and Opportunistic WiFi Sensing IEEE Journal of Selected Areas in Communications, vol.33, no.7, pp.1443-1457, July, 2015 Wireless Sensor Networking Li, Liqun; Xing, Guoliang; Sun, Limin; Liu, Yan; QVS: quality-aware voice streaming for wireless sensor networks, ICDCS‘09 Li, Liqun; Xing, Guoliang; Han, Qi; Sun, Limin; Adaptive voice stream multicast over low-power wireless networks, RTSS‘10 Li, Liqun; Xing, Guoliang; Sun, Limin; Huangfu, Wei; Zhou, Ruogu; Zhu, Hongsong; Exploiting fm radio data system for adaptive clock calibration in sensor networks, Mobisys‘11 Li, Liqun; Xing, Guoliang; Han, Qi; Sun, Limin; ASM: Adaptive Voice Stream Multicast over Low-Power Wireless Networks, TPDS‘12 Li, Liqun; Xing, Guoliang; Sun, Limin; Liu, Yan; A Quality-aware Voice Streaming System for Wireless Sensor Networks, TOSN‘14 Liqun Li; Limin Sun; Guoliang Xing; Wei Huangfu; Ruogu Zhou; Hongsong Zhu, ROCS: Exploiting FM Radio Data System for Clock Calibration in Sensor Networks, in Mobile Computing, IEEE Transactions on , vol.14, no.10, pp.2130-2144, Oct. 1 2015 Misc Li, Liqun; Sun, Limin; Seer: trend-prediction-based geographic message forwarding in sparse vehicular networks, ICC‘10 Hu, Pan; Shen, Guobin; Li, Liqun; Lu, Donghuan; ViRi: view it right, Mobisys‘13 Jiangtao Li, Angli Liu, Guobin Shen, Liqun Li, Chao Sun, and Feng Zhao. 2015. Retro-VLC: Enabling Battery-free Duplex Visible Light Communication for Mobile and IoT Applications, HotMobile ‘15 Awards 2nd place in IPSN Indoor Localization Competition in 2014 held in Berlin, Germany. Excellent Ph.D. thesis award by Chinese Academy of Sciences in 2013."}],"posts":[{"title":"新闻记录-2019","slug":"news-2019","date":"2019-03-10T03:07:28.000Z","updated":"2019-03-10T14:00:28.003Z","comments":true,"path":"news-2019/","link":"","permalink":"liqul.github.io/blog/news-2019/","excerpt":"","text":"2019-03-09 【证券时报头版评论：降息的可能性存在吗?】证券时报头版评论称，如何权衡短期和长期的利益，并在当前的宏观环境下平衡好改革和稳定的关系，是未来一段时间里整个货币政策框架面临的最大挑战。保留动用利率手段来解决问题的选择，这是更为务实的策略。引导资金价格下行和推动改革并不对立，这只是同一个问题的不同侧面而已。 （来自新浪财经APP） 当然不能随便否定某种手段。 在岸人民币兑美元（CNY）北京时间23:30收报6.7200元，较周四夜盘收盘跌67点；成交量318.98亿美元，较周四下降17.59亿美元。 （来自新浪财经APP） 需要密切观察人民币的汇率是双向波动还是主动调整。 【美联储主席表示近期将公布结束缩表细节】美国联邦储备委员会主席鲍威尔8日表示，当前经济前景不需要美联储立即进行政策调整，美联储将于近期公布结束缩减资产负债表计划细节。鲍威尔当天在加利福尼亚州出席会议并发表演讲时表示，以当前经济前景来看，美联储无需立即予以政策回应，特别是通胀压力不存在，美联储就此采取了“耐心”和“等等看”的方式来考量政策立场调整。他还表示，美联储未来的政策调整，将取决于经济数据所揭示的前景和风险。（新华社） （来自新浪财经APP） 美联储的措辞还是显得不够鸽派的。毕竟自从宣布停止加息以来美元回流的趋势就停止了，资本反倒倾向新兴市场国家，这显然不利于稳定美国本土的资产价格，所以美联储下一步行动还不那么明确。 中国2月M2货币供应同比8.0%，预期8.4%，前值8.4%。中国2月M1货币供应同比2%，预期2%，前值0.4%。中国2月M0货币供应同比-2.4%，预期10%，前值17.2%。 中国2月份新增人民币贷款8858亿元人民币，预估9500亿；中国2月份社会融资规模增量7030亿元人民币，预期1.3万亿。初步统计，2月末社会融资规模存量为205.68万亿元，同比增长10.1％。其中，对实体经济发放的人民币贷款余额为139.02万亿元，同比增长13.3％，占同期社会融资规模存量的67.6％，同比高1.9个百分点。 （来自新浪财经APP） 货币供应和融资规模都远低于预期，这意味着什么呢？从货基的收益来看市场上还是不缺钱的，也可能是1月的历史新高使2月的情况看起来较少。 2019-02-27 【人民日报：化解风险 以审慎监管筑牢金融屏障 】人民日报刊发时评文章称，金融是现代经济的核心。防范金融风险的先手，一方面要把好货币供给总闸门，把过高的杠杆率降下来，防范债务风险，另一方面又必须要提供充分、必要的流动性，保证实体经济正常运行。要看到，1月份社会融资同比多增1.56万亿元，但“贴现”和“套利”只是个别企业的行为，总体上还是以流向实业为主，使得新增1000万元以下小微企业贷款利率平均降至6.16%。这是典型的应对下行压力的预调微调，也是逆周期宏观调控的题中之义，是保持股市、汇市稳定，稳住市场预期的必要举措，绝不意味着稳健货币政策发生了转向。 （来自新浪财经APP） 按照以往的新闻理解方式，“贴现”和“套利”肯定是有一些的，否则也就不必要去强调了。。。但主体应该还是小型企业的票据融资需求。只是中小企业的难题在于需求不旺，而不是真的缺少资金来扩大生产，所以融资得到的资金会用于干啥呢？当然一方面有助于缓解经营的压力，但这不能解决根本问题。资本总是逐利的，除了房市、股市，实在想不到会流向哪里。 这是3月8日发布的2月进出口情况： 2019-01-16 【10年期国债收益率跌破3.10%】中国10年期国债收益率持续走低，盘中最低已跌破3.10%整数关口，至两年多以来低位。（中证报） （来自新浪财经APP） 十年国债收益是一个非常重要的衡量指标，目前下降的空间已经不大，大约50个基点，也就意味着2019年的债券收入不会像2018年那么稳了。这里的分析不错。 2019-01-14 【瑞幸咖啡在港寻求IPO 投行已开始为其准备上市资料】消息人士称，投资银行已开始为瑞幸咖啡准备关于香港联交所IPO的上市资料。瑞幸咖啡成立于2017年10月，2018年1月开始试运营。该公司至今筹集了两轮共4亿美元，在2018年12月12日的最新B轮融资中筹集了2亿美元，获得了22亿美元的估值。（EqualOcean） （来自新浪财经APP） 触顶期多妖孽，资本在疯狂炒作准备逃顶。 【2018年水泥行业利润或超1500亿元 海螺水泥预计全年狂赚逾300亿元】1月10日晚间，海螺水泥披露了2018年年度业绩预告，预计2018年实现归属于上市公司股东净利润同比将增加126.84亿元至158.55亿元，同比增加80%至100%；预计实现扣非后净利润同比增加140.78亿元至168.93亿元，同比增幅为100%至120%。据记者统计，除了海螺水泥，冀东水泥、万年青和华新水泥发布2018年业绩预告，均预计最高净利润翻倍。（证券日报） （来自新浪财经APP） 基建狂魔本性不改，说实话也改不了，不能改。 【备付金100%集中交存大限已至 小型支付机构面临“生死考”】昨日，支付行业迎来万亿规模客户备付金集中交存的大限。据悉，包括支付宝、腾讯财付通在内的第三方支付机构已完成100%集中交存等相关工作。在业内人士看来，没有了备付金作为收入来源的第三方支付机构，在“断直连”的基础上，业务生态将迎来巨变，如何拓展收入来源、弥补盈利缺口是第三方支付未来面临的难题。（证券日报） （来自新浪财经APP） 早该这么做了。 2019-01-09【中美短期国债利率倒挂幅度扩大】近几日，美债收益率受强劲非农数据刺激有所反弹，中国短期国债收益率则因年初资金面宽松而明显走低，中美短期国债利差倒挂程度加重。1月8日，1年期美国国债收益率报2.6%，1年期中国国债收益率报2.35%，中美1年期国债利差倒挂幅度扩大至25基点，为2007年11月底以来最大。（中证网） （来自新浪财经APP） 1年期国债的实际意义似乎不大。 2019-01-06 【北京首推集体土地共有产权房 1月16日开始竞买】北京市规划和自然委员会在日前发布了一则名为“集体土地建共有产权房探索公共服务配套与共有产权房建设的供地方式”的消息。消息显示，集体建设用地区级统筹大兴区瀛海镇YZ00-0803-2003、2004、2005A、2005B、2008地块于12月27日9时起挂牌出让公告，将于1月16日开始竞买，预计1月30日完成挂牌出让。这也是北京市第一次使用集体建设用地建设共有产权房。（华夏时报） （来自新浪财经APP） 迟来的集体所有制房产入市，制度影响大于经济影响吧。但这些房子的地理位置实在比较偏，所以也难以对房地产市场造成实质性的影响。而且稳房价是近几年的基本国策。 【交行连平：2019年基建投资力度加大 制造业投资或放缓】2019年宏观政策明显转向，着力点在于加大基建投资力度。预计2019年基建投资增长10%，成为拉动投资的关键力量。2019年基数效应减弱，制造业盈利增速已趋回落，将难以支撑制造业投资继续较快增长。受外需环境影响，与出口相关的制造业投资预期较弱。民间投资意愿下降，扩大生产投资显得非常谨慎，将影响制造业投资增长。预计2019年制造业投资增速下降到4%，高技术制造业投资增长较快。(证券时报) （来自新浪财经APP） 制造业放缓的原因是需求不旺，而GDP增长只能依赖基础建设来拉动了。 【去年中国股票基金净流入350亿美元，A股对外资吸引力大增】据资金监测机构EPFR最新报告显示，去年中国股票型基金实现资金净流入达到350亿美元，位居全球主要新兴市场榜首。同时，作为外资主要的A股配置渠道，沪、深港通全年交易活跃，外资借道北上资金净买入累计超过6400亿元，创新历史纪录。（证券时报） （来自新浪财经APP）https://mp.weixin.qq.com/s/j_0eQT13OaptWOh7JYX5FQ 中国比起其它新兴市场明显要靠谱的多，而且美国自身也是内忧外患，并且中国的金融能力有目共睹，自然对外资是有较大吸引力的。","categories":[{"name":"News","slug":"News","permalink":"liqul.github.io/blog/categories/News/"}],"tags":[]},{"title":"金融概念整理——货币贬值","slug":"currency-depreciation","date":"2019-03-08T06:24:35.000Z","updated":"2019-03-08T13:48:15.274Z","comments":true,"path":"currency-depreciation/","link":"","permalink":"liqul.github.io/blog/currency-depreciation/","excerpt":"《Principles For Navigating BIG DEBT CRISES》里有这样一段话： As explained earlier, the best way to bring that about is to let the currency depreciate sharply and quickly. While that will hurt those who are long that currency, it will make it more attractive for investors who will get long after the devaluation, because the total return on holding the currency (i.e., the spot currency appreciation plus the interest rate difference) is more likely to be positive, and at a sharply depreciated currency level it won’t take an intolerably high interest rate to make the total return attractive. In other words, the best way to ensure that investors expect positive total returns going forward at a relatively low real interest rate (which is what the weak domestic conditions need) is to depreciate the currency enough. 读起来有些违反常识。一般一个国家出现危机时，大都会坚持抵抗，尽量维持货币的汇率，否则会出现严重的资本外流。所谓抵抗，其基本手段就是主动提高利率，因为这样看起来货币的收益率更高，更有吸引力。但这里却说，最好的做法是一次大幅贬值到位，保持低利率，才是真正正确的做法。究竟怎么做才是对的呢？","text":"《Principles For Navigating BIG DEBT CRISES》里有这样一段话： As explained earlier, the best way to bring that about is to let the currency depreciate sharply and quickly. While that will hurt those who are long that currency, it will make it more attractive for investors who will get long after the devaluation, because the total return on holding the currency (i.e., the spot currency appreciation plus the interest rate difference) is more likely to be positive, and at a sharply depreciated currency level it won’t take an intolerably high interest rate to make the total return attractive. In other words, the best way to ensure that investors expect positive total returns going forward at a relatively low real interest rate (which is what the weak domestic conditions need) is to depreciate the currency enough. 读起来有些违反常识。一般一个国家出现危机时，大都会坚持抵抗，尽量维持货币的汇率，否则会出现严重的资本外流。所谓抵抗，其基本手段就是主动提高利率，因为这样看起来货币的收益率更高，更有吸引力。但这里却说，最好的做法是一次大幅贬值到位，保持低利率，才是真正正确的做法。究竟怎么做才是对的呢？ 首先，一个明确的前提是：在经济偏弱的环境下，低利率更加有利于经济发展，而高利率恰恰相反，会伤害经济发展。这里不提经济增长中的水分问题，比如，有些人可能认为用高利率主动刺破泡沫才是刮骨疗毒，但历史上却很少有人这么做。美联储从2019年初开始，也不得不停止加息，正是考虑经济放缓。从政治的角度，主动刺破泡沫后造成无法控制的局面才是真正的危机。所以，这里的讨论都基于这样的前提。 然后，如果一个国家的经济下行，利率却升高，那合理的预期是这个国家的经济将进一步下行，而且可能导致混乱局面。如果预期是这样的，国外投资者绝不会在此时进入市场，而其本国的资本也想方设法出逃，从而进一步打击了该国的经济。 所以，表面上是保卫货币，实际上却只能起到相反的效果。那怎么做才是对的呢？上面引用的话才是正解： 资产必须看起来足够便宜； 资产必须能产生正向收益； 要做到这两点，货币就必须大幅贬值，同时保持低利率。但为什么不是逐步缓慢的贬值，而是一次性大幅贬值呢？作者其实在书中有讨论。一次性贬值的好处在于能对市场产生“出其不意”的效果，所以人们心理上不会产生一种持续贬值的预期。此外，由于不进行汇率抵抗，也避免了浪费外汇储备。多方面来看，这都是最好的选择。","categories":[{"name":"Finance","slug":"Finance","permalink":"liqul.github.io/blog/categories/Finance/"}],"tags":[{"name":"currency","slug":"currency","permalink":"liqul.github.io/blog/tags/currency/"}]},{"title":"Linux下文件删除的规则","slug":"linux-delete-file","date":"2019-03-08T03:10:48.000Z","updated":"2019-03-08T03:33:53.000Z","comments":true,"path":"linux-delete-file/","link":"","permalink":"liqul.github.io/blog/linux-delete-file/","excerpt":"原本一直以为Linux下删除一个目录的条件是：user拥有该目录，以及该目录下所有子目录和子文件的写权限。但其实这样的理解是错误的。按照这里的意思，是否能删除一个文件，取决于user是否拥有文件父目录的写权限；删除一个目录，第一步是清空该目录下的所有内容，最后删除该目录本身。","text":"原本一直以为Linux下删除一个目录的条件是：user拥有该目录，以及该目录下所有子目录和子文件的写权限。但其实这样的理解是错误的。按照这里的意思，是否能删除一个文件，取决于user是否拥有文件父目录的写权限；删除一个目录，第一步是清空该目录下的所有内容，最后删除该目录本身。 以一个具体的例子来说，目录结构是这样的： 其中test和test1都是由user创建的目录，而目录下的子目录和文件均由root创建。按照前面的规则，test1可以通过 12su userrm -rf test1 删除，但如果执行 12su userrm -rf test 则会因为在/test/root/下的hello文件无法被user删除而报错。但如果/test/root/下没有无法被user删除的文件，或者/test/root/为空，则根据规则就可以被删除了。 总而言之，并不清楚为何Linux会使用这样的一套删除规则，可能是出于某种方便的考量。这意味着一个用户实际上能够删除其本来不具备权限的文件。在理解上容易发生偏差。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"liqul.github.io/blog/tags/Linux/"}]},{"title":"金融概念整理——房价指标","slug":"housing-price-as-indicator","date":"2019-03-07T07:29:22.000Z","updated":"2019-03-07T08:00:06.000Z","comments":true,"path":"housing-price-as-indicator/","link":"","permalink":"liqul.github.io/blog/housing-price-as-indicator/","excerpt":"经济好的时候，人们总喜欢买房子，这是一种自然的规律。如果你住在一个“老破小”里，这几年赚了不少钱，难道你不想来个升级？所以，房价一直是经济的一个观察指标。如果房价一直在上涨，多半大家收入也在上涨，经济处于扩张期；反之，则说明大家收入增长停止了，或放缓了。行政手段虽然能起到一定的稳定作用，但本质上是无法阻挡这种趋势的。","text":"经济好的时候，人们总喜欢买房子，这是一种自然的规律。如果你住在一个“老破小”里，这几年赚了不少钱，难道你不想来个升级？所以，房价一直是经济的一个观察指标。如果房价一直在上涨，多半大家收入也在上涨，经济处于扩张期；反之，则说明大家收入增长停止了，或放缓了。行政手段虽然能起到一定的稳定作用，但本质上是无法阻挡这种趋势的。 当然有些人会人为房价上涨可能主要是人为炒作。不可否认炒作的确起到了推波助澜的作用，比如会引发人们心理恐慌，所谓“再不上车就来不及了”。但是，本质上人们还是能买得起，否则按揭贷款终究会违约的。从2015到2017，短短三年居民家庭债务占GDP比例从36%左右上升到51%左右，人民手里是真的没钱了…从加入WTO依赖积攒了多年的收入，一大部分套在房子里了。所以，即使2019年全面放开限购，房价也不可能再像2008年到2017年这几年迅猛上涨了。 在中国，人们有了钱喜欢去北上广深买房，那么从世界的维度来看呢？一样的道理嘛，喜欢去那些风景优美、气候适宜的地方买房，比如温哥华、澳大利亚、新西兰。有趣的是，温哥华的房价被炒高以后，导致当地人也无法接受这样的高房价，纷纷表示应该对外国人空置的房子征收空置税…然而，一旦这些地方的房价开始下跌，反映的就不止是其本国经济出现问题，更是反映世界经济的一种重要指标。在全球化的今天，很难有某个国家能独善其身。在2019年，也看到了不少新闻在谈这些地方的房价纷纷开始下跌了。从产业链的角度，澳洲的地位主要是资源输出国，而经济下滑也会从资源价格下跌开始，所以澳洲的房地产还具有经济先行指标的属性。","categories":[{"name":"Finance","slug":"Finance","permalink":"liqul.github.io/blog/categories/Finance/"}],"tags":[{"name":"Housing","slug":"Housing","permalink":"liqul.github.io/blog/tags/Housing/"},{"name":"Finance","slug":"Finance","permalink":"liqul.github.io/blog/tags/Finance/"}]},{"title":"金融概念整理——信用利差（credit spread）","slug":"credit-spread","date":"2019-03-06T13:55:05.031Z","updated":"2019-03-07T07:59:15.000Z","comments":true,"path":"credit-spread/","link":"","permalink":"liqul.github.io/blog/credit-spread/","excerpt":"不同债券的利率是不一样的，为什么呢？想想如果5年国债的收益率是3%，那么一个企业发行一种5年的债券，它的利率应该定在多少？显然要高于3%，否则傻子才去买，因为企业是可能违约的，而国家会违约吗？所以，不同债券的利率本来就该不一样。","text":"不同债券的利率是不一样的，为什么呢？想想如果5年国债的收益率是3%，那么一个企业发行一种5年的债券，它的利率应该定在多少？显然要高于3%，否则傻子才去买，因为企业是可能违约的，而国家会违约吗？所以，不同债券的利率本来就该不一样。 那么如果是两家企业A和B，两边都发行5年债券，那么它们的利率相同吗？这不好说，如果两家的信誉差不多，那么它们的利率可能是一样的；反过来，信誉越高则利率越低。这里的原理与前面国债的例子也一样，反正风险越高利率也就越高。为了方便投资者购买债券，不同国家和机构都会定期评估各家公司的情况，给它们定一个级别。如果一家企业的等级比较高，那么对它肯定是有利的，因为发债的利率可以定的比较低，相当于融资成本下降了。至于投资者应不应该相信评级机构的定级呢？大家自己琢磨～ 回到信用利差。一个最简单的观察方式就是看某个级别的公司债券利率与国债的差。这个差值会随着市场波动上下起伏，而这种变化是有比较深刻的含义的。如果利差变小，说明投资人认为市场上风险在变小，所以愿意去购买那些信誉较差公司的债券，去追逐高收益；反之，则大家抛售高收益债券，转而投向国债。所以，信用利差是观察市场上风险偏好的直接指标之一。 这里给出一个指数，追踪美国高收益债利率与国债之间的利差，看这里。基本上这个指数的走势与大盘（比如s&amp;p500）是相反的，但内在含义是不同的。","categories":[{"name":"Finance","slug":"Finance","permalink":"liqul.github.io/blog/categories/Finance/"}],"tags":[{"name":"credit spread","slug":"credit-spread","permalink":"liqul.github.io/blog/tags/credit-spread/"}]},{"title":"新闻纪录-2018","slug":"news","date":"2019-01-02T16:00:00.000Z","updated":"2019-03-10T03:09:13.798Z","comments":true,"path":"news/","link":"","permalink":"liqul.github.io/blog/news/","excerpt":"每天读新闻，学习思考一些基本的经济逻辑。","text":"每天读新闻，学习思考一些基本的经济逻辑。 2019-01-03 【日元暴涨！春节赴日旅游成本或提高】1月3日，作为避险货币的日元兑美元出现了超过4%的暴涨！日元兑人民币的涨幅也一度超过2%，而最近半个月内，日元兑人民币最多升值超过6%。这就意味着，此时将手中的人民币兑换成日元，其购买力相比半个月前下降了6%。日本一直是中国游客最青睐的境外出游地之一。此番日元大涨，想在寒假、春节假期去日本玩的人们出游成本或将提高不少。（经参报） （来自新浪财经APP） 这里有篇文章在分析为啥日元在危机来临时成了一种避险货币。本质原因是在过去很长时间里日元在本国的利率都非常低，所以在正常情况下，持有日元的实体会尽量将日元兑换成其它货币从而赚取更高的回报。但一旦风险升高，这些实体就会抛弃高风险资产变现成日元，所以相当于日元的需求增加，推动日元升值。这某种程度上与美元有些相似。 【仅实施半年 青岛市高新区暂停公证摇号售房规则】记者致电青岛市高新区不动产登记中心的工作人员，工作人员称该文件已经确定实行了，以后在高新区购房如果符合购房资格，不需要经过摇号也能直接买房。工作人员还解释称，此次取消摇号的文件只是取消了购房时的摇号规则，并不是取消限购。据悉，该文件从1月1日起开始执行。记者了解到，此次取消的《青岛市高新区商品房公证摇号售房规则》于2018年6月25起开始实施，该规则从实施至取消仅半年。（中青网） （来自新浪财经APP） 不管承不承认，该放松还是要放松的。 【两年期美债收益率与联邦基金实际利率持平，为2008年以来首次】周四（1月3日）纽约尾盘，美国10年期基准国债收益率下跌6.70个基点，连续五个交易日下跌，报2.5535%，美股尾盘一度跌至2.5429%，创2018年1月17日以来盘中新低。两年期美债收益率重挫8.90个基点，也连跌五天，报2.2386%，美股尾盘一度跌至2.3705%，创2018年5月30日以来盘中新低。 （来自新浪财经APP） 看看这诡异的美债收益率曲线吧，从一年前正常的样子变成今天这样的情况，大家基本上都认为危机快来了。 【温哥华2018年房地产销量跌至2000年以来新低，12月份尤其令人感到沮丧】加拿大大温哥华房地产局报告称，12月份房地产成交量为1,072宗，环比下降33%，同比下降47%。这是最近10年来表现最差的12月。2018年全年，销量24,619宗，较之前一年下降32%，比10年平均水平低25%。不过，房价依然是加拿大全境最昂贵的。基准房价跌至103万加元（76万美元），环比下降0.9%、同比下降2.7%。根据彭博计算，温哥华12月房地产销量是2008年以来同月最低。 （来自新浪财经APP） 温哥华的房地产绝对是一个非常重要的经济指标——中国有钱人的购房天堂~ 美国12月ISM制造业指数 54.1，预期 57.5，前值 59.3。美国12月ISM制造业物价支付指数 54.9，预期 57.7，前值 60.7。美国12月ISM制造业新订单指数 51.1，前值 62.1。美国12月ISM制造业就业指数 56.2，前值 58.4。 （来自新浪财经APP） 远低于预期，看来整个世界需求都在下降。 2018-12-31 【海通姜超点评PMI数据：制造业PMI跌至荣枯线下 通缩风险升温】海通证券宏观姜超、于博点评称，制造业PMI跌至荣枯线下。量缩价跌延续，通缩风险升温。供、需、价、库存全线下滑，令12月制造业PMI跌至线下、再创（阶段）新低。而从中观高频数据看，12月以来，终端需求依然偏弱，工业生产短期稳定，库存普遍偏高，指向主动去库存延续。展望19年，需求转弱仍将继续拖累生产下滑，并带动价格回落，通缩风险正在升温，利润分配也将由上游向中下游行业倾斜。 （来自新浪财经APP） PMI持续下降。 2018-12-27 2018年三季度，我国经常账户顺差1581亿元，资本和金融账户顺差1143亿元，其中，非储备性质的金融账户顺差952亿元，储备资产减少203亿元。2018年前三季度，我国经常账户逆差249亿元，资本和金融账户顺差6141亿元，其中，非储备性质的金融账户顺差9154亿元，储备资产增加2990亿元。 （来自新浪财经APP） 注意顺差中大部分来自资本和金融，或许意味着未来一段时间国际资本会进一步介入中国市场，这与华尔街在中美贸易战中的态度有关。但这种债务周期末端，实在不适合小散操作。 中国7天期回购利率创2017年来最大升幅至3.01%。 （来自新浪财经APP） 其实并不高。 【全国财政工作会议在京召开 2019年将重点做好十大工作】会议强调，一是继续支持打好三大攻坚战。要大幅增加地方政府专项债券，严格控制地方政府隐性债务，有效防范化解财政金融风险。二是推动经济转型升级。三是进一步释放内需潜力。四是促进区域协调发展。五是贯彻实施乡村振兴战略。六是加强保障和改善民生。七是深化财税体制改革。八是确保地方财政可持续。各地要担起财政可持续的责任，始终绷紧财政可持续这根弦，增强财政运行稳定性和风险防控有效性。九是持续提升财政管理效能。十是推进国际财经合作。 （来自新浪财经APP） 感觉前两条才是重点，地方债俨然成了国内金融风险的最大因素。 2018-12-16 桥水基金创始人瑞·达利欧：桥水基金用于衡量全球范围内冲突的指标创最近逾70年来最高。 （来自新浪财经APP） 不知道在哪里能看到这个指标。 2018-12-11 英国5年期信贷违约掉期（CDS）升至2016年7月以来新高，日内涨1个基点至39个基点。 （来自新浪财经APP） 风险在升高，不知道在哪能持续跟踪这个指标。 中国11月M2货币供应同比 8%，预期 8%，前值 8%。中国11月M1货币供应同比 1.5%，预期 3%，前值 2.7%。中国11月M0货币供应同比 2.8%，预期 3%，前值 2.8%。 中国11月新增人民币贷款 1.25万亿元人民币，预期 12000亿元，前值 6970亿元。中国11月社会融资规模增量1.52万亿元人民币，预期13340亿元，前值7288亿元。 （来自新浪财经APP） 这个月终于基本符合预期了。 2018-12-10 【北京最新人口蓝皮书：外来人口、户籍人口双下降 专家预测雄安新区人口规模2050年达到千万】昨天下午（9日），最新《北京人口蓝皮书：北京人口发展研究报告（2018）》正式发布。蓝皮书显示，2017年，北京市户籍人口和外来人口实现双降，北京全市的常住人口为2170.7万人，这其中65岁及以上老年人口占比超过十分之一，近20多年来，北京人口一直维持在10‰以下的超低生育水平，当前约有四个劳动力需要抚养一个非劳动力。研究人员预测，雄安新区2030年人口规模为120万至140万人，2040年为285万人至350万人，2050年人口规模为1000万人左右。专家认为，雄安新区要积极承接非首都功能疏解，以业引人，多措并举，引导人口有序聚集。（央广） （来自新浪财经APP） 再不发力给点好处，这个趋势会越来越严重。雄安…这是检测政治影响力的试验田。无论浦东还是深圳，并不是靠政策就能建成的，这块深陷雾霾中的白洋淀要发展成下一个深圳？谨慎，谨慎。 2018-12-06 【外资落地限制将进入集中清理期】在“稳外资”系列政策密集出台的基础上，我国优化外商投资环境再加码，外资落地限制即将进入集中清理期。记者获悉，目前，商务部、国家发展改革委、司法部等三部委已经着手启动限制性法律法规的排查清理工作，全面清理取消在外商投资准入负面清单以外领域针对外资设置的准入限制。（经济参考报） （来自新浪财经APP） 这应该是答应特朗普的许诺之一。 中国11月外汇储备30中国11月外汇储备30617亿美元，预期30440亿美元，前值 30531亿美元。中国11月外汇储备环比增加86亿美元，终结三连降，上月减少339亿美元。中国11月外汇储备2.2134万亿SDR，前值2.209万亿SDR。 （来自新浪财经APP） 这让人有些意外。 2018-12-04 【银行理财子公司诞生倒计时 非银机构谋入局】《商业银行理财子公司管理办法》出台后，备受市场瞩目的理财子公司诞生已进入倒计时。分析人士预计，后续“中小银行+非银机构”合作设立理财子公司的模式可能出现，二者在渠道、产品和投研方面优势互补。与此同时，一些地方政府也向理财子公司抛出“橄榄枝”，希望成为理财子公司的注册地。（中证报） （来自新浪财经APP） 市场风险加剧的时候，总希望多一些人来接盘的，这是一种规律。 【美国国债收益率曲线部分出现十多年来首次倒挂】周一（12月3日）纽约尾盘，美国10年期基准国债收益率下跌1.82个基点，报2.9697%；10月9日曾涨至3.2594%，为2011年5月4日以来盘中最高位，当年最高位在2月9日——3.7660%。两年期美债收益率涨3.46个基点，报2.8211%；11月8日曾涨至2.9732%——逼近2008年6月25日最高位2.9966%，当年最高位在6月13日——3.1140%。三年期美债收益率涨2.77个基点，报2.8274%。五年期美债收益率涨0.50个基点，报2.8175%，与三年期、两年期美债收益率出现倒挂，为2007年以来首次；11月8日曾涨至3.0967%——逼近2008年9月22日高位3.1350%，当年最高位在6月16日——3.7577%。 （来自新浪财经APP） 如果每次危机来临之前都会发生收益率曲线倒挂，那么说明很多人，或者是一些主力资本方对危机的嗅觉是非常灵敏的。 美联储达拉斯联储主席Kaplan（2020年有投票权）：美国经济在2019年中期将“很可能”会变得不一样。 （来自新浪财经APP） 按照这个周期的各种特征，明年中期会是个阶段性转折点。 2018-12-03 【港元短期拆息急升，香港加息预期升温】市场分析人士表示，临近年底多因素将导致香港资金面持续收紧，加上市场预期美国12月加息几乎板上钉钉，因此年底前香港加息的预期进一步升温。如果香港再次加息，预期幅度为12.5个基点。（中国证券报） （来自新浪财经APP） 港币加息对于香港房地产是一记重拳。 【外资绝对控股的券商来了 证券业竞争格局将被改写】 11月30日晚间，中国证监会发布消息称，依法核准UBS AG（瑞银集团）增持瑞银证券有限责任公司的股比至51%，核准瑞银证券有限责任公司变更实际控制人。这意味着证券行业的竞争格局将被改写。外资机构绝对控股后将对国内证券业产生两方面的影响。 一方面，将会抢占国内券商的市场，对国内证券行业形成冲击， 但当前，因为外资进入的不会太快，所以这种冲击以及压力还比较小。 另一方面，随着外资机构绝对控股的券商增多，国内券商将会经历一个竞争加剧、行业集中度提升的过程，小券商面临的挑战或将更加严峻，将会加大证券业整合的力度，加剧国内券商的市场化竞争，有利于国内券商机构的竞争意识得到实质性的提升。（证券日报） （来自新浪财经APP） 原来在和谈之前，已经有诸多消息可推导出休战的信号。 【周末出新规！银行理财可投A股，首批批筹名单快来了】银保监会发布《商业银行理财子公司管理办法》。据了解，目前各银行对成立理财子公司的准备进度不一。有的股份行已把子公司组织架构等公司组建方案准备充分，但大行进展则相对较慢。北京一股份行资管部人士预计，监管部门很快将会公布第一批批筹名单，国有大行很有可能在第一批批筹名单中。不过，银行现金管理类理财产品的大爆发也引发了一些热议，有声音认为存在与货币基金监管标准不统一的问题，据了解，监管部门也注意到了这一问题，下一步或将研究出台规范“类货基”现金管理类银行理财产品的监管办法。（券商中国） （来自新浪财经APP） 在这种债务周期的顶端推出这样的理财产品真是耐人寻味。 重大利好袭来，离岸人民币大涨近500点，收复6.90关口。 （来自新浪财经APP） 等着下一波的下跌。 截至11:30，沪股通净流入69.52亿元，港股通净流入25.86亿元，超过11月2日早盘净流入规模。11月2日北上资金创纪录新高。 （来自新浪财经APP） 这基本解释了人民币汇率跳涨的内在原因：国际资本流入A股。 垃圾债收益处于一年中的高点。 相比2018年初的暴跌，这次10月份的下跌才是实质性的，垃圾债券的收益创新高。 鸿学院关于转基因农业的讨论 2018-11-29 【亚股高开 日经涨0.8% 追随隔夜美股涨势】日股日经225指数开盘涨0.8%，至22,360.98点。日股东证指数开盘涨0.8%，至1,667.16点。 韩国首尔综指涨1.2%。 鲍威尔鸽派言论推动美股上涨，标普500指数涨2.3%创八个月最大涨幅，道指涨2.5%，纳指涨近3%。美联储主席鲍威尔称联储基准利率已接近中性水平，暗示未来加息次数可能较少。 （来自新浪财经APP） 一字千金~如果加息开始变慢，那么根据以往经验倒计时半年，也就是2019年中可能会出现更大的波动。这是近距离观察一次债务周期的良机。 【海通姜超：明年中国货币政策大概率将实际宽松 债牛料持续】海通证券固收分析师姜超等报告称，国内通胀下行、融资需求低增，为防止经济衰退，货币政策仍将宽松，利率债牛市仍将持续。2019年经济下行压力依然很大，预判实际GDP中枢将下降至6%。未来不排除降准、定向降息、下调OMO利率等，打开短端利率下行空间，届时收益率曲线有望平坦化下行。 （来自新浪财经APP） 各种刺激就看有没有效果了。在经济大周期下，所有刺激也只是让经济不要过于“硬着陆”。 【短期美债收益率大跌，美债收益率曲线在鲍威尔讲话偏各派之际趋于陡峭】周三（11月28日）纽约尾盘，美国10年期基准国债收益率上涨0.18个基点，报3.0590%；10月9日曾涨至3.2594%，为2011年5月4日以来盘中最高位，当年最高位在2月9日——3.7660%。两年期美债收益率跌2.43个基点，报2.8086%，盘中交投于2.8410%-2.7945%区间；11月8日曾涨至2.9732%——逼近2008年6月25日最高位2.9966%，当年最高位在6月13日——3.1140%。 （来自新浪财经APP） 这里的逻辑关系我仍然看不明白。10年国债收益上涨（价格下跌），而两年国债收益下跌（价格上涨），似乎隐含着一部分人将原本长期不动的钱准备进行短期投资，进而意味着短期的资产行情可能会上扬。但这仍然是一个不明确的信号，还需要等待G20的新消息。 2018-11-28 【宝宝类理财平均收益率2.86% 再创年内新低】据融360监测的数据显示，昨日，473只银行理财在线产品的平均预期年化收益率为4.45%，而72只宝宝理财产品平均预期年化收益率为2.86%。同时，上周（11月16日至11月22日）互联网宝宝理财产品收益率为2.86%，较前一周下降0.03个百分点，再创年内新低。（证券日报） （来自新浪财经APP） 相比银行利率这种被政治垄断的指标，宝宝类的收益更加接近市场化，这是观察经济的一项好指标。 房企借道信托紧急“输血” 4天签约规模400亿元】近日，包括泰禾集团、福晟集团、正荣地产在内的三家房企分别与信托公司签署战略合作协议，合作规模总计达400亿元。记者注意到，近期房企再融资利率水涨船高，最高年化利率接近14%。天风证券研报指出，若新房销售市场2019年持续低迷，则房企资金面或将面临三期叠加：占到位资金接近一半的销售回款2019年或将开始下滑；2019年上半年仍是债券到期高峰，房企借新还旧压力较大；同时，2019年也是非标到期高峰。（每日经济新闻） （来自新浪财经APP） 地产如果出现风险，这些信托也是要跟着完蛋的。 2018-11-26 【中金：如政策面不果断调整 不排除2019上半年M1负增长】中金发布报告称，目前中国名义增长和企业盈利面临进一步下行的压力，如果政策面不果断调整（尤其是财政和地产相关政策），不能排除2019上半年M1（狭义货币供应量）增速转负的可能。中金宏观团队在名为《M1增长是否会转负？》的最新报告指出，鉴于地产需求、投资以及企业盈利的领先指标均在走弱，M1增长可能继续承压。中金预计2019年商品房销售额同比下降10%、土地购置费同比下降15%。 （来自新浪财经APP） 通缩压力增加，这对2019年的资产价格可不是好消息。 2018-11-17 美国9月长期资本净流入 +308亿美元，前值 1318亿美元。美国9月国际资本净流入 -291亿美元，前值 1082亿美元。中国9月所持美国国债减少137亿美元，至1.15万亿美元，创逾一年新低。日本9月所持美债下降19亿美元，至1.03万亿美元。 （来自新浪财经APP） 中国需要这笔钱来平衡汇率吧。 【部分国家豁免于伊朗制裁抑制油价 致美沙关系紧张】据华尔街日报报道，美国对8个国家和地区豁免了伊朗制裁，一定量的伊朗油进入市场正在打压油价。这导致了沙特与华盛顿发生冲突，因沙特迫切需要减产来推升油价升至80美元/桶以上来支撑其经济。美国担忧部分国家被要求削减得更多而产生抱怨，因此与豁免国家就豁免产量保密，导致欧佩克难以估算产量。沙特官员表示，特朗普政府对制裁缺乏坦率，他们感到了背叛，并将制定更加独立于美国目标的石油政策。 （来自新浪财经APP） 美国中情局（CIA）得出结论：沙特王储Bin Salman下令刺杀记者Khashoggi。（华盛顿邮报） （来自新浪财经APP） 美国国务院发言人Nauert：关于特朗普政府已经在记者Khashoggi遇害案上得出最终结论的报道失实。在记者Khashoggi死亡事件上仍然存在没有解开的疑团。美国政府将继续征求国会山的意见。美方将继续探求真相，并维持与沙特之间重要的战略关系。 （来自新浪财经APP） 美国与沙特之间的关系很有趣。原本沙特会与俄罗斯联手太高油价，但Khashoggi事件使得沙特不敢过于傲娇，还得乖乖听美国的话，大家一块提高产能压油价。这时候俄罗斯显然不会独自减产，因为如果这样会导致市场占有率下降。 另一方面，CIA的结论显然没有经过特朗普的同意，这也反映了美国政府内部的分裂。 【杭州房贷利率全面松动，首套房、二套房均包括】二手房成交惨淡，新房排队摇号盛况不再，在刚刚过去的&gt;“金九银十”，杭州市场的传统旺季行情并未如期上演。楼市降温的同时，房贷利率也出现了松动。据了解，10月底，工商银行、中国银行部分支行率先下调房贷利率；最近，建设银行、农业银行、招商银行、汇丰银行等也相继回调。这不仅是针对刚需购买的首套房，甚至还包括二套房。（都市快报） （来自新浪财经APP） 这可是个值得关注的信号。如果房市再次上行，那么加杠杆的时间就到了。 2018-11-15 【英国首相May声明：脱欧协议获得内阁支持】英国首相May发布声明称，内阁进行了充满激情的讨论。这份脱欧协议获得内阁的支持，内阁认为政府应当同意这份脱欧协议。这些决定并非掉以轻心。将坚定不移地捍卫不列颠的利益。最新进展将有助于我们在脱欧问题上继续前行。我知道，未来还会出现艰难的处境。预计英国脱欧会面临严格的审查，这是正确的做法。 （来自新浪财经APP） 英国感觉仍然是200年前的那个英国，但这个世界已经是200年后的世界了。脱欧的英国应该不会再加入了，而欧盟的未来也有一些不确定因素。民粹主义思潮还会持续一段时间吧，而仅靠法德支撑的欧盟也不免有些摇摇欲坠。最近，默克尔下台了。 【美联储主席鲍威尔：缩减资产负债表进展“非常好 ”】美联储主席鲍威尔在达拉斯联储举行的活动上发表讲话称，对经济状况感到非常满意；通货膨胀正中目标；挑战包括进一步加息的幅度和步伐；目标是维持经济增长的同时保持低通胀；努力避免加息太慢或太快；认真对待这两种风险因而逐步提高利率；有关中性利率的讨论旨在突出风险管理。 （来自新浪财经APP） 石油价格的下跌进一步抑制了通胀，而美联储越来越找不到理由加息了。 【外汇局：10月银行结售汇逆差203亿元人民币】国家外汇管理局15日披露统计数据显示，2018年10月，银行结汇10751亿元人民币（等值1552亿美元），售汇10953亿元人民币（等值1581亿美元），结售汇逆差203亿元人民币（等值29亿美元）。其中，银行代客结汇9862亿元人民币，售汇10103亿元人民币，结售汇逆差241亿元人民币；银行自身结汇888亿元人民币，售汇850亿元人民币，结售汇顺差38亿元人民币。 （来自新浪财经APP） 用钱投票的是真选择。 2018-11-13 【10月金融数据3个创出新低 降息降准等更多措施可期】10月社会融资规模增量、广义货币（M2）、狭义货币（M1）3个金融数据均创出不同期限新低。数据公布后，债市应声上涨，利率债收益率下行幅度立即扩大。市场普遍认为，考虑到当前错综复杂的内外部形势，未来降准降息都有空间。海通证券分析师姜超认为，货币宽松预期增强，降准等政策仍有很大操作空间。（券商中国） （来自新浪财经APP） 货币一直是宽松的，只是有些地方虽然紧但却仍然流不进去，比如民营企业。这是制度上的根本原因决定的。 布伦特原油日内跌幅达2%，报68.72美元/桶；WTI跌幅也扩大至2%，报58.73美元/桶。 （来自新浪财经APP） 石油一直都不是一种普通商品。虽然贸易战使人们对未来需求的预期不高，但也还不能解释这么短时间这么大幅度的下跌。中东的局面还是错综复杂，沙特、美国、俄罗斯三方的态度决定了油价的高低。对于油价的估计对散户来说实在太难。 中国10月M2同比增8％，预期8.4％，前值8.3％。 （来自新浪财经APP） 连续几个月M2增长不及预期，距离通缩并不遥远了。 中国10月新增人民币贷款 6970亿元人民币，预期 9045亿元，前值 13800亿元。中国10月社会融资规模增量 7288亿元人民币，预期 13000亿元，前值 22054亿元。 （来自新浪财经APP） 显然不是银行没有钱，只是“找不到”理想的借款人而已。 中国10月M1货币供应同比 2.7%，预期 4.2%，前值 4%。中国10月M0货币供应同比 2.8%，预期 2.8%，前值 2.2%。 （来自新浪财经APP） 巨大差距。 2018-11-07 【580亿！险资纾困专项产品规模创新高】继5日太平资产发起设立目标规模80亿的纾困专项产品后，昨日人保资产也设立专项产品，目标总规模达300亿。上周一，国寿资产成立保险业首只目标规模200亿的纾困专项产品。至此，保险业已有3只专项产品，目标总规模合计达到580亿元。（证券时报） （来自新浪财经APP） 这也算是金融创新了。几百亿的规模还是比较小的，应该没有特别的影响。 【10月境外投资者 加仓人民币债券2.53亿元】据21经济报，10月债券托管量较9月份仅仅增加2.53亿元，创下过去20个月以来的单月增加额最低值。在新兴市场策略师Timothy Ash看来，海外投资机构之所以增持人民币债券的兴趣骤降，一是中美利差持续缩水；二是人民币兑美元汇率走低导致汇率风险对冲操作成本持续增加。不过，随着人民币国际化进程与境内金融市场对外开放步伐双双加快，境外投资机构对人民币债券的投资需求并没有出现明显下降。 （来自新浪财经APP） 中期选举临近，美元的波动性会加剧，这时候中国是一个不错的选择。但这跟国际化有啥关系呢？这些编辑真是啥都能随便联系起来。 中国10月外汇储备 30531亿美元，预期 30585亿美元，前值 30870亿美元。中国10月外汇储备环比减少339亿美元，上月减少227亿美元。 （来自新浪财经APP） 继续减少在将来一段时间仍然是改变不了的。 2018-10-30 【盛松成：央行应该在必要的时候用外汇储备来稳定汇率】央行参事盛松成表示，通过汇率贬值促进出口，这是得不偿失的。因为贬值只能在短期内获得价格优势，但是并不利于企业的转型升级换代，不可能通过贬值企业会越做越大。稳定汇率，一是稳定汇率预期，第二人民银行该出手时就出手，应该在必要的时候用外汇储备来稳定汇率。（每日经济新闻） （来自新浪财经APP） 这解释了未来为啥外汇储备还会持续减少。 【国办印发关于保持基础设施领域补短板力度的指导意见，加快推动一批战略性、标志性重大铁路项目开工建设】以中西部为重点，加快推进高速铁路“八纵八横”主通道项目，拓展区域铁路连接线，进一步完善铁路骨干网络。加快推动一批战略性、标志性重大铁路项目开工建设。推进京津冀、长三角、粤港澳大湾区等地区城际铁路规划建设。加快国土开发性铁路建设。实施一批集疏港铁路、铁路专用线建设和枢纽改造工程。 （来自新浪财经APP） 新兴国家还是喜欢建建建~ 【新华社：央行下周三在港发行票据 支持离岸人民币业务】香港金融管理局31日公布，中国人民银行将在11月7日通过金管局债务工具中央结算系统发行200亿元人民币央行票据，支持香港离岸人民币业务发展。金管局总裁陈德霖表示，中国人民银行在港发行票据可进一步满足市场需求，为金融机构开发人民币产品提供更好的基础。香港特区政府财政司司长陈茂波31日表示，中国人民银行在港发行人民币票据，有助于支持香港作为全球离岸人民币业务枢纽的地位，进一步推动香港债务市场的发展。 （来自新浪财经APP） 这恐怕主要目的还是控制汇率。 2018-11-05 【外汇局：前三季度我国经常账户逆差128亿美元】外汇局公布数据显示，按美元计值，2018年三季度，我国经常账户顺差160亿美元，其中，货物贸易顺差1008亿美元，服务贸易逆差822亿美元。资本和金融账户逆差160亿美元，其中，资本账户逆差2亿美元，非储备性质的金融账户逆差188亿美元，储备资产减少30亿美元。前三季度，我国经常账户逆差128亿美元，其中，货物贸易顺差2561亿美元，服务贸易逆差2295亿美元。资本和金融账户顺差625亿美元，其中，资本账户逆差4亿美元，非储备性质的金融账户顺差1100亿美元，储备资产增加471亿美元。 （来自新浪财经APP） 经常账户逆差将成为一个常态。货物贸易顺差越来越难弥补服务贸易逆差以及资本外逃的影响。不过随着美联储措辞上的放宽，2019年的形势变得不太明朗，对资本外逃有一定的缓解。 2018-10-26 【刘元春：进一步降准应提上议事日程】中国人民大学副校长刘元春接受采访时表示，未来一段时间，我国的房地产金融风险、汇率风险、局部地区政府债务风险、流动性风险等需重点防范。刘元春建议从优化社会融资结构、提升直接融资占比、抑制政府信用和类政府信用过度膨胀等方面化解风险、提振民营企业信心。货币政策方面，建议适度提高M2增长速度，降准应提上议事日程。（中证报） （来自新浪财经APP） 外汇储备持续下降，降准是应对之策。但汇率恐怕就要let it go了。 【为避开美国制裁，欧盟伊朗将建立易货体系】路透报道援引3位欧盟外交官的话称，通过这个被称为“特殊目的实体(SPV)”的体系，欧洲在购买伊朗石油后可以用等价的商品和服务支付。但SPV并不能保住所有欧盟和伊朗的生意往来。欧盟外交官表示，该组织想向伊朗表明，尽管面临美国制裁，欧盟在努力保留伊核协议，但也只能以这种速度做到这么多。（环球时报） （来自新浪财经APP） SPV的进展是个非常值得关心的情况。 2018-10-25 【美债收益率大跌，投资者在股市大跌和美国国会中期选举之前寻求避险】周三（10月24日）纽约尾盘，美国10年期基准国债收益率下跌6.41个基点，报3.1035%，美股收盘前一刻曾跌至3.0922%，为10月3日以来盘中最低位；10月9日曾涨至3.2594%，为2011年5月4日以来盘中最高位，当年最高位在2月9日——3.7660%。两年期美债收益率跌4.85个基点，报2.8305%，美股收盘前一刻曾跌至2.8224%；10月22日曾涨至2.9123%、逼近2008年6月25日最高位2.9966%，当年最高位在6月13日——3.1140%。 （来自新浪财经APP） 美债收益大跌对股市是个很好的消息。 2018-10-24 【美联储10个地区联储在9月支持上调贴现利率】美联储发布的贴现利率政策会议纪要显示，在美联储的12个地方联储中，有10家地方联储在9月份支持将贴现利率上调25个基点，至2.75%。明尼阿波利斯联储主席Kashkari反对美联储加息，倾向于将贴现利率维持在当前2.50%的水平不变。纽约联储主席（往往要等到其他地方联储都批准之后才做决定）也没有投票支持加息。许多地方联储主席报告称，美国消费者开支强劲，零售增长；部分联储主管则指出，制造业增速强劲。许多地方联储主席称薪资上涨，（企业）输入成本上升，或者这两方面都推高终端商品价格。大多数地方联储主席担忧贸易政策会影响到特定行业的活动、并更大范围地影响美国经济的增速。 （来自新浪财经APP） 未来预料加息会逐步进行，目前美国经济并未遇到严峻的问题。 2018-10-21 【广州楼市“放开”限价 刚需高首付时代结束？真相来了】根据传出来的内容，一句话概括：从广东南沙、增城、花都开始，即将取消限价；未来逐步扩大实施范围。中介、销售将其解读为价格上涨的利好。自媒体解读为政府救市，打响限价松动第一枪。随后，广州住建委紧急回应，发布《关于我市进一步规范房地产市场管理作出说明》，强调严禁开发企业拆分价格报备，并将对新建商品住房预售和网签价格指导机制进行优化。（中国基金报） （来自新浪财经APP） 如果这是真的，那么其影响不容小觑。如果未来十年中国仍然把经济增长赌在房地产，那么将造成极其恶劣的影响。最近看到很多减税的消息，等到什么时候能开始缓慢加息，我才认为快到底了。 【美股回调给苹果提供了重要的买入机会】 美股回调的一个很好的副作用是它们提供了买入机会，这意味着一些华尔街最大的股票出现了一些有吸引力的价格波动。苹果在2018年一直是一只耀眼的股票，自今年1月以来总回报率上涨了近32%。与标普500指数其他成分股约5.96%的总回报率相比，苹果的出色表现更加令人印象深刻。更妙的是，在上周的回调之后，正面临一个重大的买入机会。（TheStreet） （来自新浪财经APP） 这种论调还在的情况下，vix必然会下降吧，又会有一次好的做空机会。 2018-10-18 中国9月外汇占款余额减少1,193.95亿元人民币，至21.4万亿元，降幅较上月明显扩大。 （来自新浪财经APP） 这说明中国的经常账户估计还是要负。 2018-10-17 【中国连续3个月抛售美债 持仓规模创去年6月来新低】当地时间10月16日美国财政部公布的数据显示，8月中国所持美国国债规模连续3个月下跌，环比减少59亿美元至1.1651万亿美元，创下去年6月以来的新低。报告显示，日本8月减持56亿美元至1.0299万亿美元，为2011年10月以来新低，但仍仅次于中国为美债第二大持有国。报告还显示，巴西和沙特的8月增持规模分列前两位。巴西成为最大的买家，8月巴西持仓环比增长181亿美元，晋升美债第三大海外持有地区。（澎湃） （来自新浪财经APP） 我所理解的基本逻辑：中国抛售美债 -&gt; 美债收益率上涨；中国获得美元；市场上美元变少 -&gt; 美元走强；美债对全球美元的吸引力加大 -&gt; 中国货币贬值。但我还是觉得这个逻辑里有不对的地方，进一步观察和学习，看看这些事件的相关性吧。 2018-10-12 按照这篇文章的观点：于是乎，有一部分对冲基金竟然另辟蹊径，选择了中国等高等级新兴市场的国债来避险，认为中国国债被下调评级的风险比较小，而且中国现阶段的货币政策至少比美国松得多，国债不会承压。还有，中国作为新兴市场，国债收益率还是较高的。这就是为什么昨天下午开始有大量外资涌入境内申购人民币债券，这波资金小幅推涨了人民币汇率。 这个还是有些不好理解，毕竟目前美元十年国债收益率与中国相比已经比较有吸引力了，这时候“另辟蹊径”还是有一些风险的吧。 2018-10-09 【全球债市再遭大幅抛售 美国10年期国债收益率续创七年新高】全球债市再遭大幅抛售，意大利30期国债收益率涨破4%关口，10年期国债收益率涨近11个基点，均创四年新高；美国10年期国债收益率一度升破3.26%关口，续创七年新高。德国10年期国债收益率转涨近3个基点，逼近逾4个月高位。 （来自新浪财经APP） 原来收益率攀升是出于国债的大批抛售。但这会进一步导致美元走强？我还是理解不到位。 2018-10-07 中国9月外汇储备30870.2亿美元，预期31050亿美元，前值31097.2亿美元。中国9月外汇储备环比减少227亿美元，上月减少82.3亿美元。中国9月末外汇储备2.212万亿SDR，8月末为2.219万亿SDR。 （来自新浪财经APP） 在加息的预期下，外汇储备减少果然超预期了。按照分析加息对应的外汇流出约1000亿美金，所以对应降准1个百分点刚好对冲这部分消失的货币。 【地方新增专项债发行全面提速 规模已达万亿】在刚刚过去的9月，地方政府专项债发行全面提速。数据显示，截至9月24日，地方政府专项债9月以来发行量已达约5000亿。此前，8月地方政府新增专项债发行规模约4200亿元，为7月发行量的3倍多。业内预计，8、9两个月新增专项债发行规模达到约万亿，全年剩下不到3000亿的额度将在10月全部发完。（经济参考报） （来自新浪财经APP） 原来是影子银行破灭以后，万亿的债务不过是小case。 2018-10-06 【高盛：股票回购将在2019年成为标普500指数成分股企业现金利用的一大部分】高盛表示，标普500指数成分股公司将在2019年将现金支出增加13%至3万亿美元，股票回购预计将再次成为他们现金支出中庞大的部分。高盛所预计的2019年现金支出中，标普500指数成分股公司将配置51%的资金用于投资增长，包括资本支出、研发和现金收购，49%的资金将通过回购和派息的方式向股东返还现金。2018年，标普500指数成分股公司现金支出增幅为19%，自2011年以来最大增幅。 （来自新浪财经APP） 不知道加息会什么时候反转这个趋势。 Refinitiv数据显示：美国10年期、五年期、以及七年期国债收益率均创2月份以来最大单周涨幅。两年期美债收益率创最近三周最大单周涨幅。30年期美债收益率创2016年11月份以来最大单周涨幅。三年期美债收益率创4月份以来最大单周涨幅。 （来自新浪财经APP） 关注这个变化。 2018-10-05 美联储：调查结果发现，金融杠杆利用情况几无变动。如果美债收益率曲线出现倒挂，融资条件将会收紧。 （来自新浪财经APP） 没有去杠杆… 【美元兑新兴市场货币在周四升值】据外媒，由于投资者越来越关注美国政府债券收益率上升，周四美元兑新兴市场货币上涨。美元兑土耳其里拉上涨2.4%，兑南非兰特上涨1.7%。兑阿根廷比索也升值2.4%。美国收益率的上升令固收多头的新兴市场资产吸引力减弱。10年期美国国债收益率是全球风险情绪的领头羊，最近收于3.183%，为2011年7月以来的最高水平。投资者将在周五的非农报告中找到关于美国经济增长前景的全新线索。 （来自新浪财经APP） 新兴市场会再受一波攻击。 【为什么美国中期选举将提振股票?】据华尔街日报报道，无论美国中期选举的结果如何，美股投资者都会赢得胜利。即使总统所在的党在国会中失去席位，但历史上股票往往会上涨：事实上，中期选举之后的一年往往是总统四年任期内最好的一年。其中一个原因是，总统的政党通常会失去部分权力，然后推动旨在促进经济发展的立法。根据Strategas Securities的数据，自1946年以来，无论选举结果如何，标普500指数平均上涨了15%，其平均年增长率为8.8%。 （来自新浪财经APP） 这个理由很有趣。拭目以待，如果再次能提振股价，那么也就会造就一个完美的做空机会。 2018-10-04 【长期美债收益率飙涨30年期美债收益率将近十年来首次站上3%，美国经济数据向好支持美联储可预见的未来加息的可能性】周三（10月3日）纽约尾盘，美国10年期基准国债收益率上涨11.82个基点，报3.1813%，美股盘后一度涨至3.1851%，逼近2011年7月1日盘中高位3.2206%、当年最高位在2月9日——3.7660%。两年期美债收益率涨6.11个基点，报2.8720%，逼近2008年6月25日高位2.9966%、当年6月13日曾涨至3.1140%——为当年最高位。 （来自新浪财经APP） 这是为什么呢？我还没找到原因，为什么最近一个月会上涨这么快。 美联储主席鲍威尔：对美国经济非常高兴。美国失业率处于最近20年最低位。美国通胀恰好处于美联储2%的目标。美国经济形势显然偏正面。我们正致力于让经济保持扩张，预计经济扩展银行还将持续相当一段时间。菲利普斯曲线尚未消失，而可能保持静止状态。渐进式加息确保通胀保持温和。预计薪资将继续渐进式增长。 （来自新浪财经APP） 美联储关注的点始终在通胀，以及为下次危机准备弹药（加息）上。 2018-09-26 中国经济2018 — For if the dusking day declined 这可能是2018年中国经济总结的最有干货的文章了，读完这篇基本就能理解很多逻辑了。btw，我还没读完Dalio的书…惭愧 【中证报头版：OMO利率上调既无机会也没条件】美联储加息，人民银行会否上调公开市场操作（OMO）利率?答案是：不会！一则，周内已不大可能再开展公开市场操作，也就没有调整利率的机会。二则，当前中美经济与政策周期不同步，我国货币政策不具备转向基础，OMO利率与市场利率价差很小，后续“追加调整”的可能性同样很小。 （来自新浪财经APP） 肯定不会。 2018-09-24 【港元1个月期Hibor创2008年以来最大涨幅】在周二假期前夕，港元1天期Hibor攀升2个百分点至3.85286%；1周期港元HIBOR大涨1.4个百分点，创2008年9月以来最大上涨；港元1个月期Hibor大涨28个基点至2.16929%，创2008年12月以来最大涨幅。 （来自新浪财经APP） 央行不敢加息，只能靠动动流动性来调整汇率了。 2018-09-22 【印度股市因金融股暴跌而剧烈波动】周五，印度股市剧烈波动，随着Yes Bank和Dewan Housing Finance暴跌，印度金融类股纷纷抛售。Yes Bank股价跌至2016年以来最低水平，而Dewan下跌43%，创下历史最大跌幅。标普BSE Sensex支出一度上涨1%，后又跌3%，创逾4年来最大单日跌幅，收盘下跌0.8%。周五的暴跌显示，在Infrastructure Leasing &amp; financial Services 最近违约动摇了印度金融类股的信心之后，投资者仍对印度金融股感到不安。 （来自新浪财经APP） 原本我以为印度是地缘政治的幸存者，但仔细一看发现这家伙的金融风险并不比中国低，反倒高不少。所以印度股市的繁荣也不过是一种暂时的现象而已。 【3788.85亿元 本周地方债发行额创历史新高】据Wind数据，本周有14省/自治区/市发行了地方政府债券，实际发行总额为3788.85亿元，超过2015年11月2日至8日当周的3627.31亿元，创下我国历史上地方债单周发行规模的新纪录。算上国债，则本周政府债券发行总额为4788.85亿元，为近1年新高。（中证网） （来自新浪财经APP） 如果地方政府没有中央兜底，估计早就穿了。 【EPFR：全球股票基金单周大幅“吸金”138亿美元 全球货币基金“失血”270亿美元】全球资金流向监测机构EPFR于22日提供的数据显示，在截至9月19日的当周，全球股票型基金吸引138亿美元资金净流入，全球债券型基金吸引7.33亿美元资金净流入，全球货币型基金则出现270亿美元的资金净流出。（中证网） （来自新浪财经APP） 股市和货基应该是好基友的。。为啥呢？因为货基收益高意味着贸易繁荣，对应股市走高。现在这俩背道而驰，感觉是要出事。 2018-09-19 【李克强对侵犯知识产权者发重声】李克强总理9月19日在2018年天津夏季达沃斯论坛开幕致辞中表示，中国近年对外支付的知识产权使用费位居世界前列。中国政府坚决依法保护知识产权。这不仅是履行国际规则，也是中国创新发展的内在需要。李克强说，中国将实施更加严格的知识产权保护制度，对侵害中外知识产权的行为坚决依法打击，加倍惩罚。让侵权者付出难以承受的代价，让创新者放心大胆去创造。 （来自新浪财经APP） 当不再有拼多多们的时候才是真正重视知识产权的时候。 【李克强阐释中国为什么主动扩大开放】李克强总理9月19日在2018年天津夏季达沃斯论坛开幕致辞中表示，中国将以更大力度扩大开放，这是我们作出的自主选择：既可以促进国内产业转型升级，也可以给国内广大消费者更多选择机会，同时，这也是用实际行动维护经济全球化和贸易自由化的规则。总理说，今年以来中国大幅放宽了包括服务业特别是金融业在内的市场准入，这些政策正在加快落实。清理进口环节不合理收费。在前期分批次降低药品、部分日用消费品等进口关税的基础上，进一步降低部分商品进口关税。 （来自新浪财经APP） 中国已开放，静候华尔街的响应。 投研帮：小散到底如何抵御即将到来的滞胀寒冬？ 里面提到了上海出口集装箱运价指数，类似波罗的海干货指数。这个指数可以在这里查到。这个指数反应了中国出口的繁忙程度。 这篇文章还不错，提出了在滞涨的情况下大宗商品是王道。可惜我们并没有什么手段，所以在我看来，小散换点美子可能是唯一能做的了。 土耳其总统埃尔多安：（向在土的美国商界人士表示）我相信，土美战略伙伴关系将在投资和贸易的促进下得到强化。土耳其留意到，商业活动不会受到土方对美贸易措施的伤害。近期形势表明，土耳其经济趋于平衡。土耳其不会在自由市场原则上妥协。 （来自新浪财经APP） 这很有趣，究竟美国商界人士与政府的行为是一体两面，还是说这是独立的行为。土耳其是美国中东战略的关键点，但也是矛盾点。观察未来土耳其的变化挺关键的。 【负债成本上升 银行存款增长持续放缓】近期多项数据显示，人民币存款增速正持续放缓。 包括大行在内，绝大部分存款同比增速维持低位，平均为4.96%。业内人士表示，未来存款增速不会有太大的增长空间，增长持续放缓将是常态，银行资产规模扩张或进入停滞期。（经济参考报） （来自新浪财经APP） 人民并没有足够的钱来储蓄了，这非常正常。 【李克强：下决心进一步开放金融服务业】关于中国金融业开放，国务院总理李克强表示，一国金融业的开放程度，与其发展阶段、经济水平、监管能力密切相关。在保持金融稳定的同时，我们下决心进一步开放金融服务业，全面实施“准入前国民待遇+负面清单”模式，有序推进全牌照开放和全股比开放，目前对银行已放开股比限制，未来保险、证券也将取消股比限制。 （来自新浪财经APP） 中国的开放实际上是不得不的行为。 2018-09-18 【北京最严公积金政策出台 房贷资金不足城市或会跟随】恒大研究院副院长夏磊表示，北京的公积金新政，本质是落实‘租购并举’，鼓励先租后买、先积累再住房消费，客观上也考虑了北京市公积金中心资金不足的实际。北京公积金政策有明显的示范效应，对于一些贷款资金不足的城市，可能会跟随，但不必理解为房地产调控再度收紧。易居研究院智库中心研究总监严跃进认为，北京政策多少是有信号意义的，所以后续不排除全国各地公积金等方面政策还是会有收紧的动作。这或也意味着今年9月份又会有部分城市进入新一轮的调控体系中。（证券日报） （来自新浪财经APP） 本质是公积金不够了…为啥老喜欢掩饰呢？ 美国7月国际资本净流入 522亿美元，前值 1145亿美元。美国7月国际资本净流入 +522亿美元，前值 +1145亿美元修正为 +1897亿美元。美国7月长期资本净流入 748亿美元，前值 -365亿美元。美国7月中国所持美国国债减少 77亿美元，至1.17万亿美元，创1月份以来新低。美国7月日本所持美债增加 51亿美元，至1.04万亿美元。 （来自新浪财经APP） 美国资产现在就指着收割全球的美金来提高资产价格。 2018-09-17 【 EPFR数据显示全球股票基金上周大幅“失血” 】资金流向监测机构EPFR最新发布的报告指出，在截至9月12日当周，该机构追踪的全球股票型基金出现55亿美元的资金大幅净流出。全球货币型基金出现高达145亿美元的资金净流出，而全球债券型基金则净流入2.71亿美元。EPFR指出，主要受全球风险偏好一度回落影响，资金大幅流出股票市场，小幅回流债券市场。（中证报） （来自新浪财经APP） 人们似乎一直看多股市，但资金已经在流出了。 【腾讯在港拿下基金牌照！携手高瓴设立高腾国际，进军海外资管业务】由腾讯和高瓴联合战略投资的高腾国际资产管理有限公司，近日获得香港证监会颁发的4号及9号牌照（公募），可在香港或其他合格境外地区设立投资于海外市场的基金，在香港开展公募、私募基金管理和证券投资咨询业务。这意味着，腾讯和高瓴两大巨头联手进军海外资管市场了。（中国证券报） （来自新浪财经APP） 出海不见得是一件好事，而中国企业对国际事务也缺乏实际经验。 【段永基：只给民营企业番号 既不给粮草也不给弹药】四通控股董事长段永基称，“如果国有企业是八路军，至少民营企业应该是新四军，可老把民营企业当忠义救国军，就给番号，既不给粮草，也不给弹药”。他认为这是思想不解放造成的现象，“思想不解放就阻碍了深化改革，希望经济学家在这方面多做一点重量级的呼吁”。 （来自新浪财经APP） 这个比喻很有趣，也很形象。 2018-09-15 【外汇占款结束七连涨 跨境资金流动保持稳定】8月末央行外汇占款21.5万亿元人民币，环比7月份减少23.95亿元人民币，结束七连涨。专家表示，外汇占款小幅减少或是一些扰动因素所致，尚不能说明有资金大幅流出，目前跨境资金流动总体保持基本稳定。未来在美国经济稳步向好、美联储加息、部分新兴经济体动荡以及贸易冲突的情况下，人民币仍有贬值压力，跨境资金流动波动可能会加大。（证券日报） （来自新浪财经APP） 这不是个好的信号…由于关税问题，很多企业已经提前把下半年的贸易给完成了，而外汇占款减少似乎正说明了拐点到来，麻烦麻烦。 【特斯拉被纳入衍生品指数】特斯拉已经被加入面向垃圾评级企业的主要信用违约掉期指数，表明投资者越来越多地利用衍生品来防范特斯拉发生债务违约的风险。摩根大通在2018年4月份上线交易与特斯拉相关的CDS交易，成交量迅速增长，使得该衍生品以3095亿美元净未平仓成为市场上交投最为活跃的1000种交易之一。IHS Markit表示，纳入特斯拉将有助于改善关于CDS指数的“相关性”。新指数将从9月27日开始交易。（英国金融时报） （来自新浪财经APP） 不得不佩服美股的创新能力。 阿根廷央行拍卖2亿美元外汇储备，以控制本币比索下跌；阿根廷央行称：9月19日开始，将把准备金要求提高5个基点。（路透） （来自新浪财经APP） 要想稳汇率就只能牺牲外汇储备了，这不是长久之计。 【任泽平：从8月份经济金融数据可以看出未来中国经济的九大特点和趋势】一、美元强势周期叠加全球贸易摩擦，全球市场动荡。二、国内，金融周期步入下半场，中央坚定去杠杆。三、固定资产投资持续回落，主要受金融去杠杆、财政整顿等影响。四、地产销售投资已于8月前后明显见顶回落。五、制造业投资筑底回升，产能出清新周期得到验证。六、消费持续低迷需要重视。七、抢出口导致出口数据异常大增。八、高度关注民企生存困境。九、中美贸易战往纵深发展。 （来自新浪财经APP） 这个总结感觉是比较深刻和全面的。最近好多需要“共度时艰”的政策出来，作为老百姓，真的要小心自己的钱袋子啊。 【俄德再次力挺“北溪2号”天然气项目】今日俄罗斯14日报道，俄罗斯和德国再次强调“北溪2号”天然气项目的重要性。俄方表示该项目完全是一个商业项目，能加强欧洲的能源安全。德国也是该项目的支持者并希望推动该项目在欧盟进一步实施。今日俄罗斯称，美国一直批评该项目，目的是让欧洲购买美国的液化天然气。（微天下） （来自新浪财经APP） 德国要摆脱美国的控制，就必须能源独立，而靠拢俄罗斯是最好的选择，因为中东、美国是穿一条裤子的。 2018-09-13 【 突发！美国主动邀请中方进行新一轮贸易谈判 】 美国《华尔街日报》刚刚援引消息人士说法称，美国财政部长姆努钦已向中方团队发出邀请，希望中方派出部级代表团在特朗普对华加征新一轮关税前与美方进行贸易谈判，地点将在北京或者华盛顿，时间是“未来几周”。（环球网） （来自新浪财经APP） 按照特朗普的个性，想必在中期之前是没有什么进展的吧。将错就错或许还算有点骨气和颜面。 2018-09-12 【美国飓风和伊朗制裁导致油价大涨】周二纽约市场原油和汽油价格创下6月底来最大涨幅，因飓风“佛罗伦斯”威胁美国东海岸燃油市场并且制裁开始限制伊朗的石油出口。隔夜纽约商业交易所10月份交割的西德克萨斯中质油合约大涨2.5%，报69.25美元/桶；伦敦ICE欧洲期货交易所11月交割的布伦特合约上涨1.69美元，收于79.06美元。 （来自新浪财经APP） 美国的飓风果然与油价有着紧密的关联。 2018-09-11 HIBOR近期走势 source: tradingeconomics.com 从这些情况来看，可以显著发现货币的流动性在收缩。 2018-09-07 美国10年期国债收益率触及2.926%，为8月10日来最高。 （来自新浪财经APP） 观察近期10年国债收益率的变化，自从5月17到达3.1%的高峰后随即处于以波动的状态。感觉全球的美金在一波波流入美国，而期间股市一直处于上涨的状态，这种波动还能持续多久呢？等到下一次收益率突破3%的时候需要观察。 【上市公司“炒房”首破万亿：7公司上半年炒房赚超亿元】统计显示，今年上半年持有投资性房地产的A股上市公司数量及合计持有金额双双创历史新高，1680家上市公司（仅统计报告期前上市的企业），合计持有投资性房地产首破万亿元，达到10477.12亿元。此外，这是A股公司持有投资性房地产连续第9个季度环比增。而就在上半年，有7家公司通过投资性房地产赚超亿元，同期没有公司亏损过亿，这一定程度上释放出房价仍处于上涨态势之中。（证券时报） （来自新浪财经APP） 操蛋的A股企业，A股不值得投资。 【马斯克剖析缘何不喜欢Instagram，以及社交媒体对心理健康带来的负面影响】据Bisinessinsider报道，马斯克在电视节目“The Joe Rogan Experience”有声杂志中现身，并讨论了他认为Instagram之类的社交媒体并不是真实生活的反映的原因。马斯克表示，有一些看起来很快乐的人其实现实生活中是最悲伤的。在将近3个小时的节目中，马斯克与喜剧演员Joe Rogan谈论了有关事业，人工智能，虚拟现实等话题。马斯克认为，社交媒体的一个问题是，人们看起来比实际生活好得多。人们会上传自己开心时的照片，会美化照片或选择最好的光线，最好的角度使自己看起来更漂亮，这可能会对粉丝的心理产生负面影响。尽管马斯克是重度推特使用者，但他对社交媒体的整体评价并不好。 （来自新浪财经APP） 马斯克陷入了糟糕的境地。 2018-09-01 From Why China Tech Stocks Are Falling Behind U.S. Peers - BloombergWhile U.S. companies like Amazon and Netflix Inc. still believe in entering new markets on their own, China Inc. seems to have decided that a strategic investment in a startup is a more attractive option. The Chinese firms may be taking their cue from long-time patrons: Small investments two decades ago by South Africa’s Naspers Ltd. and Japan’s SoftBank Group Corp. became $130 billion-plus stakes in Tencent and Alibaba, respectively. 这个看法与我一致。国外的公司在投资和收购上往往围绕其核心业务，试图扩大自己的护城河。中国的科技企业往往四面出击，比如阿里、腾讯、网易、京东等等，尤其还有所谓“没有边界”的王兴。似乎人人都是顶尖的战略家，有着不凡的布局能力。让时间来考验他们吧。 From HNA’s Debt Declines for First Time, Shrinking by $8.3 Billion - BloombergTotal debt fell 9.5 percent to 541.6 billion yuan ($79 billion) at the end of June, down about $8.3 billion from the record high set at the end of last year, according to figures derived from a half-year report dated Friday. Prior to this, HNA’s debt had always risen, based on public data stretching back to 2005. 难怪要出“翻墙死”了…安邦、万达、海航，债务三巨头，可惜生不逢时啊！ 2018-08-31 特朗普：许多人认为谷歌、Facebook与亚马逊存在反托拉斯问题。拒绝置评谷歌等是否应当被分拆。注：据CNN报道，前白宫首席策略师 Steve Bannon此前称，大型科技股必须被拆分。 （来自新浪财经APP） 如果真正遭遇反垄断，那么Google、FB和Amazon够喝一壶的了… 特朗普威胁称若WTO表现不好，美国可能退出。注：自2017年入主白宫以来，特朗普政府至少已经退出了联合国教科文组织、TPP、巴黎气候协议等多个地区乃至国际组织。 （来自新浪财经APP） 特朗普不遗余力制造波动性。 【特朗普表示考虑将资本利得税与通胀挂钩】据外媒报道，今日美国总统特朗普表示考虑通过发布资本利得税与通胀挂钩的规定以实现资本利得税的减免。资本利得的变化将通过调整通货膨胀的原始购买价格来降低投资者在出售股票或房地产等资产时的税单。 该变动一直是美国国家经济顾问库德洛的长期目标，他认为此项措施将会刺激就业创造与经济发展，因为人们不会因虚幻的收入而缴税。该议题自7月以来备受关注，美国财长努钦当时表示财政部门寻求绕过国会，发布允许资本利得与通胀挂钩的规定。 （来自新浪财经APP） 特朗普的团队解决问题的方式总是那么“奇葩”，不是蠢就是坏。 【美国证券监管部门希望更多的投资者参与私人交易】美国证券交易委员会（SEC）主席Clayton表示，SEC希望个人投资者更加容易投资私人公司，包括部分全球最火热的投资，而许多人并不能接触到这些投资。SEC希望采取措施，让更多的个人投资者对多年来一直避免上市的公司进行抨击，这些公司包括优步、Airbnb，它们一直避开公开市场而支持像投机资本家的私人投资者。SEC现在考虑对规则进行大修，以保护那些经验不足的投资者，为他们开辟新的选项。SEC计划在未来数月发表一篇名为“概念发布”的论文，将征求公众对如何改革融资流程的意见，包括通过扩大私人股本销售渠道。（华尔街日报） （来自新浪财经APP） 这真是个好的政策，希望中国也会有这种机会。 “股神”巴菲特：相比30年期国债和其他固定收益资产，股票仍然更具吸引力。对股票持乐观态度，今天早上还在买股票。6月以来，买入“略微”更多的苹果股票。 不要关注苹果的季度销量。苹果公司正真的价值在于客户与苹果公司深切的联系。 iPhone的用处对于人们来说被极大低估。 （来自新浪财经APP） 显然目前的国债收益率不够理想。 2018-08-25 【央行昨放水逾千亿元 缓冲地方债发行冲击】昨日，中国人民银行开展中期借贷便利（MLF）操作1490亿元。中信证券固定收益首席分析师明明表示，“在流动性仍然宽松的环境下，央行继续开展流动性净投放，维持资金利率低位的意图明显。”在明明看来，本次MLF操作的目标除对冲逆回购到期因素外，近期地方政府专项债券加速发行对流动性的冲击也得到央行的关注，通过中长期流动性投放，宽松的货币政策配合积极的财政政策，缓冲地方债发行的市场冲击。（证券日报） （来自新浪财经APP） 是啊，这样的情况下还如何加息呢？搬石头砸自己的脚？那么汇率该怎么收场呢？ 2018-08-24 【城投债面临万亿级到期洪峰】彭博数据显示，自今年三季度起，在岸城投债到期规模将会连四个季度超过3000亿元，季度均值达到3436亿元，较2017年季度平均水平增长约40%，并在明年一季度达到记录峰值。城投债存量规模里，AA及以下评级占到了四分之一。 （来自新浪财经APP） 城投债的违约已经是央行的信号了。 【华尔街大空头:美股不久就会现原形 企业债蕴藏杀机】Gluskin Sheff首席经济学家David Rosenberg认为美股繁荣存在人为支撑。不过随着全球央行收紧政策，他预期一切可能会现出原形。他周三（8月22日）称，经济扩张处于历史上最低迷时期，牛市却打破了历史纪录。他在7月末曾预测信用利差走阔可能会终结牛市，而现在他仍坚持这个观点。 （来自新浪财经APP） 小心观望。 【与美国唱反调！欧盟同意向伊朗提供1800万欧的援助】美国总统国家安全事务助理博尔顿22日刚刚表示，美国不打算停止并可能采取其他措施向伊朗施压。一天后，欧盟就高调与美国唱起了反调。欧盟周四（23日）同意向伊朗提供1800万欧元（约合2060万美元）的援助，其中包括对伊朗私营部门进行援助，以帮助抵消美国对其制裁的影响，挽救2015年达成的伊核协议。路透报道称，该声明反映出，欧盟对美国总统特朗普今年5月放弃的伊核协议进行了高调支持。（环球网） （来自新浪财经APP） 有趣的事情，二战过去也已经很多年了，欧洲终于要开始硬气了？ 【加拿大帝国商业银行：预计鲍威尔演讲影响较小 立场可能偏鹰派】今年的杰克逊霍尔年会将是鲍威尔作为美联储主席的首秀，他的演讲对市场影响可能是有限的，讲话立场的鹰派可能性大于鸽派，原因有三。首先，联邦基金利率仍低于2.75%（美联储官员认为的中性利率区间上限），且10年期美债收益率仍低于3%，没有必要在此时打压对利率的预期。其次，近期“挑战”美联储的阵营主要是担忧美联储加息过多、过快。第三，在美国总统特朗普已经对美联储加息表示不满的情况下，如果鲍威尔要维护美联储的独立性，那此时就绝不应该转向更鸽派立场，以免让人觉得他向特朗普屈服。鲍威尔的鹰派立场可能推升短期美债收益率，且短暂温和地拉升美元。 （来自新浪财经APP） 目前美元观察到停止加息的信号。 【特朗普：在美墨边境建墙已经花了35亿美元 】美国总统特朗普在接受福克斯新闻频道采访时说：“我们正在修建隔离墙。施工已经开始，已经耗资35亿美元，我们希望今年获得50亿美元的预算拨款。”特朗普表示，他希望工程进度能快一些，但与民主党人打交道很困难。7月下旬，特朗普在美国佛州坦帕与支持者见面时表示，美墨边境隔离墙的修建工作已经开始。 （来自新浪财经APP） 特朗普是个有趣的人。 2018-08-23 【两万亿地方债来袭 市场呼吁央行仍需积极注水以维系宽松环境】据彭博，中金、中债资信等机构分析师均预计，8-9月间将有约两万亿地方债供给，将是去年同期发行规模的2.4倍。8月迄今已发行超7000亿元，至少是约一年高位。市场人士指出，从维稳资金面、维持低利率角度考虑，央行后续需加码宽松，可选政策选项包括继续大量投放MLF和降准。 （来自新浪财经APP） 中国金融如果出现问题，必然在地方债和国企债中爆发出来。 美联储会议纪要：利率更加接近中性利率；通胀中期内稳定在2%附近；主席暗示资产负债表将继续下降；部分成员认为财政面临上行风险，部分官员认为，强大的经济动能属于上行风险；许多美联储官员预计下次加息可能很快到来时合适的；主席鲍威尔暗示，将在秋季继续讨论资产负债表问题； 许多委员指出，在“不太遥远的未来”可能适合在谈到货币政策立场时不再用宽松一词； 普遍预期美国GDP增速会在下半年放缓， 但仍高于趋势水平；官员们讨论了收益率曲线趋平的影响。美联储官员讨论了银行逆周期资本缓冲。 （来自新浪财经APP） 加息估计正常进行。 【于学军：中国经济以2008年为界大致可划分为两个阶段】 中国银监会国有重点金融机构监事会主席于学军在中国银行业发展论坛表示，在2008年金融危机发生之前，中国保持了巨大的比较成本优势。从2014年二季度开始，人民币出现贬值压力，外汇储备增长放缓，随后又出现趋势性减少。政府出台更多刺激性政策拉动经济增长，主要手段是投资，尤其是公益性的基础设施投资。增加投资的背后是货币性的大量投放，也就是所谓“放水”行动，其结果必然造成货币超发的局面，使资产泡沫化更加严重，宏观杠杆率水平快速上升。人民币的增持购买力大幅缩水，从而面临着对外贬值、对内通胀压力加大这样一种局面。 （来自新浪财经APP） 这个评价比较中肯。 2018-08-22 【美国2000亿关税听证会 首日61人发言只3人赞同】听证会首日的情形也证明了美国业界的焦虑之情。据在听证会现场的记者核查，61个发言人共被分成八组，其背景涵盖了箱包、服装、食品加工到半导体、自行车、化工等多个产业，赞同征税者寥寥，仅有3位。（一财） （来自新浪财经APP） 如果哪天突然说贸易战不打了，会带来什么影响呢？可能股市会涨几天吧，仅此而已。而且，随后要观察究竟哪些领域的对外开放真正意义上加大了。 2018-08-20 【264只个股破净 估值底能否成为市场底】上周A股市场继续寻底，成交继续缩量，上证综指周跌幅为4.52%，8月17日盘中最低探至2666点创7月初以来新低，逼近2016年1月底的2638点。在此情势下，市场上出现了估值底、政策底、资金底和情绪底等各种“底部论”，而本周策略报告中，各大券商探讨最多莫过于估值底。其中，天风证券表示，较低的估值能提供一定安全边际，但不能作为见底的充分条件；而海通证券认为，A股处于第五次历史大底磨底中，换手率低至143%，情绪扭转至少需两大信号之一：一是去杠杆拐点，二是改革加速。（证券时报） （来自新浪财经APP） 如果你的公司净值100亿，可现在市场对其估值90亿，那你会怎么办？当然是借钱也要买自己的股票！但现在的全年回购仅600多亿，去年一年的回购87亿，想想不觉得好笑么？可见估值的水分之大，难以评估。 2018-08-19 鸿学院有关土耳其问题的讨论。 在2018年3月就发生了刘鹤访问美国的事件，但没有达到化解“贸易战”的效果。后来，6月美国商务部长罗斯访问中国，仍然没有达成实质性的结果。在3月到6月期间，中方有意无意的发布了一系列的消息和对外开放政策，外界解释为中方对美方示好，尤其是中国的负面清单里在一些重要领域选择了对外开放，但美方一直表现出强硬的态度，包括两轮已经生效的加税。8月底，中国应邀派出商务部副部长赴美谈判。 宋鸿兵认为这次访问有几大特点： 1. 美国邀请 2. 低调开展 3. 低层次沟通 可能是实质性的沟通。自古以来，政治决定很少是在镁光灯聚焦之下完成的。因为人都是好面子的，而谈判必然存在冲突，所以在围观群众面前，大家都更加希望维护自己的面子，而很难达成实质性的妥协。接下来，需要密切关注本轮谈判释放出来的消息。 从商人的角度，希望达到最大的权益。如果真弄得老死不相往来，本质上丢掉了本该能获得的利益，这是不符合其根本利益的。我观察到，本次冲突中，由于民粹主义抬头，制造业回流就成了一个重要的目标。所以，特朗普的恐吓并非完全站在商业帝国的立场，最终两方利益肯定会达成一个折中。 宋鸿兵认为贸易战中国受到的影响有限，其依据是关税受伤的主体是美国，而中国由于出口只占31%，所受的影响有限。另外，美元必须长期逆差才能做到影响全球金融的目的，所以所谓的贸易顺差与华尔街利益背道而驰，必然无法真正达到顺差的状态。我同意这种看法，中国最大的问题还是来自内部而不是外部，这方面的讨论会另外开一个页面。贸易战对此而言只是一种舆论上的导火索。 需要理解冲突的本质是利益，从而关键在于谈判的底牌在哪里。 土耳其是以色列和美国国家利益派的矛盾体现。以色列希望以库尔德人独立来达到快速颠覆伊朗政权的目的，而美国希望通过经济制裁来缓慢达到目的。但以色列显然在美国更占优势，所以土耳其在这个问题上无法容忍，必然会激烈反抗美国。土耳其在伊朗问题、欧洲难民问题、借债问题上与欧洲利益一致，所以美国的压力会促使土耳其与欧洲和俄罗斯走的更近。 7月结售汇由顺差转逆差 远期售汇额同比增3.7倍 美元流出的预期难以简单消除。 2018-08-18 周五（8月17日）纽约尾盘，美国10年期基准国债收益率下跌0.55个基点，报2.8605%，本周累跌1.27个基点；5月18日曾涨至3.1261%，逼近2011年7月8日盘中高位3.1805%。两年期美债收益率跌1.05个基点，报2.6059%，本周累涨0.47个基点；7月26日曾涨至2.6856%，逼近2008年7月24日高位2.8309%。 （来自新浪财经APP） 美国国债最近一直在下跌，股市却在上涨，而且货币收紧的预期并没有减弱，那么这些美元是从哪里来的呢？想必对新兴市场的抽血还没有停止。 【专家预计央行9月份加息概率大】中信证券固定收益首席分析师明明表示，6月份央行出于内部环境稳增长和防风险的考虑没有跟随美联储加息，面对美联储9月份的加息，出于保汇率和稳经济的考虑，央行大概率还是会选择跟随加息。（证券日报） （来自新浪财经APP） 个人看法与之相反。 美国证券交易委员会（SEC）：采用简化和更新上市企业信息披露要求的修正提议。修正案将在联邦登记处公告30天之后生效。 （来自新浪财经APP） 的确有利于企业开展经营活动。 2018-08-14 【消费品市场上半年增速放缓】中国商业联合会和中华全国商业信息中心联合发布了今年上半年消费品市场运行情况，上半年消费品市场增长9.4%，回落幅度有所加大；其中网上实物商品零售额增速同比增长29.8%，增速有所加快。消费品市场增速放缓幅度加大原因包括：居民收入增速放缓，2018年上半年，全国人均可支配收入同比增长8.7%，实际增长6.6%，分别比2017年同期回落0.1个和0.7个百分点；社会消费品零售总额基数已非常巨大等。（北京商报） （来自新浪财经APP） 可支配收入增速和消费增速回落是一致的，问题变得越来越困难。 【中资银行7月份公司债减持幅度创17个月之最】尽管央行近期出台的宽松措施鼓励银行增持公司债，但是国内的商业银行，为应对海量地方政府债发行，7月份大幅减持了公司债。据彭博援引中国债券信息网和上海清算所数据，国内商业银行的商业票据、中期票据及信用债持有量7月份环比减少670亿元人民币（97亿美元）至1.8万亿元，创出了2017年2月以来的最大减幅。 （来自新浪财经APP） 由于银行本质上受制于政府，所以最终必然会增持公司债。既然这些债务并不“可口”，那么政府还需要对应的机构来帮助银行消化。借鉴美国次贷危机的经验，这种消化应该就是逆回购这些债务，然后将其证券化。且看事情是不是如此发展下去。 2018-08-13 【货币基金7日年化收益率降至3.4% 短期内下行趋势可能仍将延续】受到货币市场利率下行影响，8月份以来，货币基金收益率出现回落，货币基金7日年化收益率平均3.4%水平，较6、7月份下滑了近0.5个百分点。“余额宝”等理财宝宝的收益率也在走低。如天弘余额宝8月10日的7日年化收益率为3.34%，与腾讯理财通合作的货币基金收益率也较前期走低，基本都下滑至3.5%左右。另有人士认为，货币基金收益率中短期内的下行趋势可能仍将延续，而“互联网宝宝”的收益走势也不乐观，继续下降的概率较大。(证券时报) （来自新浪财经APP） 遇到这种情况，先看M2的增速。 【经济参考报头版评论：中国经济发展不会再走也不可以再走“经济房地产化”老路】房地产去杠杆需要打消民众对房价上涨的预期。如果大部分人都认为未来房价将横盘或者回调，那么加杠杆的动力才会减弱。加速长效机制落地，释放投资者、投机者持有成本增加信号，让投资人群有所畏忌，促使房地产真正回归只住不炒已时不我待。中国经济不能再走“老路”，房地产调控也不能再走“回头路”。降低地方对房地产的依赖，同时有效调整市场预期，促进房地产调控长效机制落地成为房地产调控的关键所在。 （来自新浪财经APP） 增强和打消民众对房价上涨的预期同样是那么痛苦。 2018-08-12 土耳其总统埃尔多安在土耳其与美国关系危机情况下表示，土耳其准备同中国、俄罗斯、伊朗及乌克兰这样的大贸易伙伴国改为本国货币结算。 （来自新浪财经APP） 可能性太小，谁会要你的货币呢？除非这种货币是以石油背书的，而且需要保证土耳其政府不至于崩溃了。 2018-08-10 【阿根廷通胀高企 民众以物换物节省开支】阿根廷央行预计今年本国通胀率可能超过30%。生活成本的升高让民众开始寻找节省开支的办法。比如，不再用钱买东西。在这些物物交换市场上，大都是交换一些柴米油盐的生活用品。人们事先通过社交网络沟通好需求，再来到市集交易。 （来自新浪财经APP） 回到了以物易物的时代。 【专家称输入性通胀有望抬头】摩根士丹利华鑫证券首席经济学家章俊称，今年国内成品油价格已经连续9次上调，7月份汽油和柴油价格同比上涨22.7%和25.1%，合计影响CPI上涨约0.42个百分点。目前来看，全球经济复苏虽然进入尾声，但对能源的需求依然比较旺盛，但供给端受到欧佩克限产、美国原油增产低于预期，以及伊朗等地缘政治风险等因素影响而存在很大的不确定性，因此油价存在较大上行风险。鉴于目前中国原油对外依存度达70%，如果原油价格出现跳升，则输入性通胀可能会再次抬头，成为拉升通胀的重要推手。（证券日报） （来自新浪财经APP） 对我国而言这个“经济复苏虽然进入尾声”不知作何感想。 2018-08-08 【货币利率跌到“地板价” 实体融资困局仍未缓解】2.0%、1.8%、1.6%……进入8月，货币市场隔夜利率继续“断崖式”下跌。上海银行间同业拆放利率隔夜资金价格创下自2015年8月12日以来的历史新低。今年市场利率中枢下行，但实体端的融资利率仍偏高。银行的钱堆积在利率债和货币市场上，没有流入实体经济。”民生银行首席研究员温彬告诉记者。（上证） （来自新浪财经APP） 问题的本质在于实体经济赚不到钱，而且风险在逐渐增加，那么银行就没有意愿把资本借到实体经济中去。有趣的是，其对外表现出来的形势看起来却与货币宽松类似。 【美23日起将对160亿美元进口中国产品征收额外关税】美国贸易代表办公室公布第二轮关税计划，对价值160亿美元的进口中国产品征收25%的额外关税，8月23日生效。据CNN报道，美国贸易代表办公室于美国当地时间周二，公布了一份价值约160亿美元的中国产品清单，并称从8月23日起，将针对清单上产品征收25%的额外关税。该清单包含了6月15日公布的拟议清单中最初的284个关税细目中的279个。（中国日报） （来自新浪财经APP） 继续观察。 【沙特和加拿大争端升级 沙特将抛售加拿大资产】据英国金融时报报道，两位知情人士表示，沙特央行和国家养老基金已指示其海外资产管理的经理，“无论成本如何，都要抛售加拿大股票、债券和现金持有”。高管们表示，据估计，第三方基金经理被授权向全球市场投资逾1000亿美元的沙特基金。一位知情人士表示，尽管投资于加拿大资产的比例“以绝对值计算“相当小，但资产抛售却传递了一个强烈的信号。这轮抛售始于周二，突显出沙特正展示其金融和政治实力，警告外国不要干涉其主权事务。 （来自新浪财经APP） 加拿大在国际上的存在感如此的渺小。 【铁矿石有望突破70美元 中国转向支持经济料刺激需求】新交所AsiaClear期货价格周二上涨至每吨70美元，创下3月以来的最高盘中水平。根据我的钢铁网，在现货市场上，品位62%的基准铁矿石周一触及69.55美元。品位65%的优质矿石表现更好，飙升至96.20美元，为11个月新高。 （来自新浪财经APP） 铁工基的刺激能力有限，不知道还能坚持多久。如果不能同时进行大刀阔斧的利益再分配，实在没有什么真正变革的希望。 2018-08-07 【美国对伊朗重新实施的第一轮制裁生效】美国重启对伊朗金融、金属、矿产、汽车等一系列非能源领域制裁。据报道，此次制裁将涉及伊朗政府购买美元；黄金等贵金属交易；工业用石墨、钢、铝、煤炭和软件；与伊朗货币相关交易；与伊朗政府发行主权债务相关活动；伊朗汽车行业。 （来自新浪财经APP） 美元是美国经济制裁的刺刀。 2018-08-02 【中证报：“六稳”透露调控新信号】中国证券报头版刊文称，中共中央政治局日前召开会议，分析研究当前经济形势，部署下半年经济工作。其中，“稳”是一个关键词，“六稳”即稳就业、稳金融、稳外贸、稳外资、稳投资、稳预期是保持经济平稳健康发展的具体要求，直接反映了稳增长将被放在更加重要位置。在稳增长重要性抬升背景下，政策组合的前瞻性、灵活性、协调性将提高，去杠杆力度和节奏会更加务实。这将有利于从供求两端稳定实体经济的融资需求及增长预期。可以期待的是，监管部门将在实现“六稳”方面出台一些具体政策。 （来自新浪财经APP） 这六个“稳”的顺序很有意思，个人感觉是按照优先级由高到低排列的。首先，稳就业是第一位的，因为就业不稳则社会动荡；然后是稳金融，因为金融业是经济的血液；稳外贸和外资都表明希望外来资本能以稳定的形势保证金融安全，而其中外贸比外资更重要，因为那是劳动所得，而外资流动性和波动性更大；稳投资是在外资无法补足经济增长空缺的情况下，通过本国货币政策来填补空白；最后稳预期就比较虚了，也无法简单理解其内在的含义。 2018-07-26 美国总统特朗普与欧盟委员会主席容克举行联合新闻发布会，特朗普称：与容克关系进入新阶段。我们同意致力于零关税。欧盟将提高购买美国大豆的力度。欧盟将进口更多的美国液化天然气（LNG）。（已经达成的）协议将有利于强化战略能源合作。在谈判进行期间，我们不会违背贸易协议所约定的精神。将启动贸易谈判，化解钢铝关税和报复性关税问题。 （来自新浪财经APP） 美国的霸主地位意味着无论何时他都有机会选择坐下来谈。与欧洲前段时间看似水深火热，但根本上其它的利益还是无法撼动两者合作的凝聚力。接下来恐怕美国对伊朗的压力还会加大，对中国仍然继续保持一种高压的状态。石油想必会上涨，而A股恐怕还是逃不开下跌的命运。 2018-07-25 【特朗普向美农民提供120亿美元紧急援助】商务部网站消息，据《美国之音》7月25日消息，美国农业部长桑尼-珀度称，美国农业部将对受关税影响的美国农牧场主提供120亿美元的支持。这些项目将包括直接向农民付钱、贸促措施以及购买食物。按照《商品贸易公司宪章法》，这些项目已经获得授权，无需国会专门批准。珀度说，这些项目是短期解决方案，以便让特朗普有时间谈判达成贸易协议。 （来自新浪财经APP） 从这样的做法来看，基本上美中之间的贸易战没那么容易结束。中国对外开放农场品市场，悄无声息就把中国农民的利益出让了，但明面上说的冠冕堂皇，什么加大对外开放是基本国策。天天说特朗普不好，但美国农民受损的利益是可以得到实实在在补偿的。 【经参：我国事实上面临着较大的经济下行压力】 一方面，经济走稳的支撑经不住细抠，体现为：上半年房地产投资居高不下，主要是靠拿地，地方政府依旧不改土地财政依赖症；4月起PPI连续第三个月回升，但价格回升主要来自供给收缩并非需求恢复。此外，上半年贸易坚挺但有“抢跑”的成分，基建投资和消费则明显走坏。另一方面，美国挑起的这轮贸易纠纷有愈演愈烈之势。据有关测算，基于中美拟加征关税的规模，将导致我国GDP增速放缓0.1至0.5个百分点。换言之，如果不出台对冲举措，我国GDP增速有可能跌至6%甚至更低。经济高速增长已经不是硬要求，但这并不意味着当前可以容忍经济失速下行。本次国务院常务会议提出“保持经济运行在合理区间”，就是旨在稳增长。具体到经济对策，主要还是财政政策和货币政策双管齐下。 （来自新浪财经APP） 评价相对中肯，下半年面临的最大问题估计是外部贸易战的压力和内部经济继续下行的压力，而真正的难处实际在内部。在经济减速的时候，往往是社会矛盾激化的时候，如何保持国内稳定的局面同时不断深化改革是一件非常非常困难的事情，拭目以待并时刻准备吧。 2018-07-24 【全面宽松基本确认！国常会刚刚一锤定音，不搞大水漫灌强刺激，将影响股市债市】刚刚召开的国务院常务会议一锤定音，释放了全面宽松的信号。金融研究院院长管清友表示，政策面正式变化，但不会再度出现2014那种大水漫灌。中信固收称，货币宽松政策再确认。解读一：积极的财政政策，或利好基建。解读二：货币宽松政策再确认，或迎降准。解读三：扩内需政策明确方向。解读四：保障融资平台合理融资需求，利好城投债。解读五：地方政府专项债发行为基建筹集资金。解读六：开展有效投资，框定多个区域。解读七：股市方面，中信证券债券研究团队认为，宽货币向宽信用转变是利好，未来可能得到宏观数据进一步支撑；利率债目前机会不大。（券商中国） （来自新浪财经APP） 股市请开始你的表演。不过越是真的想解决问题，可能股市压力越大，否则股市房市还是会像过去一样。 2018-07-20 【上交所地方债发行突破7000亿元】随着7月20日安徽省269.5751亿元政府债券成功通过财政部上海证券交易所政府债券发行系统公开招标发行，今年上交所地方债累计发行规模已经突破7000亿元。据了解，在发行常态化的基础上，上交所正在积极推动地方债交易机制完善与地方债产品创新，提升二级市场的流动性，有助于不断提升地方债的投资价值与配置价值，促进地方债市场健康发展。(证券日报) （来自新浪财经APP） 对于债务的流动性，在信息不对等的情况下，一方面是公有资产外流，另一方面是垃圾债务转移。 2018-07-19 日本石油企业考虑暂停进口伊朗石油。日本对伊朗石油进口最早可能在10月份降至零，沙特和阿联酋都名列潜在的替代性供应方。（日经新闻） （来自新浪财经APP） 在基本利益面前，国家之间是没有什么“情谊”的，简单明了。带有感情的外交是幼稚的。这样来看石油价格上涨趋势仍然保持不变。 央行窗口指导银行增配“AA+”以下信用债。（一财） （来自新浪财经APP） 国家开始托债市，否则中小企业的生存就存在明显问题了。中国的信用评级形同虚设，有没有政府兜底才是放贷的最最最重要标准。 2018-07-18 【央行连续释放流动性 专家预计9月份或再降准】继央行16日超预期放水3000亿元之后，17日共释放2400亿元流动性。东方金诚首席宏观分析师王青表示，预计下半年央行还将有1次至2次降准操作，其中9月份可能实施下半年的首次降准。适时降准有助于稳定市场情绪，控制金融体系流动性风险。（证券日报） （来自新浪财经APP） 密切关注央行降准的政策，同时关注美联储的加息政策。 【俄罗斯不再是美国国债的大型持有方】美国5月国际资本净流入 +699亿美元，前值 +1387亿美元修正为 +2331亿美元。美国5月长期资本净流入 +456亿美元，前值 +939亿美元修正为 +940亿美元。中国5月所持美国国债 +12亿美元，至1.18万亿美元。日本5月所持美债 +176亿美元，至1.05万亿美元。 （来自新浪财经APP） 国债规模没有大的变化，说明并没有急于控制汇率。 2018-07-16 【货币政策或有放松空间 未来两个月央行再次降准概率较大】6月社会融资规模增量为1.18万亿元，较上年同期减少5902亿元。人民币贷款增加1.84万亿元，同比多增3054亿元。6月M2货币供应同比增8%，增速创历史新低。业内人士表示，整体来看，6月份信贷放量符合预期，但社融、M2同比均不及预期，表内信贷难以承接表外融资的局面仍在继续，货币仍有进一步放松的空间，未来两个月央行再次降准概率较大。多位业内人士表示，货币结构性宽松趋势有望延续。（经参） （来自新浪财经APP） 在宽松的情况下货币贬值的预期就形成了。 2018-07-15 【社科院张明：房产税的到来可能比预想的还要早】社科院张明于2018国际货币论坛上称，一线城市房价有三种可能：一是不得不放开刚需市场，但由于没有库存，再次暴涨。二是政府会“憋一个大招”，“房产税的到来可能比预想的还要早。”第三种就就是拖，但这不是一个长期的手段。 （来自新浪财经APP） 无疑，应该是第二个选择。 【王国刚：经济降杠杆比较难 甚至稳都稳不住】社科院学部委员王国刚在2018国际货币论坛表示，中国经济去杠杆去不了，降杠杆也比较难，甚至稳都稳不住。王国刚建议，应该优化金融机构和“去杠杆”举措，增加长期负债，减少短期负债，加大长期公司债券发行数量，推进准资本性资金的形成。王国刚建议用负债总额/资产总额计算杠杆率。 （来自新浪财经APP） 有句话叫“解铃还须系铃人”，中国的杠杆是怎么加上去的呢？一方面是在政府、国企、银行之间的默契下加上去的；另一方面则是在美元催动的货币潮水中加上去的。后者已经在通过IPO给自己转杠杆了，这个也不劳国家费心，国家就只要搞好地方债和国有企业债务就好了。最近的定向降准支持债转股，以及允许地方债在二级市场流通都是这个趋势下的结果。其实还有些没存在感的中小民企，入不了大家的法眼，想必日子过得就很不好了。 2018-07-14 面对房地产的调控政策的层层加码，土地市场的火热形势仍然延续。今年上半年，数量众多的三四线城市卖地行情继续向好，成为支撑全国土地出让收入大涨的主要原因。对地方政府而言，一次性卖地收入，是比财政收入更主要的收入来源。仅今年上半年卖地收入就与去年全年地方一般公共预算收入相当（占比超过90%）的城市数量多达14个，绝大部分为非省会的三四线城市。 （来自新浪财经APP） 土地财政最后的疯狂。 2018-07-13 融资不够债来补。 2018-07-11 离岸人民币兑美元快速走低， 连续跌破6.65、6.66、6.67、6.68四道关口 ， 目前报6.6839， 刷新7月6日以来新低。 意料之中。 我国对美国大豆加征25%进口关税，将使美国大豆进口成本增加700—800元/吨，较巴西大豆高300元/吨左右。由于加征关税后失去竞争优势，国内企业将大幅减少美国大豆的采购。实际上，截至6月28日，我国已经3周没有新增采购美国大豆，同期累计取消了61.5万吨美国大豆订单。过去20年，全球大豆贸易增幅的85%来自我国，未来我国需求依然是推进全球大豆贸易增加的主要来源，美国豆农将无缘分享我国大豆需求增长带来的红利。 中国也只能承担涨价带来的生活成本增加。 7月8日、9日的两场上市公司座谈会分别由证监会主席刘士余、副主席阎庆民主持召开。在第一天会议上，共有15家上市公司的代表出席，既有中国人寿、中国建筑、中国石油等央企，也有海螺水泥、三一重工等老牌行业龙头，更有科大讯飞、科陆电子等新型产业的代表。有一些央企或是行业龙头集团企业在发言时，表示自己企业一切平稳，并不存在市场所担忧的行业、资金和股权质押问题。然而有少数民营企业则在发言时直诉痛点，表示在新一轮的市场环境之下，企业已经需要直面生存问题。证监会此次收集的上市公司反馈的意见和建议，不排除后续汇报给上层以推动相关政策落地。（21） 无耻的政治。 7月11日，中国社会科学院财经战略研究院专家邹琳华在经济参考报撰文指出，随着棚改货币化安置比例的下降以及三四线城市加入调控行列，2018年下半年二三四线城市房价总体仍将惯性上涨，但涨速将有所下降，局部房价可能出现微降。部分一线城市止跌企稳，在跷跷板效应的作用下，资金可能重回北京等一线城市。北京等一线城市房价有再度上涨的风险，房价调控压力将增大。 言之有理。 特朗普：刚刚与辉瑞首席执行官讨论了我们的药品定价。辉瑞正在抑制价格上涨，因此美国病人不需要支付更多的钱。我们赞赏辉瑞的这一决定，并希望其他公司也这么做。这对美国人民来说是个好消息！（推特） 面对药品公司，川建国也只能认怂。 2018-07-10 商务部、外交部、发展改革委等20部门昨日联合发布《关于扩大进口促进对外贸易平衡发展的意见》，提出在稳定出口国际市场份额的基础上，充分发挥进口对提升消费、调整结构、发展经济、扩大开放的重要作用，推动进口与出口平衡发展。这一重磅文件主要是落实6月13日国务院常务会议提出的相关要求。分析人士认为，由20个政府部门联合发文提出扩大进口意见的做法非常罕见，后续各部委还会陆续出台更加具体细化的政策。（上证报） 中国对外开放已经是趋势了。 央行主管的金融时报刊文称，近期上市公司股权质押风险个案频频出现，其背后大都凸显了个别公司股东激进扩张和“加杠杆”的不当行为，这种现象在当前资本市场属于个案，未呈现风险集中的趋势。从总体上看，股权质押压力最大的上半年时点已过，潜在风险总体可控，各方要理性科学对待这一现象。 一般新闻里提到就已经有所风险了。 证券日报刊文称，现在的A股市场已经处于历史底部，这已是民间和监管层达成的共识，当前的A股市场是信心底。近期监管层对A股市场的表态，就是估值底的由来，也是信心底的重要依据。恢复A股市场的信心不会一蹴而就，这需要时间。监管层喊话有助于市场恢复信心，但扎实推进改革开放发展才是提振市场信心的根本举措。监管层只要按照自己划定的道路前进：统筹考虑研究拟定长远性、基础性制度建设举措，进一步丰富资本市场投资主体，拓宽资金入市渠道，优化资本市场结构，提高资本市场对外开放程度和国际化水平。如此，一个稳健发展的A股市场在不久之后就会呈现在国内外投资者面前。 A股的底是由中国生产力的增长和人民币的坚挺为基础的。 数据显示，6月共有61家信托公司发行了972款集合信托产品，环比微降。与此同时，共有49家信托公司成立了995款集合信托产品，环比上升10.8%，共募集资金1206.76亿元（不包括未公布募集规模的产品），环比劲增38.41%。此外，房地产信托依然是发行市场的绝对生力军。从6月以来的新发行信托产品看，收益率最高的产品中有超过50%是投入到房地产市场的。（证券时报） 房地产的债务基本进入垃圾债的范畴了。 中国人民银行参事盛松成日前参加活动表示，稳健中性的货币政策边际上不应再趋紧。随着金融去杠杆的边际力度下降，预计今年M2增速将高于去年，“我相信会高于8.5%。”对于未来货币政策的走向，盛松成认为，从财政政策来看，今年积极的财政政策并未更加宽松，甚至比去年要紧。（一财） 在债务违约风险加剧的时刻，中国没法执行趋紧的货币政策，也正因此中国的汇率稳定就很难保证了。 据了解，国开行对棚改贷款的收紧只是开始，其他金融机构也将对棚改贷款收紧。（经济观察报） 如果棚改再不收手，那么后果将会换来中国未来20年的失落。 2018-07-09 中国6月外汇储备 31121.3亿美元，预期 31028亿美元，前值 31106.2亿美元。 外汇储备基本保持不变，当然前提是中国仍然对其国民有较严格的外汇管制。 一家名为“中精国投”的私募基金公司，出现严重兑付危机，涉事资金超过18亿元。更令人忧心的是，这家私募背后的实控人“外滩控股”已经人去楼空。而“外滩控股”又系上市公司雷科防务的第二大股东，此前曾爆出股权质押风险。种种迹象表明，事态发展并不乐观。（券商中国） 中国的债务问题在今年应该会有一波爆发。 8日下午，海南省委常委、常务副省长毛超峰率调研组到海航集团调研暑期航空业运行情况，并以电视电话会议形式与海航集团系统进行了座谈。海航集团董事局主席、董事长陈峰就海航集团近期的工作情况和下一步工作计划作了汇报。毛超峰指出，希望海航集团专注航空主业，精益求精、深耕细作，把航空业进一步做大做强。作为海南本土企业，海航集团要抢抓机遇，积极投身海南自由贸易区和中国特色自由贸易港建设。海南省委、省政府对海航集团发展有信心，希望海航集团上下同心，按照既定方案解决集团目前遇到的发展问题。（海南日报） 反过来说，海航还真是不务正业，但这样的企业有多少呢？只有海航一家么？ 2018-07-08根据最近理解的一些事件和逻辑整理的图。 2018-07-06 美国东部时间7月5日，美联储公布了其6月货币政策会议纪要，纪要显示与会者总体认为，美国经济已非常强健， 中期来看，通胀将持续处于2%左右，在通胀处于或略高于预期的背景下，适合继续渐进加息。但美联储大多数与会者也表达了对于美国目前贸易政策的担心。很多人提到，贸易政策相关的不确定性和风险可能对商业信心和投资支出有负面影响。（央视记者 王威） 只要市场没有大反应，加息是必然的事件。 经历过2015年及2016年的疯狂、2017年的惨淡后，北京楼市在2018年的“前半场”喜忧参半。虽然新房、土地、房企销售等各项指标仍处于较低水平，但是作为北京楼市风向标的二手房市场，触底反弹，交易量连续四个月超过万套，让寒冷多时的北京房地产市场多了些暖色。（新京报） 回过头来看，这想必也是棚户货币化的结果之一，毕竟最为坚挺的始终是一线城市的房价。 2018-07-05 刚刚过去的6月份，银行理财收益并没有出现“年中反弹行情”，整体表现较为平稳。融360数据显示，6月份银行理财的平均收益率为4.81%，较2月份的最高点4.91%下降了0.1个百分点。分析认为，7月份降准正式实施之后，市场资金流动性将进一步宽松，下半年银行理财的收益率继续走低的概率较大。（经参） 原本以为逆回购利率会比3月更高，但实际观察发现并没有那种反弹出现，基本上可以确定近期货币政策的基调是松的。 美国6月30日当周首次申请失业救济人数 23.1万人，预期 22.5万人，前值 22.7万人；美国6月23日当周续请失业救济人数 173.9万人，预期 171.8万人，前值 170.5万人。 比预期稍微多点。 中国人民银行党委书记、中国银保监会主席郭树清在接受采访时表示，上市公司盈利能力提升，平均估值水平在主要经济体中居于低位。国际投资界普遍认为，中国资本市场已显示出较好的投资价值。上半年，境外资金净流入股票市场1313亿元，境外机构投资者净买入中国政府债3089亿元。信用债市场略有波动，但违约率远低于国际市场平均水平，总体风险完全可控。 结合最近的一些分析，境外直接投资已经成为外汇储备增长的主要动力。这一点与其它新兴市场国家相比越来越接近了，一旦资本流动加快就必然会有牺牲汇率稳定的代价。这里有具体数据。 外商注册的企业数量增加迅猛，是一个值得思考的问题。 2018-07-04 因提早收市的美国股市下滑，引发对美国国债的避险买需，令收益率曲线处于近十一年来最平水准。交易商和分析师表示，对全球贸易战全面爆发的担忧引发投资者押注。全球经济增长和通胀放缓，这意味着即使美联储提高短期利率，但美国长债收益率将不会大幅上涨。当前对国债的避险需求缩窄了短期和长期国债收益率之间的差距。一些投资者担心，短债收益率可能高于长债收益率，这现象被称为收益率曲线倒挂，发生在过去五次美国经济衰退之前的大约12至18个月。（路透） 从图上看的确在过去的一年里美债收益率曲线在慢慢变平。扁平化的本质是2年收益率上升而10年收益率下降，意味着人们更加倾向购买10年期的国债，而不买短期国债。换句话说，就是资金不看好未来短期的经济形势，所以选择了更为长期的国债作为避险方式。这里讲的更加清楚，提到： If they believe a recession is coming, they expect the value of the short-term bills to plummet sometime in the next year. They know that the Federal Reserve lowers the fed funds rate when economic growth slows. Short-term Treasury bill yields track the fed funds rate. 据国际清算银行的数据显示，欧洲各银行在其资产负债表上隐藏了数千亿美元的高风险交易，仅在报告日期前几天将其转移。同样的问题不会发生在英国或美国的银行身上，因为它们有更好的设计规则。但自从欧洲引入新的、结构糟糕的规则以来，这个问题变得越来越严重。欧洲的银行在报告日期之间增加了在货币市场上的借款，把这些钱在短期内借出以快速获利。在本季度末和年底，当必须拿出自己敞口的快照时，它们就会关闭交易，使它们向欧洲监管机构报告的杠杆水平低于实际水平。 这也可能是促进英国脱欧的原因之一。不得不承认英国有着相当丰富的金融经验，所以更加能防范金融风险。从这个角度，英镑有着比欧元更好的投资价值。 今年以来，美股上市公司正在以前所未有的加速度回购自家股票。上半年美国上市公司累计回购了逾6700亿美元股票，回购规模已超过2017年全年创下的5300亿美元纪录，再创历史新高。在回购等多重利好的刺激下，今年以来，美国三大股指不断刷新历史高位。 值得关注的是，公司通常会在股价较低时回购股票，而美股今年的大幅回购却发生在股价走高时。对此市场分析人士指出，在看好后市的同时，这也是公司试图提振股价的做法之一，后市要警惕回购带来的流动性和债务风险，并密切关注回购放缓后的美股表现。（中国证券报） 这也是经济形势不够景气的体现之一。虽然回购不会对公司业绩有积极正面的促进作用，但也好过盲目投资造成产能过剩，再不济也那些A股公司啥也不作为，或搞搞内幕交易强百倍。 美联储：将于7月13日披露递交给国会山的货币政策报告，美联储主席鲍威尔将出席国会听证会。 密切关注。 2018-06-30 有分析人士称，人民币贬值，主要是因为美元走强，并伴随市场风险偏好和外汇供求力量变化。央行未跟随美国上调公开市场利率，表明人民币贬值依旧可控，风险不会太大，因此央行也选择不干预。当前人民币有效汇率事实上已经贴近均衡汇率，高估压力得到有效释放。四季度人民币汇率可能迎来反转，2018年全年人民币对美元汇率的中枢将稳定在6.3-6.4的位置，双向波动的区间有望在6.2-6.7之内。（证券日报） 这种分析不过混淆视听。 2018-06-29 1. 不再投资正股，除非30%以上的收益 2. 已持有的正股在合适的时机抛出 3. 逢低做多vix 4. 配置一定石油 2018-06-28 今年以来，影响类别资产表现的主要因素是量化紧缩，今年是曾经助推资产价格攀升的全球流动性大潮的转向年。据估算，美联储、欧州央行及日本央行今年迄今的证券购买量仅有1250亿美元，远不及2017年1.5万亿美元的规模。此情此景表明，由于决策机构的改弦易辙，市场将会失去大约1.38万亿美元的流动性注入。策略师们预计，未来六到八个月，流动性将会全面萎缩，这也是他们为什么即便在经历了近来的市场下挫后依然对全球股市前景表示悲观的原因之一。 那么正股的风险变得越来越大，而波动性的风险越来越低，只有超过30%的吸引力才足够投资吧。 英国央行副行长Cunliffe：英国央行预计未来几年利率将逐步上行。 流动性下降是全球的趋势。 2018-06-27 报道称美国要求盟友11月4日前将伊朗石油进口量降至零，带动WTI原油大涨3%至70美元/桶关口上方，刷新一个月高位。之前美国制裁要求伊朗原油进口国每六个月把伊朗原油进口量降低20%，现在的要求要严格多了。伊朗每天出口原油约240万桶，主要流向欧洲、中国、印度和土耳其，之后欧洲和土耳其可能会在压力下顺应美国的要求，但中印可就不一定了。若美国计划实现，油市供应将大量减少。目前WTI原油已站上70美元/桶，跨过所有重大技术阻力位，似乎正“走向春天”。 法国巴黎银行资深石油策略师Harry Tchilinguirian撰写报告称，在美国输油管限制在2019年得到解决和美国出口扩张到通常由OPEC海湾国家供应的亚洲市场之前，布伦特油价可能会达到80美元/桶之上；未来六个月油价仍将获得支撑；OPEC+的净供应增加仍然不足以改变世界石油市场短缺的局面；由于美国的制裁，伊朗80万桶/日的石油出口可能会在年底之前退出市场。 虽然石油看起来在涨，但情况似乎变得越来越复杂。如果考虑沙特、俄罗斯这两大产油国的态度，估计上涨的概率要低于下跌，可以适当配置一定的石油资产。 据Techcrunch媒体报道，苹果今天凌晨为教师推出一款免费的应用程序——Schoolwork。获悉该款基于云计算的应用程序允许教师在课堂上使用iPad建立以及分发讲义与作业，与学生单独协作，跟踪学生的进步。最值得注意的是，该程序允许教师在各种教育类程序中布置特别的活动。 Ipad从最初一种尴尬的存在，到现在逐渐成为课堂的必备，这是一个不断寻找killer application的过程。pad这种电子产品走到今天，大部分的厂家都已经不赚钱，卖不下去了，而ipad却能找到这个把握年轻人的很好的机会，不得不佩服苹果的洞察力。long apple~ 预计年内人民币兑美元汇率或将发生贬值加大的过程，年内破7的概率存在；这一水平符合中国经济阶段需求，人民币兑美元汇率基点在6.6元附近，有利于外贸发展对实体经济以及市场信心的稳固，人民币高估风险需要防范。人民币破7不是坏事，但时机、节奏很重要。人民币升值破6.2不是好事，这一价位不利于中国外贸与经济稳定、持续发展。（中新经纬） 从这里可以清楚看到这就是主动的贬值。 2018-06-25 截至06月24日，上交所融资余额报5581.96亿元，较前一交易日减少26.13亿元；深交所融资余额报3612.69亿元，较前一交易日减少21.95亿元；两市合计9194.65亿元，较前一交易日减少48.07亿元。 最近融资额一直在下降，因此股市也一直在下跌。 快讯：离岸人民币兑美元亚市早盘跌逾150点，报6.5256，连续八天下跌。 人民币的贬值应该是刻意为之。虽然政府严格控制资本流动，但水是堵不住的，压力在不断上涨。7月份是个关键的月份，届时情况可能更加复杂，也可能更加明朗。 日本央行发布6月14-15日政策会议纪要：一位日本央行政策委员称，适宜推进强有力的宽松举措。一位政策委员称，实现通胀目标仍有很长距离。 日本央行没有加息。 2018-06-24 为进一步推进市场化法治化“债转股”，加大对小微企业的支持力度，中国人民银行决定，从2018年7月5日起，下调国有大型商业银行、股份制商业银行、邮政储蓄银行、城市商业银行、非县域农村商业银行、外资银行人民币存款准备金率0.5个百分点。鼓励5家国有大型商业银行和12家股份制商业银行运用定向降准和从市场上募集的资金，按照市场化定价原则实施“债转股”项目。支持“债转股”实施主体真正行使股东权利，参与公司治理，并推动混合所有制改革。定向降准资金不支持“名股实债”和“僵尸企业”的项目。同时，邮政储蓄银行和城市商业银行、非县域农商行等中小银行应将降准资金主要用于小微企业贷款，着力缓解小微企业融资难融资贵问题。 人民银行将继续按照党中央、国务院的统一部署，实施好稳健中性的货币政策，把握好结构性去杠杆的力度和节奏，为高质量发展和供给侧结构性改革营造适宜的货币金融环境。 这次降准是为了两个目标：1. 债转股；2. 微小企业。这两者都很重要，但不知道怎么才能真正实现，而又如何阻止这笔钱流入房地产这样的领域。看有评论说这还是会利好房地产，看看最近几天的股市表现就知道了。另外，贬值是真的，所以配置更多美元资产是一个手段。 2018-06-21 据证券日报，厚生智库研究员赵亚赟称，既要去杠杆，又要防止出现金融危机，央行的选择并不多。6月份是纳税和还贷大月，流动性会吃紧，所以央行会大量释放流动性，严控金融风险。由于贸易战等原因，很多企业奉行现金为王的政策，市场资金压力会非常大，央行近期的货币政策应该会比前几个月更加偏宽松，不排除再次降准。 选择真的不多。。。逆回购可以薅点羊毛，但小钱还是算了。 推迟A股CDR发行之后，小米决定分步先在港交所上市。据接近小米人士获悉，小米很可能于6月21日早上八点在港交所披露更新后的招股书。有消息称，小米计划发行21.8亿股股票，融资规模最多为61亿美元，IPO定价区间在17港元/股至22港元/股。不过以上消息并未得到小米方面确认。有小米内部人士表示，“我们目前还不清楚。”（证券日报） 小米的价格比较公道。 券商统计显示，如果将预警线和平仓线分别设定为150%和130%，截至6月19日收盘，共有619家公司股价接近预警线，其中564家公司跌破预警线；共有425家公司股价接近平仓线，其中353家公司跌破平仓线。整体来看，A股股权质押当前平仓线以下市值规模约为9351亿元。不过，研究机构也指出，目前A股市场股权质押风险整体可控，并不存在系统性风险。（经参） 股权质押这种方式本来就是高风险的，尤其考虑影子银行的操作方式。 据一位券商人士透露，头条新一轮融资正在进行中，云峰基金和KKR正在与头条接洽，不包括抖音，头条估值350亿美元，抖音将独立融资。“目前，抖音估值在80亿美元到100亿美元之间。 抖音的消息据说是谣传，头条估值感觉高了，而抖音感觉还凑合。但抖音这样的公司其护城河是什么呢？感觉只要稍微撒点儿金币，谁都能成长起来。 根据美国财政部6月15日披露的最新数据，持有美国国债的外国当局在今年4月间纷纷减持，在前十大“债主”中，只有巴西和开曼群岛没有减持美国国债。第一和第二大“债主”中国和日本在4月间分别减持58亿和123亿美元，英国、印度、加拿大、韩国、墨西哥、瑞士、爱尔兰等国也加入减持行列。俄罗斯持有美国国债的规模从今年3月的961亿美元下降到4月的487亿美元，一个月内减持超过49.5%。（每日经济新闻） 从二月以来的确在上涨，但似乎影响还是比较有限的。如果这是个趋势，还是应该引起重视。 2018-06-20 国务院办公厅发布关于调整国务院促进中小企业发展工作领导小组的通知，组长为国务院副总理刘鹤。这意味着有点开始重视中小企业了。 美联储主席杰罗姆·鲍威尔表示，从美国经济的角度来看，“联邦基金利率仍然处于宽松的位置，委员会成员们认为，可能比中性利率的估计中值低100个基点”。那么中性利率大概在3.5%左右？ 2018-06-19 特朗普称，若中国拒绝改变做法，关税举措将生效；上述拟议关税是对中国上周五针对美国关税计划宣布的对500亿美元美国商品征税的回应。特朗普威胁称若中国再次加征关税美国将进一步加征关税。 有趣，周而复始，最终的结果我很好奇！ 地方债新增发行已放量。财政部公布最新数据显示，5月份，全国发行地方政府债券3553亿元。其中，新增债券171亿元，置换债券3382亿元。由于全年发行量压力不小，机构普遍预计，二季度将迎来地方债发行高潮。 屋漏偏逢连夜雨，还能说什么呢？必须看好自己的钱袋子。如果你不学习经济，那就等着被别人收割吧。 在岸人民币兑美元跌破6.48关口，最新报6.4815元，创1月份来新低，较上周五夜盘收盘跌465点。离岸人民币兑美元跌破6.49关口，现报6.4915元，创今年1月份来新低，日内跌约370点。 股市、债市、汇市三杀？别人恐惧的时候，你该怎么样呢？ 2018-06-18 彭博汇总的数据显示，海外资金正在以2008年全球金融危机以来最快速度撤离亚洲六大新兴股市，今年以来从印度、印度尼西亚、菲律宾、韩国、台湾和泰国撤离的资金总额已达190亿美元。 新兴市场是真的被洗劫了，但这又该怪谁呢？ 联合石油数据库JODI：沙特4月原油库存跌99.3万桶至2.3442亿桶；沙特4月原油产量环比跌3.9万桶/日，至986.8万桶/日；沙特4月原油出口增加19万桶/日，至731.2万桶/日；沙特4月对原油产品需求下降15.8万桶/日，至204万桶/日；沙特4月对原油产品出口量下降8.7万桶/日，至166.4万桶/日。 石油价格看起来有点不明朗，沙特和俄罗斯都有限产的意愿，但这会造成一些国家通胀无法控制，所以在政治上可能不允许。注意，石油永远不是一种普通的商品，而是一种政治工具。 近期以来，阿根廷、土耳其和巴西等国的货币出现动荡；阿根廷寻求IMF的援助，巴西因高油价引发了全国范围的罢工。而美联储正在加息路径上，对这些高负债国家构成实质压力，其货币相对美元出现贬值的情况下尤为如此。潜在伴随而来的经济衰退可能会降低对石油的需求；此外，石油的需求端也存在不稳定因素；今年5月，因为油价高企，国际能源署将石油需求下调了10万桶/日。同时，新兴市场的经济可能出现放缓，这可能拖累石油需求增长。 受伤的总是新兴市场国家。 OPEC据悉讨论将产量提高30-60万桶/日，沙特、俄罗斯致力于达成永久性石油联盟。（彭博） 在利益面前哪里有永远的敌人。 2018-06-14 ① 从2019年1月开始，将在每次FOMC政策会议结束时举行新闻发布会，以改善沟通。 ② 前两轮周期是因为金融稳定性而结束，而不是因为通胀的缘故。 ③ 通胀处于2%目标附近，希望观察这是否具有可持续性；承诺维持2%的通胀目标。 ④ 不寻求美联储在贸易政策上发挥作用。 ⑤ 主要结论是，美国经济形势“非常不错”。 ⑥ 预计减税将带来三年“实质性支撑”。 ⑦ 现在变更前瞻指引正当其时。 ⑧ 不会置评金融风险是否高于正常水平。 联邦公开市场委员会(The Federal Open Market Committee )，简称FOMC。 鲍威尔真是个政治家，从而他的发言很少有真正有价值的内容。实际上美联储就考虑三个方面：1. 股市不暴跌；2. 失业率不飙升；3. 通胀不低于2%，就会加息。 近期信用债市场违约事件增多，多家上市公司现身其中，引发各方关注。在业界看来，“打破刚兑”已逐渐成为债市参与各方的共识，但在信用债违约处置方面，“最后一公里”仍待打通。（经参） 债市违约正是金融风险的一项指标。 恒大研究院任泽平发文称，5月金融数据显示，M1同比增6%，环比再降1.2pct，M2同比增8.3%，环比持平。M1增速创2015年7月以来新低，印证企业融资压力，M2增速继续保持8.3%水平。存款方面，表外回表趋势下，银行存款增速基本保持平稳，5月存款新增1.3万亿元，同比多增1900亿元，存款余额同比增8.9%，存款准备金相对保持充沛，为后续降准留有充足空间。 M1和M2增速用同比，而存款用同比，这其中是有什么不方便说的么？8.3%的M2增速，和6.5%的GDP增速预期之间存在什么关联呢？又与通货膨胀有什么关联呢？ 国家统计局新闻发言人毛盛勇在发布会上表示，固定资产投资1-5月份增长了6.1%，比1-4月份回落了0.9个百分点。固定资产投资回落主要是基础设施投资所致。1-5月份制造业投资增长5.2%，加快0.4个百分点；房地产投资增长10.2%，小幅回落0.1个百分点；基础设施投资增长9.4%，回落3个百分点。现在三大攻坚战当中要防范化解重大风险，包括对不合规、不合法项目要进行规范和清理，要求地方融资行为、举债行为更加规范，因此，对资金的空间，以及有些项目因为不合规不合法停建、缓建带来了一些影响。 房地产的投资增长亮眼！天天喊要各种规范，而法规政策在哪里呢？如果地方政府没有动力减少房地产收入，靠什么真正去压制房价呢？ 这些突出问题是：一是自我革命本身意味着存在许多特有的困难。这就需要刮骨疗伤，壮士断腕的态度。二是道德风险根深蒂固。相当多的金融机构仍然存在“垒大户”情结，不少企业高度依赖债务投入，各类隐性担保和“刚性兑付”没有真正打破，“预算软约束”“投资饥渴症”问题仍然比较突出，特别是市场化、法治化破产机制远未形成。三是一些地方部门、银行和企业缺乏应有的紧迫感和危机意识，对去产能、去杠杆心存侥幸。四是平衡各方利益面临很多制约。随着改革不断向纵深推进，兼顾多重利益的难度越来越大，调整越来越困难，有待于各个方面付出更大的努力。 刮别人的骨，断别人的腕？难道要机构自己去主动承担风险吗？之所以垒大户，还不是因为一旦户大了就有国家在兜底么？Too big to fall! 一位拒绝透露姓名的国有银行经理向中国日报表示，其所在银行已获得额外配额，预计资金将于6月底前发放，以支持非金融部门。 呵呵，看来牌桌下还有只看不见的手。 证监会副主席方星海：从国际竞争格局看，也要加快上海国际金融中心建设，我们设想一下，如果全球的企业都到上海来融资，那么今后谁制裁谁都很难说了。 引用新浪财经上网友的评论：睁着眼睛说瞎话！为啥全球的企业要来上海融资？是上海的资金量很充沛？还是资金进出中国很方便？ 今日在“2018年陆家嘴论坛”上，中国人民银行党委书记、中国银行保险监督管理委员会主席郭树清示，今年来针对房地产贷款、地方政府债务和互联网金融等系统性风险隐患较大的领域，设定了审慎监管指标，开展压力测试，加强清理规范，及早介入干预，有效遏制了风险的累积。 在金融风险面前，任何手段都不如倒腾房地产，这在过去10年里已经无数次被证明了。那么，现在该买哪只股票呢？你猜。 郭树清: 不必担忧金融业开放会冲击中国金融市场。治理金融风险隐患必须考虑市场承受能力。银行体系内逾期90天以上的贷款占比从120%降到了100%。 中央的新闻总是要反着看会比较真实。 欧洲央行声明：每月300亿欧元的资产购买规模将持续到9月份；QE将在12月份结束。10月份至12月的月度购债规模为150亿欧元。将保持利率不变至少至2019年夏天。 年底欧洲会退出QE，那么接下来会发生什么事情呢？当潮水退下，那个没有穿底裤的人会是谁？从而想到为啥国内的各种科技公司都在加快上市的节奏呢，因为需要有人来接盘了。 小米将于6月23日举行全球发售股份的新闻发布会；一般情况下，新股新闻发布会下一个交易日将启动招股，即6月25日起招股。（香港经济日报） 需要密切关注小米的IPO。 2018-06-11 6月14日即将公布美联储议息会议最新利率决议，市场普遍预期此次美联储加息的可能性较大。苏宁金融研究院宏观经济中心主任、高级研究员黄志龙表示，不排除美联储加息之后，中国央行可能跟进调整公开市场操作利率，以维持美元升值趋势下人民币汇率的相对稳定，管控人民币贬值预期进一步加大。另外，从国内经济来看，降准、加息的政策操作组合可能会成为下半年央行保持稳健中性货币政策选项之一。（证券日报） 美联储的下一次加息即将到来，但市场上对于多次加息的反应已经越来越麻木，仿佛这并不意味着任何特别的事情，这正是问题所在。 尽管降准并没有在预期中到来，但各界坚信，当下实体经济稳中向好的态势以及金融市场生态环境的改善都需要央行在货币政策层面给予适时适度的支持。也就是说，降准可能会迟到，但绝不会缺席。高企的存款准备金率仍有进一步下调空间，但央行下调的过程将是谨慎的。未来流动性将会继续注入小微企业、绿色经济等领域。今年以来金融市场、资本市场波动加大，也需要货币政策给予必要的支持。在全国经济一盘棋、金融市场与资本市场联动的大框架下，货币政策应该更敏感一些，反应更快速一些。 加息和降准本就是两面，在单方面降准改成加息的时刻，却去讨论这样的话题，不知道其心究竟想达到什么样的特殊目的呢? 2018-06-09 兴业银行、华福证券首席经济学家鲁政委和兴业研究公司研究员何津津表示，中国央行6月份仍将大概率跟随美联储加息，但对市场利率影响或有限，需要关注金融市场的波动可能对具体政策落地时间点的影响。预计中国年内仍会有降准操作，同时MLF操作仍会延续。（证券日报） 央行也终于要开始加息了，这对于本来就脆弱的金融市场可绝对不是一件好事，拭目以待，开始保持低仓位，没有好的标的绝不轻易出手。 2018-06-08 记者调查发现，个别银行已在全北京范围内将首套按揭房贷利率抬升至基准利率的1.3倍，另有至少5家股份制银行的网点员工透露“额度不多”、“今年基本暂停接单”，甚至直接建议询问国有大行。业内人士分析称，中小银行减少房贷业务是因资金成本偏高，个人申请房贷利率仍有上行空间。（北京商报） 在中国总有个特色，如果一件事情非常难做到，那么那必然意味着利润，房子也是这样的。 截至昨日，内地及香港上市房企中已经有22家公司公布了前五月的销售数据。1-5月，销售金额合计达到1.69万亿元，同比上涨约35%。5月份，大部分龙头企业销售业绩乐观，22家房企销售收入达到了3515.7亿元。（中证报） 去库存进入尾声，按照过去的经验，这将是下一轮调控和涨价。 美国总统特朗普发布推文称，为什么欧盟和加拿大没有告诉公众，多年来他们对美国采用了大规模的贸易关税和非货币贸易壁垒，这对我们的农民，工人和公司是完全不公平的。取消你的关税和障碍，否则我们会比你更多！ 美国与加拿大难得如此互相指责。 据悉，中兴通讯董事长、CEO、CTO都将更换，可能还要更换部分执行副总裁（EVP）。中兴很有可能将成为中国首个国企体制，但美国政府监督的全球科技公司。（《财经》） 新时代的买办公司。 【深交所：进一步完善重大违法强制退市实施制度】深交所表示，进一步完善重大违法强制退市实施制度，优化财务类和市场类退市指标体系，对持续经营能力存疑或存在重大不确定事项的高风险公司加大风险警示力度，不断夯实制度基础，防范系统性金融风险，促进深市多层次资本市场健康稳定发展。 越是要加强金融风险控制，不正是意味着风险较大么？综合各个方面的指标和政策，个人觉得危机已经快来临了，当然这个快也并不是一两个月，而是可能在未来的一两年内。 2018-06-07 在今日商务部召开例行新闻发布会上，外媒记者向发言人提问：“华尔街日报的消息说，中国提出来，如果美方放弃新的进口关税，中方愿意购买近700亿美元的农业制造和能源产品，请问这个消息是不是真的？”对此，商务部新闻发言人高峰表示：中美双方在上周末的磋商中，就一些具体的贸易合作领域，特别是农产品、能源领域进行了深入、具体的探讨；中方愿意在相向而行的前提下，扩大自美进口。 这个消息恐怕是真的。 美国商务部部长罗斯表示，美国与中兴达成协议；将对中兴通讯罚款10亿美元；中兴通讯必须30天内更换董事会和管理层；美国将会挑选人员进入中兴通讯的合规团队。（万得） 中兴成为了一个被美国直接监管的国企。 “股神”巴菲特：自己现在是股票市场的净买入者；打赌欧元将在未来十年继续存在；对股市的决策应该独立于当前的商业前景；毫无疑问，美国将在未来10年、20年和30年遥遥领先。 美国的确有资格引领未来的10年、20年，但未必是30年，而且这个差距必然是在缩小的。 桥水基金开始做空大批金融资产，认为金融危机已经快来临了，其核心指标如下： 1.资金正在以2008年金融危机以来前所未见的速度从股市中抽离；2.穆迪$(MCO)$警告，垃圾债券违约的大浪潮正在来临。垃圾债券通常可以视作是大的金融危机到来前的预警信号；3.联邦存款保险公司（FDIC）数据显示，一个被市场密切关注的指标——银行不良资产规模在2018年第一季度扩大了3倍，这意味着一些较大规模的银行正处在资产不良的区间；4.今年美国债券的开局表现是大萧条以来的最糟糕的一次；5.抵押贷款利率创出了7年新高，同时这一利率正在以50年来最快的速度增长，这对于房地产行业而言绝对是一个灾难；6.零售行业债务违约率在2018年创了历史新高；7.目前正处在零售商店关闭情况最糟糕的一年；8.全球最大的两个经济体贸易摩擦仍在继续；9.世界第九大经济体意大利正面临着金融体系的崩溃，事实上这还不是最糟糕的，因为不排除意大利困局的负面影响可能会传导至欧元区的其他国家；10.意大利的银行类股近期出现了大幅的下跌；11.意大利10年债券收益率创出了2014年以来的最高水平；12.德国银行巨头德意志银行$(DB)$近期宣布将会裁员7000名以寻求扭亏为盈，事实上德意志银行已经连续亏损多年，如果2018年德意志银行破产的话从本质上而言只是让市场见证了另一雷曼兄弟倒闭。 德意志银行计划今年把员工数量裁减至93000人以下。 果然德意志银行存在问题。 2018-06-06 近期碧桂园、富力地产、合生创展等多家房企发债中止，引发市场在流动性紧张背景下的担忧。专家预计，今年下半年会有较多的中小房企违约风险事件发生，中小房企或加速退出市场，行业集中度提升速度料进一步加快。与此同时，众多房企亦在探索新融资模式。（中证报） 房地产的上涨不是政府能随意操控的，释放出的几万亿如洪水猛兽，无法简单的遏制。正如当年大禹治水，不能靠堵，而是要疏导。而这个疏导的出口在哪呢？是非常有价值的问题！ 2018-06-05 可以断言，房产税一旦开征，整个社会对于楼市未来走向的预期必然逆转。拥有大量房产的人一定会积极抛售多余的房子，而有了房子的人再也不会把房子当做最佳投资品而想着要去多买几套。楼市供求格局大变也就势在必然。政策与其抡起大棒直接朝房价砸下去，还不如尽快把房产税推出来。 得出这样的结论，不是蠢就是坏！房产税的拐点在于税收的量可以超过今天靠卖地的土地财政，而不是为了平均分配房子。 2018-06-03 今日，今日头条官方表示，腾讯利用垄断地位以各种理由、多次进行不正当竞争的行为。针对其中的“腾讯QQ空间拦截、屏蔽头条网页链接” “腾讯安全管家作为安全软件拦截、屏蔽头条网页链接”，今日头条已经起诉腾讯，目前两案都已于6月1日获得海淀区人民法院立案相关证据都已经提供给法院，详见起诉状。 如果头条可以成功，那么阿里为什么不这么做呢？而且头条的形象还不如阿里正面，感觉在今天的中国，这只是引起人们注意的一种方式而已。 2018-05-31 消息人士透露，美国有99.9%的可能性将对加拿大、墨西哥和欧盟征收钢铝关税，决定将很快做出。（CNBC） 有趣，这是在声东击西吗？这个西又是什么呢？ 2018-05-30 白宫周二在声明中表示，美国将对500亿美元含有“重要工业技术”的中国进口商品加征25%的关税，包括与“中国制造2025”相关的技术。最终清单将在6月15日之前公布，关税将在此后不久施行。 美国政府的内部似乎已经很难达成一种相对一致的意见，这在过去一段时间内已经充分体现出来了。 2018-05-25 朝鲜副外相金桂冠称，朝方愿意在任何时候与美国对话。特朗普取消美朝峰会的决定与全世界的希望相悖。朝鲜最高领导人金正恩已尽最大努力，来与特朗普会谈。（朝中社） 朝鲜问题如果圆满解决，那么美国在日韩驻军的合法性就存在问题了，那么现在美国掌权的保守主义还能开心吗？所以即使特朗普非常希望在自己的政绩上加上一笔，但也要那些人点头才行。 2018-05-24 俄罗斯能源部长：将在6月讨论逐步恢复原油产量，预计2018年油价均价将在60美元/桶上方。 俄罗斯原本是希望油价上涨的，但在一些其它目标的驱动下主动放弃了短期的利益。 2018-05-23 据香港经济日报报道，香港改革上市制度后，再有巨型新经济企业拟到港上市。内地最大网约车平台“滴滴出行”最快下半年启动上市，已初步决定落户香港，并考虑不同上市架构，不排除以同股不同权形式上市。消息称，滴滴正积极寻觅主要投资者，询价相当约估值约550亿美元，与去年底最新一轮融资时估值相若，预计滴滴最终上市时市值或能达700亿至800亿美元，随时超越全球另一网约车龙头Uber的约700亿美元估值。 这是个做空的好机会。 我国养老保险基金支出规模仍在快速增加。人社部21日发布的《2017年度人力资源和社会保障事业发展统计公报》显示，2017年职工基本养老保险基金总支出38052亿元，同比增长19.5%，相比2013年的18470亿元增长106%，这意味着近四年职工养老保险基金支出翻了一倍多。此外，基金征缴收入与基金支出缺口持续扩大，制度对于财政补助资金的依赖程度不断提高。 必须自己考虑自己的养老问题。 据华尔街日报：改革多德弗兰克法案的议案以258-159的票数获得众议院通过。改革多德弗兰克法案的议案旨在减少金融危机后对小型银行与社区银行所实施的监管限制。 民粹主义开始流行起来了，金融危机的伏笔各种显现。 房企供应链ABS井喷，规模已达782亿元，至少14家公司涉足以解资金之渴。近日有消息称，恒大拟于5月23日左右发行供应链ABS，由深交所发行。据悉，恒大已获100亿元储架发行额度，期限不超过1年，第一期规模约10亿元。 房企没钱了，但后续会怎么发展呢？ 阿根廷央行维持七天回购利率在40.00%不变。 阿根廷金融危机已经爆发。 欧盟委员会：若当前宽松货币政策立场逆转，意大利可能出现风险。由于最近的政策措施和不利的人口趋势，意大利的长期财政可持续性也在减弱。意大利的中期可持续性风险仍然很高，因结构性初步盈余不足以促成公共债务迅速减少。短期内意大利面临的再融资风险似乎有限，主要因市场流动性充裕以及外部环境改善。 意大利总是火药桶的角色。 土耳其里拉兑美元下跌超过3%，跌破4.81里拉。土耳其10年期国债收益率升至15.30% 土耳其经融危机已经爆发。在美元环流的影响下，这些当年无节制借钱的国家很快就尝到苦果了。 意大利5年期信用违约掉期（CDS）升至152个基点，触及近12个月以来最高。 违约掉期上涨意味着大家一致觉得会出现违约。 2018-05-21 央行《一季度货币政策执行报告》在宏观杠杆率、未来货币政策侧重点、金融监管思路转变等方面的表述，与以往有着“明显”不同。宏观杠杆率方面，提出“宏观杠杆率增速放缓，金融体系控制内部杠杆取得阶段性成效”，一改以往强调积极推进“去杠杆”的表述；未来货币政策侧重点方面，将“去杠杆”调整为“调结构”；金融监管思路转变方面，删去“统筹政策力度和节奏，防止叠加共振”，新增“全面清理整顿金融秩序”。“这不免让一些金融机构认为，随着去杠杆接近尾声，各部门金融严监管政策协调力度相对减弱，货币政策侧重点开始转向调结构促经济发展，未来货币政策存在趋松空间，资金流动性较一季度更加宽裕。”某股份制银行信贷部主管表示。 去完杠杆是否会走入下一轮调控？这个恶性循环能否被打破？ 2018-05-20 《中美联合声明》 双方同意，将采取有效措施实质性减少美对华货物贸易逆差。为满足中国人民不断增长的消费需求和促进高质量经济发展，中方将大量增加自美购买商品和服务。这也有助于美国经济增长和就业。 双方同意有意义地增加美国农产品和能源出口，美方将派团赴华讨论具体事项。 双方就扩大制造业产品和服务贸易进行了讨论，就创造有利条件增加上述领域的贸易达成共识。 双方高度重视知识产权保护，同意加强合作。中方将推进包括《专利法》在内的相关法律法规修订工作。 双方同意鼓励双向投资，将努力创造公平竞争营商环境。 双方同意继续就此保持高层沟通，积极寻求解决各自关注的经贸问题 按照宋鸿兵的解读：美方真正获得利益方首推华尔街金融业，其次是制药公司，再是农业。中国的金融开放如何避免风险？这将是未来一段时间要观察的要点。农业的打击可能很深远，这对中国社会将会产生什么影响？制药公司会不断制造焦虑，中国人真是多灾多难，在任何一个最基础的需求上挣扎着。 2018-05-19 【易方达基金詹余引：接近50%的公募基金投资者持有时间在一年以内】 易方达基金公司董事长詹余引19日在“2018清华五道口全球金融论坛”上表示，现在公募基金的投资者持股是比较短期的，有数据显示，持有单只基金超过5年的投资者大概只有12%左右，有45%到50%的投资者持有时间只在一年以内，投资者的结构会影响基金投资理念的发挥。（中国证券网） 这个统计无情的批判着各种价值投机者。银行螺丝钉的粉丝自从指数下降以后明显就不热情了，之前可是各种价值投资理论烂熟于胸的。 【央行研究局局长徐忠：防范金融风险和经济转型，要提高资产回报率】中国人民银行研究局局长徐忠表示，未来3-5年，中国经济主要把握两条主线：一是经济转型，要从高速增长向高质量发展，要提高全要素生产率；二是防范金融风险。发达国家货币政策也在做结构性改革，全球金融危机以后十年，主要经济体谁的结构性改革走到前面谁走的好，可能对今后二十年全球经济格局的演变有非常重要的作用。提高资产的回报率、提高全要素生产率，是经济转型的需要，也是防范风险的需要。（21世纪经济报道） “提高资产回报率”这不正好是要投资资产的机会吗？ 2018-05-18 近期有关部门召集部分券商进行座谈，就股票质押式回购业务风控、两融绕标套现融资等具体问题展开了讨论。据悉。有关部门在会上叫停了两融绕标套现融资的操作模式，再次强调了股票质押违约处置的流程，要求各家公司控制集中度，并督促各家券商谨慎开展股票质押式回购业务，不得低价竞争，促进行业良性发展。（上证报） 影子银行的控制只能通过座谈？ 法国总统马克龙：将维护法国企业的合法权益。欧洲将采取报复措施来捍卫自己的利益。即将召开关于伊朗导弹项目和地区活动的会谈。确认欧洲将维护伊核协议的成果。 马克龙还需要真正做点事情。 2018-05-15 人民日报海外版头版评论文章称，中国将继续坚持对外开放基本国策，坚定不移地推进经济全球化、贸易投资自由化。预计未来五年，中国货物进口额累计超过10万亿美元，对外投资超过1万亿美元，出境旅游人数超过7亿人次，出国留学人员超过350万人，提交PCT国际专利申请量超过30万件。中国发展，世界受益；中国扩大开放，助力世界繁荣。 真正的开放是中国的企业走出去，而不是外国的企业走进来，尤其是金融企业。 美国驻华大使Branstad：特朗普希望大幅度增加对中国的农产品出口。（彭博） 特朗普的希望马上就会成真，只是苦了中国的农民补贴了美国的农民。 美国商务部长威尔伯罗斯美国东部时间5月14日表示，愿意尽快改变对中国手机制造商中兴通讯的销售禁令，此前一天美国总统特朗普表示，要求美国商务部帮助中兴通讯恢复运营。“中兴确实做了一些不合适的事情。 他们已经承认这一点”。罗斯在美国全国新闻俱乐部的活动中说。 “问题是：我们最初提出的措施是否有其他替代方案？ 这将是我们将非常、非常迅速地研究的地方。”（央视） 说谎的最高境界就是明目张胆的说谎，但没有人敢站出来说不。 隔夜纽约原油期货上涨0.4%，收于每桶70.96美元，而布伦特原油收涨1.4%，自2014年底以来首次升至每桶78美元上方。50多名巴勒斯坦人在抗议美国驻耶路撒冷大使馆开馆典礼时遇害，凸显该地区的紧张局势；一周前，特朗普政府重新恢复对OPEC第三大原油生产国伊朗的制裁。 油价节节攀升。 2018-05-14 近两个月楼市出现升温迹象，成交量环比上升，新房价格环比涨幅有所扩大。面对楼市新变化，近半月，住建部约谈12个城市并再次强调楼市调控目标不变，各地楼市再次迎来调控密集期。（中国新闻网） 一轮又一轮，这样做真的有意思吗？ 特朗普发推称：我们正在为中兴通讯提供一种快速恢复业务的途径。（因中兴业务无法正常开展使得）中国有太多的工作岗位流失，我已告知商务部要尽快完成这项工作。 中兴柳暗花明。哎，实际上制裁中心美国的芯片公司会损失惨重，那么中兴可能被制裁吗？ 2018-05-11 特朗普：将于6月12日在新加坡会见朝鲜领导人金正恩。 时间敲定，但美国必然是没有诚意的。 2018-05-10 去年“317新政”后，北京二手房成交量和成交价持续走低。近两个月以来，北京二手房交易量出现回升，价格趋稳。有分析称，这只是朝着正常交易水平恢复而已，过热并未出现，市场仍旧稳定。（中新网） 压抑资本的投资需求是难以为继的。 美国能源调查公司Rystad Energy：美国重新实施对伊朗的制裁，将导致伊朗原油出口量到今年11月减少70万桶/日。 制裁伊朗的一个重要结果就是要提升石油价格，这符合沙特的预期。 美国WTI 6月原油期货实盘价格周三收涨2.08美元，涨幅3%，报71.14美元/桶。 石油这种大宗商品能跳涨3%算是少有的了。 美国财政部拍卖250亿美元10年期国债，得标利率2.995%、创2014年1月份以来新高，投标倍数2.56、前次为2.46。 10年国债利率在上涨。 沙特能源大臣：美国决定退出伊朗核协议之后，确认沙特出于对产油国和石油消费者利益的考虑而维持原油市场稳定性的承诺。 与OPEC轮值主席国、俄罗斯、以及美国保持密切联系，未来数日，沙特还将接触其他产油国和重要消费者，以确保市场稳定。 沙特的内心里充满了微笑。 美国能源信息署（EIA）：美国5月4日当周EIA石油产出1070万桶/日，创单周历史新高。 美国的页岩油也在欢庆量价提升。 2018-05-09 德国外长：特朗普退出伊朗核协议的决定不算太令人意外。 我们将与盟友一起努力阻止中东局势升级。 我们的目标是继续维持伊朗核协议，协议是有效的。 我们呼吁伊朗坚持核协议下做出的承诺。 我们将留意美国退出伊核协议给企业带来的影响，并考虑欧洲的整体应对措施。 按照鸿学院的分析，伊朗目前来看是走着最保守的路，也不是美国、以色列、沙特希望他走的路。在美国金融制裁的压力下，未来会发生什么变数呢？拭目以待，这可能是近10年最大的一场政治风波。不过，石油看来要涨。 2018-05-08 美国东部时间5月7日下午，白宫称中共中央政治局委员、中国国务院副总理刘鹤将于下周访问美国首都华盛顿，与美国经贸官员继续举行经贸磋商。（财新） 关注谈判局势。 纽约原油跳水跌超3%，因为外媒报道美国总统特朗普可能将不会退出伊朗核协议。 石油居然能在极短时间大幅跳水，有趣。 美国总统特朗普：周二（5月8日）当地时间14:00（北京时间周三02:00）将宣布伊朗核协议决定。 不出意外肯定退出。 意大利五星运动党党首Di Maio：五星运动党不会支持中性政府，呼吁7月份大选。 如果五星运动无法达到足够的选票比例，看起来意大利大选将毫无意义的一轮又一轮。 京东第一季度调整后每ADS收益0.71元人民币，市场预期0.82元人民币。 第一季度营收1001亿元人民币，市场预期989.9亿元人民币。 第一季度商品交易总额（GMV）3,302亿元人民币。 京东预计第二季度营收1200亿~1240亿元人民币，市场预期1223.9亿元人民币。 京东预计全年营收增长29%~33%。 不及预期，跌吧。经过这么多年，京东真的应该证明自己有足够强的护城河和盈利能力，否则这么多钱砸进去，出来个啥呢？只为改善人们生活？ 2018-05-04 鸿学院有关芯片问题的讨论。 中国人普遍的系统性思维不足。 美国推行单边的强权政治导致规矩的破坏，不守规矩会导致维护成本上升，芯片的成本必然上升。 凡是地大物博的国家都比较粗放，难以形成所谓的“匠人”。 美国是谈判的高手，可以无中生有制造筹码。 阿里巴巴大肆收购引发投资者疑虑，担忧其盈利能力或遭遇冲击。随着利润和收入增长放缓，阿里巴巴频频收购的行为或终于开始令其盈利受到影响。而该公司市值相比高点损失610亿美元，也反映了投资者的不满情绪。 最近一段时间阿里的表现毫无亮点，护城河上毫无建树，反倒马云天天在国际上各种呼吁。 2018-04-23 分析人士认为，此次海南发布的限购政策，条件异常苛刻，包含户籍、社保、贷款、年限等多重调控手段，这意味着外地新增炒房资金在数年内几乎无法进入海南楼市，前几年流入海南楼市的数千亿炒房资金面临着被“关门打狗”的尴尬局面。易居研究院智库中心研究总监严跃进指出，从限购的广度来说，较其他城市因城施策不一样，海南省全域限购是非常罕见的。（每经） 中国人对房地产的狂热是过去十多年来培养出来的，政府一遍遍惩罚那些没有买房的人，不断去强化房子必须涨价的预期，才造成了像现在这种现象。要纠正这种情况，不可能通过限购，限购只是为了降低房价的杠杆，减少系统风险。 这一建议初听让人热血沸腾，但冷静思考发现，它并不可行，甚至很危险。产业化的芯片业与“两弹一星”服从完全不同的经济规律。夸大“两弹一星”中的独立自主和人定胜天因素，并据此不计成本、闭门发展芯片业，更是有陷入过度社会动员的风险。脱离常识，一门心思想着弯道超车恐怕是欲速而不达。守正出奇才是正确的态度。产业环境和社会人文环境改善了，规模大了，基础厚实了，逆袭才有可能发生。真正的国家意志应该是创造环境，培植基础，而非亲自去做逆袭的计划，逆袭意志的主体只能是企业，并且是民营企业。 两弹一星中领军人员与一线国家的差距要更小，而且只是一锤子买卖，而芯片如果没法赚钱，是不可能不计成本烧钱的。 近期网约车在多城市开展“烧钱大战”引发社会强烈反响。针对这一现象，交通运输部微信号近期连发三篇评论性文章，指出“烧钱大战”不可持续，呼吁网约车发展要“脱虚向实”，运输市场要公平竞争。（中国交通报） 再多的钱也烧不了多久了，赚司机的辛苦钱，是支撑不了成千上万码农的工资的。 2018-04-22 刚成立的公司，如果有很像高管的高管，那这公司也长不了。 – 《知乎》 对于刚成立的公司，每个人都必须实心用事。 国资委报告，在此事件中，中兴通讯公司一系列应对都十分愚蠢和被动，美国的制裁对中兴通讯公司自身及其他中央企业都可能带来高危影响。一定程度上，不仅通讯行业，也不仅国有企业，国内很多企业都在为中兴通讯公司的短视和无诚信经营付出惨痛代价，我国外交布局和国家形象也不可避免地受到影响。 这抹黑的不仅仅是企业，也是一大批充满神秘色彩的“老总们”，他们不再神乎其神，也会犯愚蠢的错误。 鸿学院有关金融对外开放的讨论。 弗洛伦撒之所以能兴起，成为欧洲的金融中心，主要靠三点： 商人走出去，形成遍布欧洲的商业网络； 商业网络为教会服务，收取十一税，形成稳定的商业活动； 参与国家政府融资项目，获取税收等垄断权； 作为反例，西班牙虽然有发现美洲的优势，但在本国没有形成相应的金融优势，使自己的胜利果实被热那亚商人攫取。而中国要开放，首要问题是这种开放是“对外”还是“对内”。对外意味着中国的商业大规模要走出国门，去服务世界各国；而对内意味着国际金融网络接入国内资本，中国的国有银行将面临全球的竞争，几乎毫无胜算！ 中国的芯片虽然落后，但不过几十年而已。而金融的落后，政治的落后，与发达国家相比相差有数百年，真的需要密切关注这块的发展，因为它关系到每个人的财富变化。 2018-04-21 朝中社报道，朝鲜最高领导人金正恩20日宣布，朝鲜将从21日开始不再进行任何核试验和洲际弹道导弹发射，废弃朝鲜北部核试验场。只要朝鲜不受核威胁挑衅，朝鲜绝对不使用核武器，不泄露核武器和核技术，集中全部力量发展经济，并将与周边国家和国际社会积极展开紧密联系和对话。（央视) 朝鲜果断停止核试验并开始改革开放，这早就在鸿学院里提到，不出意料。下一步，如果有机会购买朝鲜的资产或许是一种很好的投资。 Bitstamp平台数据显示，比特币最近24小时涨7.47%，刷新3月25日以来高位至8888美元。 比特币已经彻底成为各种交易平台炒作赚钱的手段，这8888美元不正好是一种讽刺吗？ 美国10年期国债收益率涨至2.955%，为2014年1月以来最高。 国债收益率提高意味着股市下跌。 苹果跌幅达到4.2%。 像苹果这样的大公司也能一天下跌4.2%，还有什么是不可能的呢？ 中国企业被诟病过太久，重资产轻人力，行业从业者待遇普遍不高。 这句话很好的描述了中国的现象，正是“重资产而轻人力”。无论什么项目，似乎首先做的总是批地、盖楼，然后找一两个凑合的人才，带领一帮水平低下的人开始搞项目。似乎只要找对了方向，人才不是重点。可问题是现在的项目不是搬砖，不是靠蛮力就能搞定的。但我们的政府或企业往往没有长期规划，大批项目的重点不是把好东西做出来，而是要应对各级领导的参观和工作指导。可笑的是，如果一个企业只踏踏实实做事，往往是难以成功的，因为必须得到政府的“支持”，否则银行连贷款都不会给你。 2018-04-20 深圳某投资机构负责人认为，“互联网泡沫2.0”有可能就在未来几年爆发，O2O、互联网金融、大数据、AI可能是“高危地带”，而在这些大领域的“独角兽”企业占比达半数以上。这意味着，可能有一批“独角兽”企业要倒下。“即使没倒下，‘挤泡沫’也是很难避免的。制度设计要避免二级市场泡沫提前向一级市场转移，对于“独角兽”企业IPO前一年内突击入股的投资人需延长锁定期，降低投机色彩。 这种认识我个人非常认同，因为现在的互联网存在大量靠债务支撑的公司，不断融资，拆东墙补西墙却迟迟上不了岸，这种泡沫最终总会以某种形式破裂。 2018-03-13 截至2017年9月30日，美国政府助学贷款总额达1.4万亿美元，有近500万人违约，违约率高达22%。 直接反映美国就业形势并不好，至少工资离债务存在较大的距离。 美国白宫：特朗普总统将在3月20日与沙特王储穆罕默德·本·萨勒曼举行会面。 OPEC对石油的价格无法达成一致，美国页岩油产量还在增加，所以沙特面临较大的财政风险，所以这么着急会面。 https://www.reddit.com/r/investing/comments/83wctc/junk_bonds_diverging_from_sp_500/ 该网友指出了自己的一个指标：观察junk bounds yield与S&amp;P 500 return之间的关系。他发现最近一段时间两者出现一定的背离，即S&amp;P一直在涨而PHDAX出现明显的下降。这种现象的出现不能直接解读成某种问题，直接反映的可能是资本变得更加倾向一些体量大的公司，就好比前一段时间在A股上证50的涨幅远超过沪深300，更是远超过中小板。这两者始终会一致，所以一方面可能是大盘股回调或小盘股追赶。 2018-03-14 穆迪：将油价预估从45美元/桶上调至65美元/桶，因全球原油减产且需求增速强劲。 最近的油价很焦灼，沙特和伊朗之间的分歧不知道最终会走向何方，沙特依靠美国对伊朗施压的效果究竟会不会提高油价？在3月底可能会有一定的消息透露出来。 美国3月9日当周API原油库存 +115.6万桶，前值 +566万桶。库欣地区原油库存 -15.6万桶，前值 -79万桶。 汽油库存 -126.2万桶，前值 -454万桶。 精炼油库存 -425.8万桶，前值 +149万桶。 原油库存下降有利于油价上升。 “新债王”Gundlach：预计美国政府预算赤字将在2019年达到1.3万亿美元。 美国财政赤字还在加大，系统风险继续加大。 “新债王”Gundlach：考虑到规模为6000亿美元的量化紧缩（QT），美国债市可能会出现大约2万亿美元的债券供应。 各国都在资金回笼过程，人民币也是这样，从而影响到一些垃圾创业公司的生存问题。 蚂蚁金服1.845亿美元入股巴基斯坦TMB 打造当地版“支付宝”：北京时间3月13日晚，挪威Telenor集团和蚂蚁金服联合宣布，双方已达成战略合作伙伴关系。蚂蚁金服将出资1．845亿美元，购入前者在巴基斯坦的子公司TMB（TelenorMicrofinanceBank）45%股权。 蚂蚁金服可能出现系统风险。一方面国内频繁投资垃圾公司（最近还包括多家共享单车）；另一方面海外的高风险投资可能点燃导火索。 2018-03-15 全国50大热点城市的土地出让金高达6452.3亿：国家统计局数据显示，1-2月全国楼市（商品住宅）销售面积增速收窄至2.3%。但与楼市销售持续放缓相反的是，房企拿地的热情仍然不减。最新数据显示，在2018年的前2个月里，全国50大热点城市的土地出让金高达6452.3亿，与2017年前两个月的4019.2亿土地出让金相比同比上涨了60.5%。 又是一轮地王？房企视政策于不顾，真的是有作死的感觉了。需要考察一些龙头企业在这次的表现。@新闻里提到龙头企业带头拿地…果然政府的政策都是搞笑的么？政策的执行上还是有些问题。 央行将进行200亿元人民币7天期逆回购、200亿元人民币28天期逆回购。央行公开市场今日净投放400亿元人民币。 仔细阅读数量，资金实际上是净回笼的，对应了稍紧的货币政策。 蚂蚁金服消费贷款达 6000 亿人民币，是建行的 3.7 倍。彭博社援引知情人士称，尽管政府监管加强，但通过花呗、借呗业务，阿里巴巴旗下蚂蚁金服的消费贷款规模 2017 年至今增长了一倍。这一规模很可能进一步引发严格的监管措施，影响公司的增长速度。 蚂蚁金服贷款的增加可能来自其互联网征信机制的高效率，但也不能排除其这么大的盘子里的系统风险问题。尚不清楚其违约情况。 2018-03-16 美国1月长期资本净流入 621亿，前值 273亿。国际资本净流入 1197亿，前值 -1193亿。 所谓的国际资本给美国人接盘，其实不能完全这样讲，毕竟美国仍然是世界上最为强大的国家，自然有人愿意来接美国的盘，而不是其它国家。因此，与其说美国有风险，其它国家同样有风险。 加拿大楼市出现崩跌 房屋销售创五年新低：加拿大地产协会（CREA）最新统计显示，2月加拿大的成屋销售环比跌6.5%，创近五年最低，也较去年12月的峰值连续两个月回落。未经季调的实际销售活动同比下跌16.9%，也为五年新低，并较10年均值低了7%。 如上一条，加拿大这种“度假型”国家太弱，其房地产在金融风险出现时就会出现暴跌。估计不少国内炒房客也在其中吧。 2018-03-17 美国债务首次触及21万亿美元。美国财政部最新数据显示，联邦债务水平3月15日首次触及21万亿美元。自特朗普2017年1月就职以来，债务增长超过1万亿美元。 系统风险仍然在增加。 从横向整合到垂直整合 全球芯片业并购愈演愈烈。随着芯片业成本压力与竞争压力加剧，行业龙头纷纷通过并购提升竞争力与市场份额。国际半导体产业协会预计，未来十年芯片产业或从横向整合进入到上下游垂直整合阶段。通过并购，芯片厂商的综合实力将越来越强，产业集中度将越来越高，寡头垄断格局将进一步强化。（中证） 芯片业出现了与以往不同的发展，需要持续关注。Intel自然有它的技术优势，但是这种老牌公司内部的问题往往也根深蒂固，不好说将会如何。 百度申请发行两批次美元债。百度申请发行两批次高级无担保债券，但没有言明每批次的发行额度。 美元的加息对美债形成越来越大的压力。 17日，上海证券交易所公司债券项目信息平台显示，中信证券-滴滴第【N】期CP资产支持专项计划于昨日受理。该计划显示，此次滴滴拟融资金额为100亿元，品种为持证券-ABS。滴滴出行营销副总裁李敏确认该消息属实，但对于滴滴外卖业务他三缄其口。（21世纪经济报道） 最近垃圾公司呈现出许多有趣的特征，比如垃圾公司A与垃圾公司B合作就以为能变成非垃圾公司，比如垃圾公司A开展各种乱七八糟的业务就变成非垃圾公司。滴滴这家公司搞不好就是下一个乐视。当然，它也有一个强劲的竞争对手，美团。 ABS（Asset-Backed Security）通常的用法是在公司在主营业务之外开辟新的产品时，将新的产品线作为资产抵押借贷。如果该业务失败，那么损失也不会波及原有的主营业务。这对投资人来讲是风险很大的。 2018-03-18 《鸿学院——朝鲜与美国的会晤》 战略就必须有敌人和朋友 东北问题的根源是地缘环境恶劣，而改善的关键也在环境改变 如果你要投资一个无法放弃的棋子，就必须做到它感恩戴德，否则就是为他人做嫁衣裳 如果朝鲜真的走向改革开放，那么将会出现巨大的商机 关注东北与一带一路；关注朝鲜改革开放的机遇。 英媒称，2017年全球不动产投资创下1.62万亿美元的历史新高，来自亚洲的投资占比超过半数。但伦敦仍是最受青睐的全球不动产投资目的地，因为对英国脱欧的担忧已被英镑贬值的影响抵消。 中国炒房客的爱好：伦敦、澳洲、加拿大。 日前，碧桂园400亿ABS获审批通过，一时间，非标转身ABS似乎成为非标转型最确定的出路。部分非标业务吃重的银行、券商正在山重水复之时，仿佛迎来柳暗花明的前景。但有业内人士表示，部分ABS底层资产涉及项目多，合规难度大，投资勿盲目入场。（中证） 在这种时候提高债务的风险很大，美元加息的压力，非主营业务的风险等等，投资要避开这样的流氓公司。 深圳市交委称3月17日凌晨收到相关举报，经调查，滴滴出行本次违规投放青桔单车约2万辆，该委联合市城管局、交警局于3月17日下午约谈滴滴出行，责令整改并立即收回违规投放车辆。（南方网） 垃圾公司制造垃圾。 2018-03-19 黑龙江省围绕“哈欧班列”已经建成“哈尔滨－汉堡”、“哈尔滨－明斯克”、“大庆－泽布鲁日”、“哈尔滨－莫斯科”四条核心线路，并形成了相应跨境产业链条。黑龙江省这个传统意义上的东北边陲省份，正借力“一带一路”加速成为我国向北开放的前沿地带。 这才是正路！但朝鲜问题是否能顺利解决呢？有待进一步观察。 2018-03-20 OPEC秘书长Mohammad Barkindo：OPEC和伙伴国家目前的焦点是全面、及时执行协议控制原油产出直至2018年底。 油价在2018年底之前应该能保持不会出现大的波动。 据媒体报道，全球最大的消费类无人机制造商大疆正在与投资者进行洽谈，希望能够以150亿美元的估值进行新一轮融资。此次募集的资金大约在5亿至10亿美元之间，交易细节尚未最终确定。“此次融资可能是以股权融资与债务融资结合的方式进行。”一位熟悉交易的内部人士表示，这也是大疆迄今为止规模最大的一轮融资。 智能硬件仍然没有哪家企业真正走出消亡的命运。 被称为固收衍生品交易界的传奇人物、全球最大债基PIMCO前基金经理Harley Bassman，撰文阐释了“美股熊市真正到来的信号”：美股与美债收益率的相关性由正转负时。据他列出的条件，这很可能出现在今年末。参考 1和 2。 读了一遍，里面的逻辑感觉有互相矛盾的地方，感觉并不靠谱。 摩根士丹利：预计美债收益率将在9月份发生倒挂。 倒挂的形势最近一个月时间里变得越来越明显。 白宫发言人桑德斯：美国总统特朗普和沙特王储的会面取得了进展。 石油与军火的交易。 近日，记者多次收到低佣金开户的广告，“无论资金量大小，均可享受万分之二点五的超低佣金率。”部分中小券商实际给出的佣金率更低，万分之二甚至是万分之一点八的佣金率已经不再是资金量大的客户的“专利”。此外，记者还了解到，为争夺优质客户，多家券商已经打破两融业务中的融资基准利率8%以上的行业惯例，融资基准利率在6%到7%之间。（证券日报） 客户的争夺意味着金融市场的不稳定性还在上涨，越来越多的韭菜进入市场了。 2018-03-21 中国商务部：拟对美对华出口的改性乙醇、无缝钢管等加征15%的关税。 中美贸易战的回应。 美国国家安全顾问McMaster将辞职，届时将由John Bolton接任。（纽约时报） 温和派改成了鹰派。 周四（3月22日）纽约尾盘，美国10年期基准国债收益率跌5.86个基点，报2.8244%，财政部拍卖TIPS后不久一度跌至2.7971%。两年期美债收益率跌2.69个基点，报2.2786%，财政部拍卖TIPS后不久一度跌至2.2538%、意味着美联储3月决议声明宣布加息以来的跌幅达到10.36个基点。 股市和债市终于出现此消彼长的现象了，这是QE以来未出现过的状态。是否说明QE的退出已经接近尾声？这正是熊市来临的信号吗？ 2018-03-22 沙特能源部长al-Falih：OPEC与非OPEC将持续合作至2019年。原油市场三分之二的供应过剩已经得到解决。沙特有大量可用作燃料的铀储备。 鼓励油价上涨的消息。 新华社华盛顿3月22日电：美国总统特朗普22日签署总统备忘录，依据“301调查”结果，将对从中国进口的商品大规模征收关税，并限制中国企业对美投资并购。特朗普在白宫签字前对媒体说，涉及征税的中国商品规模可达600亿美元。中国商务部此前表示，中方绝不会坐视合法权益受到损害，必将采取所有必要措施，坚决捍卫自身合法权益。 贸易战的硝烟，但只是硝烟。 特朗普律师辞职。（纽约时报） 特兰普身边的人换了一个又一个，如果真出现金融危机，那么特朗普就算是完蛋了。而他肯定不会让危机这么快到来，会尽量拖延，但总有个时间节点是无法抑制住的。 道指跌幅再度超过400点，标普跌约1.6%，纳指跌约1.7%。据纽约时报报道，特朗普律师Johyn Dowd辞职，Dowd负责调查“通俄门”特别检察官Mueller。 中美贸易战升级，股指暴跌。但评论基本上持有这只是一次谈判而已，因为中美经济纠葛太深，如果真的贸易战双方都不会有好结果。 2018-03-23 林毅夫在 #中国发展高层论坛# 称，从计划经济向市场经济转型，中国不是唯一的国家，而很多转型中的国家出现了经济崩溃，危机不断，但中国却是稳定并快速发展，这其中的奥秘或许就是“新人新办法，老人老办法”。 这不正是保守主义的做法吗？一刀切是政治上很不理智的行为，或者说是一种政治幼稚的表现。 3月24日上午，中央政治局委员、国务院副总理、中财办主任、中美全面经济对话中方牵头人刘鹤应约与美国财政部长姆努钦通话。姆努钦向中方通报了美方公布301调查报告最新情况。刘鹤表示，美方近日公布301调查报告，违背国际贸易规则，不利于中方利益，不利于美方利益，不利于全球利益。中方已经做好准备，有实力捍卫国家利益，希望双方保持理性，共同努力，维护中美经贸关系总体稳定的大局。双方同意继续就此保持沟通。（新华社） 贸易战照会后马上开始谈判。 3月23日，在第二届中国直播与短视频峰会上发布的《2017年中国直播行业研究报告》显示，2017年，我国直播行业市场总收入超过300亿元，比上年增长39%。从数据来看，行业仍然在走上坡路。2017年，直播行业用户人数达到了4.2亿，无论是游戏直播用户，还是秀场直播用户，同比增速都超过了50%。直播行业用户规模呈现良好增长态势。（广州日报） 我原本认为直播是一种不靠谱的行业，但自从理解每个人都有选择自己生活的原则后，开始认真审视这个行业。直播实际上满足了相当大一部分人的工作需求和消费需求，那么这就是一个合理的行业。而直播与许多互联网行业不同，它有着非常清晰的变现模式，在互联网这个十分不靠谱的环境下，直播俨然一种清流式的存在。严重看好YY。 2018-03-24 宗庆后在中国发展高层论坛谈到“新零售”时表示，最终来讲互联网的成本更高，“它要把商品送到消费者那里去，最后又要回归到实体零售来”。“我不太认可互联网颠覆，全部颠覆我们不都死掉了？” http://t.cn/RnCURRJ 这里的讨论非常有价值。随着网络零售对实体店的冲击，造成实体店盈利困难，间接肯定会造成实体店租金下降，从而慢慢压低实体店的成本。随着可能的无人零售的到来，这将与网络零售造成冲击，因为无人零售在人力成本上远远低于快递送达业务。下一步应该密切关注无人零售的发展。但注意这不是指自动贩卖机，自动贩卖机已经在很大程度上证明是入不敷出的，而且是个竞争极其激烈的行业。无人零售特指大型商超，而人力成本又极地的情况，这更像美国商超的状态。从这个角度，每个WMT和COSCO在亚马逊面前并非无力一战！ 2018-03-25 沙特王储：OPEC寻求与俄罗斯和其他产油国实现10-20年的供应合作。与俄罗斯就“整体情况”达成共识，正致力于磋商长期性石油合作的细节性内容。 俄罗斯有非常强大的动机要跟沙特合作，而沙特刚刚访问美国，从这个角度来看似乎沙特王储访美并没有达到自己的目的，否则以美国的立场必然要求沙特拒绝与俄罗斯合作，这个政治格局有点意外，可能跟美国本土的石油行业有一定关系。如果是这样，那么沙特和俄罗斯的合作将对石油价格造成很强的影响，而美国因为本国一些利益，失去了沙特在中东的影响力。 2018-03-27 今日，长沙市住建委印发《关于实施差别化购房措施的通知》。长沙市限购区域内“限房价、竞地价”的商品住房项目（不含定向限价房）和新建商品住房项目中144平方米（含）以下户型的普通商品住房，将优先满足首套刚需购房群体。首套购房刚需群体为长沙市户籍的无房家庭和个人（文件施行后离婚且不满1年的不包括在内）、自签订征收协议之日起1年内的被征收人以及符合长沙市限购政策的本市以外户籍无房家庭。 确定这不是突破限购的借口吗？ 美国10年期国债收益率跌破2.8%。 10年国债的变化是否符合经济危机的趋势？ 美国财政部拍卖350亿美元五年期国债，得标利率2.612%，投标倍数2.50、前次为2.44。 美国发债仍在继续，而收益率还在下降，似乎说明人们把钱都投入债券。 美国30年期国债收益率跌破3%，为2月6日以来首次。 倒挂似乎在加剧，需要关注2年国债收益变化情况，如果其收益上升则说明问题。 2018-03-28 巴克莱：预计WTI原油在下半年会跌至51美元/桶，布伦特原油年底会跌至57美元/桶。 这个预测很别出心裁。。沙特要完？石油是个政治游戏啊！如果真是这样，沙特指定要跟美国翻脸了。从最近沙特与俄罗斯走得很近来看，这不是不可能。 随着上市房企2017年年报的陆续披露，上市房企的融资情况相继浮出水面。从融资成本来看，银行贷款、中票、公司债等方式融资成本多在4%-5%之间，与信托融资相比优势明显。整理目前已披露2017年年报的30余家上市房企信托融资数据发现，上市房企信托融资成本大多集中在6%-10%之间，个别公司的信托融资成本达11%。（证券日报） 这样高的债务利息，看来去杠杆的效果还没真正提现出来，值得警惕了。 2018-03-29 周四（3月29日）纽约尾盘，美国10年期基准国债收益率跌4.18个基点，报2.7389%，美股盘中一度跌至2.7389%，为2月5日以来盘中新低；第一季度，10年期美债收益率累计上涨33.35个基点。两年期美债收益率跌1.81个基点，报2.2661%，美股收盘前一度跌至2.2620%，3月21日美联储发布FOMC利率决议声明前，一度涨至2.3574%、为2008年9月9日以来盘中最高位；第一季度，两年期美债收益率累计上涨38.31个基点。 我一直觉得指望靠国债来避险是个非常幼稚的行为，而考虑10年甚至30年的预期是个绝顶幼稚的行为，购买竞争力足够强大的企业股票才是正道，跟巴菲特的思路才是王道。 CNN恐惧与贪婪指数周四(3月29日)读数8，表明市场处在极度恐慌状态，较上日涨2个点；CNN恐惧与贪婪指数，和“VIX恐慌指数”差不多的功能；当CNN恐惧与贪婪指数处在贪婪状态，相当于VIX恐慌指数处在低位，利好风险资产，不利避险资产；相反，CNN恐惧与贪婪指数处在恐惧状态，相当于VIX恐慌指数处在高位，不利风险资产，利好避险资产。 又一个指数，市场总是不理性的。 这是一次补记，因为实在比较重要。金正恩临时访华，想必是决定跟中国就访美先交个底？金正恩访美的进程值得关注。 2018-03-30 摩根大通：预计近期LIBOR/OIS利差扩大会给相应的业务领域带来110亿美元利率成本，并使得家庭利率成本增加50亿美元。 近期LIBOR/OIS利差扩大仅仅是金融条件“非常小幅度的收紧”，不足以促使改变对宏观经济前景的预期。 同意。难道宏观经济有问题就不生活了吗？这不过是政治问题周期性的发酵而已。既不能消除，也没必要过于恐惧。 美联储：美联储主席鲍威尔4月6日将在芝加哥针对经济发表讲话。 已关注。（更新）符合市场预期，并没有什么值得说的。 2018-03-31 湖南能源监管办发布的《湖南省电力企业2017年财务经营情况通报》显示，纳入监测范畴的58家电力企业2017年累计利润总额为8.46亿元，同比下降77.89%。其中，17家火电厂发电599.91亿千瓦时，同比增长17.68%，亏损15.23亿元，同比利润减少19.93亿元，共有14家亏损，占82.4%。 火电在增长的情况下亏损还是如此严重，说明什么问题呢？电价在大幅下降？ 2018-04-02 财政部网站1日发布通告称，经国务院批准，国务院关税税则委员会决定对原产于美国的部分进口商品中止关税减让义务，自2018年4月2日起实施。中国对原产于美国的7类128项进口商品中止关税减让义务，在现行适用关税税率基础上加征关税，对水果及制品等120项进口商品加征关税税率为15%，对猪肉及制品等8项进口商品加征关税税率为25%。 种种迹象，我感觉这次贸易战是真的要开打了。从朝鲜、沙特、俄罗斯等国的行为，看起来美国正处于特朗普一个人的个人show time，而这种并没有政治框架的莽夫，注定要经受一次考验了。 2018-04-05 俄罗斯能源部长Novak：与沙特能源部长讨论了当前减产协议过后的长期合作；当前合作机制是有效的；长期合作可能包括监督石油市场，交换信息和采取一些联合行动；目前尚无联合文件公布，但在进行讨论。 OPEC主席称，俄罗斯在石油减产计划中是一个“好伙伴”。 沙特与俄罗斯关系暧昧，充分说明美国在放弃自己在中东的立场，也可以理解为何麦克马斯特会辞职了。 近日，在腾讯董事会主席兼CEO马化腾的牵线之下，美团点评正式谈妥投资入股摩拜单车事宜，且投资股权占比较大。另一边，ofo第一大股东滴滴亦最终同意蚂蚁金服投资ofo，此前蚂蚁金服对ofo的借债得以实现债转股，双方就投资细节尚在谈判中。（财新） 祝贺摩拜已上岸。有些无法理解腾讯的立场，难道只是为了支付的流量入口？ 2018-04-06 据路透社报道，就在特朗普下令考虑对中国1000亿美元商品加征关税后，美国政府一位高级官员表示，美国愿意与中国就贸易问题进行谈判，但前提是谈判是认真的，因为以往的努力没有取得什么进展。双方尚未安排正式的谈判会议，“目前正与中方就贸易问题进行沟通”。 贸易战再次升级，中方应该已经做好准备，等待特兰普的下一步show。 消息称，欧盟和日本加入美国针对中国技术许可要求提起的世贸组织磋商请求。 欧洲为何要追随美国？这次的态度值得深思。按理说这不符合欧盟的根本利益才对。 2018-04-07 波音从美国航空获得123亿美元的47架梦想飞机协议。 美国似乎也在准备着，给受伤的企业发一点甜枣，但这根本不解决问题。离开中国这个全球最大的市场来谈经济是一种愚蠢的决定。 特朗普致函美国国会参议院金融委员会共和党籍主席Orrin Hatch和众议院共和党籍议长Ryan，解释商务部如何审查国家安全、并发现钢铁和铝进口适用于3月23日所实施的25%钢铁进口关税和10%的铝进口关税。特朗普称，对阿根廷、澳大利亚、巴西、加拿大、墨西哥、欧盟成员国、以及韩国等征收的金属关税被推迟至5月1日。 一些有趣的观察，发现日本不在美国的第一批推迟执行关税国家里，这是不好理解的问题。美国与墨西哥仍然就边境墙的问题在做出一些相互攻击的言论。现在国际局势变得有些微妙，还需要进一步观察。如果EU不能在这件事上保持自己独立的立场，让人无法相信EU真的有复兴的一天，仍然是美国的跟屁虫而已。 I think it’s fascinating to watch how Trump keeps testing his power and discovering that there are very real limits to it. Before entering politics, he has probably had to deal mostly with two categories of people - yes men, and people he could bully. Whether he’s bad at business or not, he has billions of dollars, and lawyers, and a private security force. He could certainly intimidate or bankrupt most people he didn’t like. So then he becomes President and thinks he now has much more power because he still has all the money, but is now also in charge of the world’s largest economy and military. And then he discovers that the President’s power is limited by design. With his travel ban, Trump discovered that the judicial branch doesn’t work for him and can work against him. Then, as he discovered with Sessions and Mueller, even the executive branch can work against him, or at least not the way he’d like. Trump discovered that no, the Department of Justice isn’t like having his own team of attorneys, and the FBI isn’t the President’s personal police force. Then Trump, who apparently doesn’t quite understand what a trade deficit is and how tariffs work, starts talking about tariffs left and right, feeling that it’s him/the US throwing his weight around. Only for him to discover that no, the Chinese government cannot be easily intimidated, and can retaliate by introducing tariffs in kind. 另一个有趣的评论。 The Trump organization is privately owned and run like a family business. I’ve found that family businesses have no room for true meritocracy; they depend on kowtowing to the man at the top. Succession is not by rising through the ranks. It is by being related to the family. If Trump was the CEO of a large publicly traded firm - like Rex Tillerson - he’d have a very different opinion on how decisions are made and how to build consensus. But because he was the CEO of a family business, he still believes in those old-school ideas Reddit上的有趣的评论。 最近在看《大明王朝1566》，里面海瑞的格局实在是最高的，能看到封建王朝末期，大地主阶级土地兼并而导致的贫富分化，终于将走向无法调和，导致大明王朝的覆灭。古代200~300年才会出现一次轮回，而现代社会这个周期要短的多。如果不是核武器的发展使大家不再通过战争来解决争端，想必第三次、第四次世界大战也早就打完了。最近的中美贸易战，一方面可以认为是中国和美国之间的矛盾，其实更是全球化和逆全球化之间的矛盾。看起来全球化能加快经济发展，但这使得那些可以通过全球化获利的人的财富积累远远快过普通老百姓，从而加剧了贫富分化。在这场斗争中，没有谁对谁错，只是大家的立场不同而已。逆全球化实际上并不真正解决问题，解决问题的是让那些前几十年得到大量好处的阶层把利益分享出来，否则只是走形式主义，没有用。特朗普能帮助美国解决这个根本问题么？我觉得没戏，因为到目前为止仍然看不到他究竟做了什么有价值的事情。美国之所以强大，是因为他为强大提供了最好的土壤，自由平等包容，这些才是创造力的肥料，而今天他打算闭关来把这些好的留下，怎么可能呢？背道而驰。他真正该做的是在美国人民中提倡教育，发动那些整体无所事事的美国人做一些更有意义的事业，而不是只会吃喝玩乐，享乐生活。在过去的相当长的时间里，美国依靠的是来自其它国家的先进的生产力在维持它自身的高速发展，而最近这些优势在逐步散失。中国正相反，环境在变得越来越好，政府在有意识的吸引优秀人才，这是个非常好的趋势。中国也有贫富分化的问题，而19大已经明确把这件事当成中国社会的主要矛盾来说，不得不感慨中国政府真的做到了与时俱进，而反观美国政府，仍然在一些边缘问题上磨磨唧唧，不敢指出问题的要害。如果问题都提错了，怎么可能解决好呢？这次的“贸易战”于中国正是一次好的机会，一次表明自己立场的机会，正如宋鸿兵所说，如果你有战略却没有敌人，别人怎么敢跟你站队呢？没错，美国就是中国的敌人，而中国的朋友要向西发展，中东、东欧、西欧、非洲，都是中国非常重视的要发展的朋友，而孤悬海外的美国，如果还做着自己“天朝上国”的美梦，就跟清朝差不多了。 2018-04-08 罗奇：特朗普应该好好照照镜子！美国是“扛不住的” 作为一个中国人，实在是一种没有选择的选择，而我目前来说还是觉得中国在经济上会超越美国的，当然前提是中国人的基本观念不发生变化，这依赖政府时刻保持高度警戒，抓住社会的主要矛盾不断深化改革，这的确很难。我越来越佩服我们的政府了，要是我肯定做不到那么全面。 2018-04-09 阿里巴巴董事局主席马云今天向员工发出了一封内部信宣布，彭蕾将卸任蚂蚁金服董事长，蚂蚁金服CEO井贤栋将兼任董事长一职。 阿里内部的变化也挺有意思，蚂蚁金服在这个时候换人究竟传达了一种什么信号呢？ 2018-04-13 2018年以来，两市质押股数同比下降16.22%、质押市值下降23%。股票质押市场如今出现“量跌价升”状况，即便出质人降低价格要求，仍难以做成业务，在当前一段时间里的处于“有价无市”状态。2017年期间股票质押的融资利率一般在6%～6.5%之间，如今主流的股票质押利率一般在7%左右，6.5%的质押利率已经少见。 有一些券商，在综合考虑目前的风险与资金成本后，则直接将利率在8%以下的质押业务拒之门外。（证券时报） 似乎反映了金融市场对不确定性（即风险）的感知在加深，7%~8%的利率已经较高了。 美债收益率大涨，特朗普表态暂时缓和投资者对中东局势的担忧。周四（4月12日）纽约尾盘，美国10年期基准国债收益率涨5.50个基点，报2.8358%，盘中交投于2.7698%-2.8413%区间。两年期美债收益率涨4.10个基点，报2.3480%，盘中交投于2.2989%-2.3521%区间。 资金又从债市去了股市？ 新媒体信息平台的发展，呼唤并推动着监管机制的创新，特别是更加重视人工智能算法的治理。而即便如此，仅靠监管部门也是不够的，互联网时代需要更加重视用户的力量，建立更具响应能力的举报渠道，发挥好用户作为内容生产者的正面作用。同时，也可以从用户的角度出发，更科学、更合理地对内容、平台进行分类管理。网络治理有了“用户思维”，才能从根本上改变内容生产的生态。 互联网健康监控绝对是AI的好应用场景。 中国证券报头版刊文称，对资本市场尤其是债市而言，除非未来资金面超预期宽松，否则资金面对债市的正面贡献可能逐渐衰弱。目前来看，随着收益率的快速回落，稳货币带来的流动性悲观预期的修复似乎已逐渐反映在市场预期中，且债券供给压力和4、5月存在连续大额财政收税的考验也逐渐临近，后续资金面宽松程度趋于收敛将是大概率事件。 在美元加息的前提下，全球资金量还是以紧缩为基本趋势。 外媒报道称小米正在考虑是否收购运动相机制造商GoPro，消息传出后，GoPro股价飙升近8.8%。根据惠普在2010年收购同样生产艰难的Palm时的出价，GoPro可能售价10亿美元。消息人士表示，尽管小米有收购意向，但不代表公司愿意出高价。 http://t.cn/Rm9yzwK 还是那句话，单纯的智能硬件产商很难生存下去，GoPro当年可不是一般的火。小米的根基在手机和家电，这是赚生活费的生意，而其它的各种“玩具”硬件都缺乏现金牛这重保险。 OPEC秘书长默罕默德·巴金多：全球原油过剩自2017年开始有效的下降了十分之九。 最近油价涨的厉害，但也要考虑整体经济下行的压力。 特朗普：只有在TPP协议比奥巴马的版本更好的情况下，才会重新加入。我们已与TPP11个成员国中的6个达成双边协定，正致力于与最大成员国日本达成协议，后者多年来给我们在贸易上带来沉重打击！ 连日本都要敲打，这位总统在走钢丝呢。 在此前的一次由中国汽车行业组织的高端论坛上，有相关负责人曾透露特斯拉将会在2018年正式在华独资建厂，这个消息似乎也从侧面佐证了上述汽车产业合资股比放开将在年内正式落地的消息。（经济观察报） 特斯拉股票得涨~ 联社4月13日讯，中国央行据悉将放宽对商业银行存款利率上限的非正式指导。（新浪） 如果这是真的，那么将意味着重大的金融改革。中国的银行体系终于能慢慢走出权威政府模式，这带给银行业许多机遇和挑战，例如需要银行加强信用评级体系，强化风控意识等等，而广大的中小企业也将慢慢获得相对优惠的银行贷款。银行的储蓄利率和放贷利率之间的差额在充分竞争后缩小到合理的区间。如果真的能做到这些，余额宝之类的“替代品”将受到严重的威胁。 4 月 13 日，由小米投资的黑鲨科技发布了国内第一款游戏手机。 当年PC游戏火爆的时代，游戏本并没有走出失败的命运；今天，一款面向手游的手机，能否改变这个宿命呢？个人并不看好。手机游戏本质上比PC游戏更加体现休闲，打发碎片化的时间，这似乎离hard core模式更加遥远。 2018-04-14 从A股自身的供给侧改革来看，当前阶段亟需更多增量资金入场，而互联互通额度扩大，提升国际化水平，将对A股营造优胜劣汰生态环境、塑造价值导向具有重要意义。从全球资产配置来看，当前外资对A股整体严重低配，且以被动型基金覆盖为主，这与我国位列世界第二大经济体的地位存在巨大错位矛盾，亟需优化；同时，吸引“独角兽”企业前来A股上市，内地监管层也持欢迎态度，这就需要进一步营造充沛的资金环境，创建高效的估值体系。 感觉证券市场改革是最近一段时间的主流，而国家终于开始认真重视这块了，对A股和中国的所有企业都是一种利好。中国企业依赖银行内部关系的时代是否要终结了？ 朱啸虎首次承认已清仓ofo：我们是财务投资人，战略投资人诉求跟我们不一样。 垃圾公司，能有多远躲多远。 新华社记者14日凌晨在叙利亚首都大马士革听到空中传来巨大爆炸声，叙利亚国家电视台说美英法三国对叙利亚“发动了侵略”。（新华社） 弱国无外交。 鸿学院有关叙利亚问题的讨论。 当前世界的主要矛盾是民族主义和全球化之间的矛盾，叙利亚只是这种矛盾下的一个旁支，有一定的关联和体现，但不是纯主线。 天然气 地理上占据了中东天然气输入欧洲的路线，在俄罗斯和伊朗的支持下加强了这两者对欧洲的天然气控制权。而美国本土的天然气输出与此矛盾。英法试图减弱对俄罗斯在天然气方面的依赖。 库尔德问题 库尔德人独立是英法美的一张牌，但打出来同样会损害土耳其的利益，将导致土耳其倒向对手。 石油价格 沙特依赖美国巩固其在中东的地位，美国依赖沙特联合打压俄罗斯油价。但最近的形势出现变化，OPEC与俄罗斯达成了协议来提高油价。石油价格上涨根本上也符合美国本土页岩油利益集团的期望，但也跟霸权主义不符。 化武事件 3月份英俄之间的神经毒气事件“意外”发酵，也为4月美国对俄罗斯的经济制裁做了铺垫，更为英法美对叙利亚因化学武器的空袭做了铺垫。 三派势力 新保守主义、新自由主义、新民粹主义三派势力强弱目前依次递减。前20年新自由主义一直是中国的支持者，但在中国提出一带一路和智能制造2025以后决定转支持为对抗；新民粹主义痛恨中国抢走了他们的制造业，在全球化问题上与中国对抗。 通俄门 特朗普上台后即通俄门不断，反应了其它利益集团对民粹主义的不满。本质上特朗普和普京都是反全球化的，在这一点是他们的利益一致。 一个有趣的问题：中国当前是这三者哪个成分居多呢？ 2018-04-15 腾讯智能音箱“腾讯听听”4月20日上市。 智能音箱已经成为红海，各大厂商都有自己的产品，但智能音箱的竞争是资本的竞争，只有足够的钱才能买足够优质的媒体版权，才能雇佣优秀的人工智能人才，才能不计成本推广自己的产品。在这场竞争中，百度是最没有希望的，一个对自己过去的产品三心二意的公司是无法做出好的产品的；腾讯有一定的希望，因为它足够财大气粗，但腾讯的强项是社交，除非沉溺社交的用户，否则来一个跟钉钉一样的使用体验真的很糟糕吧；阿里也财大气粗，但除非这货敢往里面加广告，否则与其已有的业务关联不大，不过这与其做平台的思路是一致的，搭建一个生态系统，让别家来完善；小米似乎有最正当的理由来做这件事，因为智能音箱正好是智能家居的入口，但真正geek到生活的人毕竟少数，而且由于其在媒体资源上相对弱，所以版权上不占优势；京东做这个真心不看好，基本没有什么技术积累和优势；喜马拉雅和yeelight都属于小厂，没法通过补贴推广自己的设备，比较难有前途。 现在我倒是期待网易的表现，毕竟它一直是以一种有格调的生活为定位的，且其版权资源也算不错。 2018-04-17 美国财政部公布的2018年2月数据显示，中国所持美国国债规模较上月增加85亿美元至1.1767万亿美元，为美国第一大债权国。而在1月，中国则减持了167亿美元，当时持仓规模创下去年7月来的新低。 中国仍然是美债的最大买家。 美国商务部向中兴通讯发出出口权限禁止令。针对中兴通讯发出出口权限禁止令意味着，该公司将被禁止参与接受美国政府出口管理条例管辖的“任何形式的任何交易”，美国出口管理条例管制范围涵盖敏感技术的海外出口。(彭博) 招商电子4月17日研报关注中兴通讯禁运事件。招商电子认为，中兴通讯的三大应用领域里，芯片门槛最高的板块是RRU基站，这一领域要想实现国产替代，需要较长时间。光通信和手机产业链门槛相对较低，一些细分领域的国产芯片方案甚至于成为了国际龙头，但整体来看，还是偏低端应用。本次中兴通讯的禁运事件，对于通信产业冲击较大，也敲响了半导体产业的警钟，自主可控不仅仅是口号，而是涉及到国家安全，国计民生的要务。 美国商务部禁止美国公司向中兴出售零部件等产品 7 年。美国商务部称，2016 年 3 月中兴已被限制出口，由于中兴在缓期执行期间再次向美国政府做出虚假陈述，违反之前关于向伊朗和朝鲜非法出口电信设备的“和解协议”，商务部依据协议颁布出口禁令，禁止所有美国企业和个人以任何方式向中兴出售硬件、软件、技术服务，期限 7 年，立即执行。中兴手机等产品也将受制裁影响，无法获得零部件 这次贸易战时刻提醒中国的国家安全仍然受到很强的挑战。如果没有微软、Intel、Google中国的IT产业起码倒退20年…因此，应该极度看好中国本土的芯片产业，例如紫光国芯（虽然这货已经很贵），只有拥有可选择的余地才能保障自身安全。 李大霄：降准对稳定经济增长和股市有正面作用 今日央行决定4月25日起下调部分金融机构存款准备金率1个百分点。对此，英大证券首席经济学家李大霄表示，央行对部分金融机构降准及置换中期借贷便利（MLF）的操作，目的在于增加长期资金供应，降低企业融资成本，释放4000亿元增量资金，增加了小微企业贷款的低成本资金来源，解决小微企业融资难融资贵的问题，对于稳定经济增长有正面作用，对于股票市场的稳定也有非常好的正面作用，属重大利好消息。 国金李立峰：货币政策由中性偏紧转向适度扩大内需 中国人民银行决定，从2018年4月25日起，下调大型商业银行、股份制商业银行、城市商业银行、非县域农村商业银行、外资银行人民币存款准备金率1个百分点。对此，国金证券李立峰表示，随着外需今年的高度不确定性，今年货币政策理应有所转向，由中性偏紧转向适度扩大内需。今天披露的“央行针对部分银行实施定向降低存款准备金率”，正印证了我们的这一看法：货币政策在微调转向。 货币政策在由严转宽，这似乎意味着某种资产（股票或房子）又开始要涨价了。所谓“照顾”中小企业不知道能怎么执行。接下来应该关注房产限购方面的变化。 俄罗斯政府：普京和默克尔通电话探讨了叙利亚问题。 似乎从某个时间点起英法就已经与美国穿一条裤子了，已经不再独立思考问题了。而德国则有一定的独立性，在此中扮演一种特定的角色。 美国财长Mnuchin：特朗普有关中俄汇率的言论是对两国的“鸣枪警告”，特朗普希望确保中国不会像过去那样让本币贬值。 中美金融战远远没有结束，也不会以这种不清不楚的形式结束。 2018-04-18 美国总统特朗普确认已与朝鲜领导人金正恩直接通话。 期待有进一步的内容曝露。 记者17日从国家发改委获悉：新的外商负面清单将于今年上半年尽早公布实施，包括2018年及未来几年的开放措施。新的外商投资负面清单将把制造业开放作为重点。汽车行业将分类型实行过渡期开放，2018年取消专用车、新能源汽车外资股比限制；2020年取消商用车外资股比限制；2022年取消乘用车外资股比限制，同时取消合资企业不超过两家的限制。通过5年过渡期，汽车行业将全部取消限制。 国产汽车扶植这么多年了，也没见什么起色，而现在放开估计也有外部的压力吧。对于汽车行业真不是好消息，回头想想中国的国产服装，随着国外品牌的进入，什么美特斯邦威、森马、班尼路之流都消失不见了。实际上，随着人们消费能力的上升，并非不能支撑起好的品牌，只是现在资金大都流入一些新兴行业，传统行业的从业人员收入不够，无法聚集足够好的人才，归根结底还是钱的问题。 华尔街日报：以色列据称在美国对叙利亚行动上与美国进行了协商。 美国无法亲自出面的情况下，是否会依赖这个中东盟友做点什么呢？ 福特表示对于中国有关外资股比限制的公告感到鼓舞。福特发言人在一封电子邮件声明中表示：“中国发改委的声明让我们感到鼓舞，这清楚地表明了中国政府进一步开放汽车行业的承诺。” 对于福特和特斯拉，这的确是利好。 美联储Williams：真正的收益率曲线倒挂会发出强有力的经济衰退信号。 过去的一个月变化不明显。 美国商务部向中兴通讯发出出口权限禁止，将禁止美国公司向中兴通讯销售零部件、商品、软件和技术7年。去年底工信部发布中国光电子器件产业技术发展路线图（2018-2022年）指出，核心、高端光电子器件落后已经成为制约我国信息产业发展瓶颈。目前高速率光芯片国产化率仅3%左右。政策要求在2022年中低端光电子芯片的国产化率超过60%，高端光电子芯片国产化率突破20%。A股公司可关注光迅科技、新易盛。 路透社援引知情人士消息称，美国限制中兴通讯使用美国制造的零部件之后，中兴通讯可能无法在其移动设备上使用谷歌的Android操作系统。消息人士补充称，中兴通讯和Alphabet公司一直在讨论美国政府禁令的影响，不过截至周二上午，两家公司尚未就中兴使用Android的问题做出决定。（中新社） Andorid都被限制，各种软硬件的限制很严重。 根据美国4月17日提交WTO的文件《美国对来自中国的某些商品的关税措施》，美国表示愿意与中国进行磋商。 终于要走到谈判桌前了吗？","categories":[{"name":"News","slug":"News","permalink":"liqul.github.io/blog/categories/News/"}],"tags":[]},{"title":"Raft协议实现学习之—写入过程","slug":"etcd_raft_4","date":"2018-11-23T13:22:31.847Z","updated":"2019-03-06T14:47:06.859Z","comments":true,"path":"etcd_raft_4/","link":"","permalink":"liqul.github.io/blog/etcd_raft_4/","excerpt":"在上一篇文章重点梳理了选举的过程，而这一篇想着重梳理一下写入的过程。仍然沿着节点初始化的日志开始： 12345678910111213141516# ...# 阶段7：节点1成为leader后向其它节点广播MsgApp17:40:19 1-&gt;2 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 3-&gt;1 MsgVoteResp Term:2 Log:0/017:40:19 INFO: raft.node: 2 elected leader 1 at term 217:40:19 1-&gt;3 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/4 # 这里2汇报已经保存了新的entry17:40:19 INFO: raft.node: 3 elected leader 1 at term 217:40:19 1-&gt;2 MsgApp Term:2 Log:2/4 Commit:4 # 在这里commit从3变成417:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/4 17:40:19 node 1: processing entry: &#123;2 4 EntryNormal [] []&#125; # 由于已经确认大部分节点都保存成功，可以apply到state machine17:40:19 1-&gt;3 MsgApp Term:2 Log:2/4 Commit:417:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 2: processing entry: &#123;2 4 EntryNormal [] []&#125; # 2在接收到来自1的MsgApp后得知commit=4，可以apply到本地的state machie17:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 3: processing entry: &#123;2 4 EntryNormal [] []&#125;","text":"在上一篇文章重点梳理了选举的过程，而这一篇想着重梳理一下写入的过程。仍然沿着节点初始化的日志开始： 12345678910111213141516# ...# 阶段7：节点1成为leader后向其它节点广播MsgApp17:40:19 1-&gt;2 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 3-&gt;1 MsgVoteResp Term:2 Log:0/017:40:19 INFO: raft.node: 2 elected leader 1 at term 217:40:19 1-&gt;3 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/4 # 这里2汇报已经保存了新的entry17:40:19 INFO: raft.node: 3 elected leader 1 at term 217:40:19 1-&gt;2 MsgApp Term:2 Log:2/4 Commit:4 # 在这里commit从3变成417:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/4 17:40:19 node 1: processing entry: &#123;2 4 EntryNormal [] []&#125; # 由于已经确认大部分节点都保存成功，可以apply到state machine17:40:19 1-&gt;3 MsgApp Term:2 Log:2/4 Commit:417:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 2: processing entry: &#123;2 4 EntryNormal [] []&#125; # 2在接收到来自1的MsgApp后得知commit=4，可以apply到本地的state machie17:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 3: processing entry: &#123;2 4 EntryNormal [] []&#125; 在raft.becomeLeader方法的实现里，节点一旦选举成为leader会主动向自己的日志里写入一条空日志： 1r.appendEntry(pb.Entry&#123;Data: nil&#125;) 注意这条空日志并非前面提到过的dummy entry，而仅仅是内容为空的一条真实的日志，所以在上面程序输出的日志里，可以看到节点1将该日志复制到节点2和3的过程。当然，正常的写入流程的入口在node.Propose，其中会将用户给的数据组织成一个MsgProp然后发送到node的propc等待处理。 在Raft协议下，只有leader能处理来自client的写入请求，如果其它follower节点接收到请求也会转发给leader。leader和follower处理MsgProp的逻辑自然是不同的，分别在raft.stepLeader和raft.stepFollower里定义。 123456// leader的处理逻辑r.appendEntry(m.Entries...)r.bcastAppend()// follower的处理逻辑r.handleAppendEntries(m) Leader的处理逻辑非常简单，将日志写入unstable，然后广播到所有followers。这里需要展开说明follower的处理情况： 1234567891011121314151617181920/// handleAppendEntries的处理逻辑// 正常运行情况下m.Index应该等于r.raftLog.committedif m.Index &lt; r.raftLog.committed &#123; r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed&#125;) return&#125;// 注意这里几个参数的含义// index是新entries的前一个entry的index// logterm是新entries前一个entry的term// commit是leader最大的committed entry的index// entries是leader复制到follower的日志，但需要注意这些日志可能&lt;commitif mlastIndex, ok := r.raftLog.maybeAppend(m.Index, m.LogTerm, m.Commit, m.Entries...); ok &#123; r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: mlastIndex&#125;)&#125; else &#123; r.logger.Debugf(\"%x [logterm: %d, index: %d] rejected msgApp [logterm: %d, index: %d] from %x\", r.id, r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(m.Index)), m.Index, m.LogTerm, m.Index, m.From) r.send(pb.Message&#123;To: m.From, Type: pb.MsgAppResp, Index: m.Index, Reject: true, RejectHint: r.raftLog.lastIndex()&#125;)&#125; 理解这里逻辑的关键在于了解maybeAppend方法的含义。Follower在接收到一组来自leader的日志后需要判断这些日志是否能追加到自己本地的日志。MsgApp中的index和logterm分别是leader节点新复制的entries的前一条日志的index和term。可以再仔细阅读论文里Figure 3.1里AppendEntries RPC的结构说明。在正常运行状态下，这条日志应该是已经被commit的最后一条日志，从本文最开始的程序输出日志里就能看到： 117:40:19 1-&gt;2 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"] 这条程序输出日志里打出了leader当前处于term2，而新entries的前一条日志是配置peer的日志，其term和index分别为1和3，leader最后commit的index也是3，最后新entry里的term和index分别为2和4。接收到这条记录的follower根据前一条日志的index来判断leader与自己的日志是否能匹配，即不存在中间漏掉日志的情况。判断的依据是matchTerm(index, logTerm)，即前一条日志是否已经在本地保存，如果没有说明中间存在漏洞。如果新来的日志可追加到本地，用户线程会根据接收到的Ready结构将日志持久化，然后给leader返回一条消息，如下： 117:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/4 这条消息里包含了写入日志的index，这里是4，通知leader index=4的日志已经在节点上成功复制。Leader节点在接收到MsgAppResp消息后，在raft.stepLeader下处理消息，处理逻辑如下： 123456789101112131415161718192021222324252627// 在正常情况下，会进入如下判断if pr.maybeUpdate(m.Index) &#123; // 在这里面可能会更新peer的progress // peer的状态转移，参考raft/design.md文件说明 switch &#123; case pr.State == ProgressStateProbe: fmt.Printf(\"%x become replicate\\n\", m.From) pr.becomeReplicate() case pr.State == ProgressStateSnapshot &amp;&amp; pr.needSnapshotAbort(): r.logger.Debugf(\"%x snapshot aborted, resumed sending replication messages to %x [%s]\", r.id, m.From, pr) pr.becomeProbe() case pr.State == ProgressStateReplicate: pr.ins.freeTo(m.Index) &#125; if r.maybeCommit() &#123; // 这里判断是否有新的待commit的日志 r.bcastAppend() &#125; else if oldPaused &#123; // If we were paused before, this node may be missing the // latest commit index, so send it. r.sendAppend(m.From) &#125; // 可能继续向落后的节点发日志 for r.maybeSendAppend(m.From, false) &#123; &#125; // ...&#125; 上面代码里的raft.maybeCommit的判断依据是大部分节点是否都已经复制了新的日志。如果判断成立，leader节点会调用raft.bcastAppend，从实现代码里可以看到即使没有新的日志，leader也会发送空信息来传达新的commit消息。所以，观察文章最前面的程序输出可以看到 117:40:19 1-&gt;2 MsgApp Term:2 Log:2/4 Commit:4 这里节点1向2发出的MsgApp消息里附带的commit已经由3变成4了。Leader节点一旦更新commit消息，在用户线程获得这个信息后（通过Ready结构）就可以把这条日志应用到状态机了，于是有 117:40:19 node 1: processing entry: &#123;2 4 EntryNormal [] []&#125; 至此，一条日志的写入过程结束。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"}]},{"title":"从Raft到MultiRaft","slug":"multiraft","date":"2018-11-01T08:52:03.000Z","updated":"2019-03-06T14:47:42.480Z","comments":true,"path":"multiraft/","link":"","permalink":"liqul.github.io/blog/multiraft/","excerpt":"Raft是一种共识算法，在之前的文章里已经提到过。简而言之，每次集群处理一次请求，都需要经过集群中大部分节点协商。所以一个Raft集群的规模一般不会太大，否则协商的代价就会比较大。那么如果希望基于Raft实现一些规模比较大的服务该怎么扩展呢？","text":"Raft是一种共识算法，在之前的文章里已经提到过。简而言之，每次集群处理一次请求，都需要经过集群中大部分节点协商。所以一个Raft集群的规模一般不会太大，否则协商的代价就会比较大。那么如果希望基于Raft实现一些规模比较大的服务该怎么扩展呢？ 例如我们想做一个kv存储，那么一个简单的想法是把key分为多个range，然后不同的range由不同的Raft集群来控制。实际上，MultiRaft的思想就是这么简单…只是在实现上有一些细节需要考虑。如果希望更多理解MultiRaft的概念，可以读读这篇文章，还有这里。从中可以发现，MultiRaft解决的两个核心问题分别是： 共享物理节点的问题：多个Raft集群实际上是共享物理节点的，所以需要小心组织每个节点上的数据； Heartbeat过多的问题：每个Raft集群逻辑节点需要处理Heartbeat消息，如果每个物理节点上都有多个Raft逻辑节点，那么开销会比较大，所以希望Heartbeat以物理节点为单位而不是逻辑节点。 如果考虑跨Raft集群操作，实际上还有一个问题，就是如果一次操作跨不同的Raft集群怎么办？如果服务不需要提供事务那其实是没有问题的，但如果需要呢？现在使用MultiRaft的两个服务Cockroachdb和Tidb都有文档说明： Cockroachdb：看这里和这里； Tidb：看这里。 Cockroachdb的思路比较容易理解，也跟我想的差不多，而Tidb的则没有看明白，尤其是关于锁的问题。 下面按照我自己的理解来说明。首先，数据需要以MVCC方式存储，即每个kv保存多个版本，例如： key value commit state a 1 1 stable a 2 2 unstable b 1 1 stable 每个kv除了key和value额外保留两个字段，分别是commit和state。在这里stable代表一次事务已经完成，可以被外界读取的情况；反之，如果是unstable，表示事务没有完结，对外不可见。 在一次写入的时候，如果所涉及的数据都分布在一个Raft集群内，那么是不需要考虑事务的，因为这些变更可以记做一条Raft日志，从而达到事务的效果。只有跨多个Raft集群时才需要用Two-phase commit (2PC)来达到整体的事务效果。 在2PC的第一个阶段，每个Raft集群完成写入后，内部节点的状态（即一个kv map）对应的state都是unstable，表示这时候只是单个Raft完成写入，还需要等待2PC coordinator确定是否所有Raft集群都完成写入。数据里的commit是事务的编号，这可以由一个独立的服务来产生事务编号，保证commit单调递增。当所有Raft集群都写入成功，2PC进入第二个阶段，由coordinator向所有集群通告已经成功的commit号，接收到该信息后各个Raft集群将commit对应数据的state由unstable改变成stable，一次事务完成。 总的来说，MultiRaft是对Raft的一种扩展。但是，MultiRaft还不方便简单抽取出来作为一种可供其它应用直接使用的库，与业务逻辑的关联性比较强。不过，有了Cockroachdb和Tidb的实际应用，对其它类似的存储结构的扩展是很好的参考。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"}]},{"title":"《人为制造的脆弱性》--读后感","slug":"banks","date":"2018-09-20T14:04:03.000Z","updated":"2018-09-20T14:35:14.167Z","comments":true,"path":"banks/","link":"","permalink":"liqul.github.io/blog/banks/","excerpt":"断断续续读了一遍《人为制造的脆弱性》。作者在第一部分里已经基本讲明了自己的观点，后面几部分只是列举一些国家佐证自己的论点，所以我也只重点看了前面前两个部分，后面的部分大致翻看了几眼。这本书现在可以在网上直接看。 刚开始读的时候就被前两段话吸引了，一个好的开头的确能起到吸引读者的作用，贴在下面： 每个人都知道生活是不公平的，即所谓“政治在起作用”。当最钟爱的电影痛失奥斯卡奖时，我们会这样说；当办公室最底层的蠢人陪领导打高尔夫而获得了本应属于我们的升职机会时，我们会这么说；当发现一座毫无用途的大桥得以建成，仅仅是因为某个很有权势的参议员将联邦基础设施建设资金带回了家乡时，我们会这么说；当发现一个有关系的企业家获得数百亿政府补贴去组建一个根本不可能有竞争力的公司时，我们也会这么说。","text":"断断续续读了一遍《人为制造的脆弱性》。作者在第一部分里已经基本讲明了自己的观点，后面几部分只是列举一些国家佐证自己的论点，所以我也只重点看了前面前两个部分，后面的部分大致翻看了几眼。这本书现在可以在网上直接看。 刚开始读的时候就被前两段话吸引了，一个好的开头的确能起到吸引读者的作用，贴在下面： 每个人都知道生活是不公平的，即所谓“政治在起作用”。当最钟爱的电影痛失奥斯卡奖时，我们会这样说；当办公室最底层的蠢人陪领导打高尔夫而获得了本应属于我们的升职机会时，我们会这么说；当发现一座毫无用途的大桥得以建成，仅仅是因为某个很有权势的参议员将联邦基础设施建设资金带回了家乡时，我们会这么说；当发现一个有关系的企业家获得数百亿政府补贴去组建一个根本不可能有竞争力的公司时，我们也会这么说。 我们意识到政治无处不在，有时却认为银行业危机是去政治化的，就如同地震和冰雹灾害一样，银行业危机是一种无法预知的非常规情形。我们之所以相信这种观点，是因为它作为一种正式言论屡屡被央行和财政部官员们提及，然后又被财经记者和电视台主播不断重复。在这一不断重复的故事里，那些有着良好意图且专业娴熟的人竭尽全力去建立高效的金融机构，有效分配信贷，并处理不断出现的问题。但他们不是全能的。他们不能预见每一个可能出现的意外事故，有时还会遭遇一连串噩运。“经济冲击”可能无法预见，它会动摇原本平稳运行的经济体系。在这个故事里，经济危机就如列夫托尔斯泰所说的“不幸的家庭各有各的不幸”。 读完这两段话后，我意识到我以前也是这样认为的。我不知道我的这种思维根源来自哪里，或许是像我以前对历史的认知一样，认为“天下大势，合久必分分久必合”，所以王朝更替不过是个自然现象。但是当你仔细检查每个场景的时候，却发现这种宏观的高度概括的理解是毫无意义的，因为你无法总结任何可执行的经验。另一方面，作者提出需要极力避免从个人道德方面来得出结论，因为个人道德是一种随机事件，而隐藏在个人随机事件底下的趋势才是值得研究的。 图1:不同政治体制下的银行业 这本书的中心思想实际上也就在这两段话里：银行业是与政治是密不可分的。为什么呢？因为无论在民主国家还是威权国家，银行业总是不可避免的依赖政府提供的各项“照顾”，而政府也依赖银行来达到其政治目的。在不同的政治体制下，银行业和政治的关系表现形式是不同的。例如在威权政治下，不是谁想办银行就能做到的，往往政府控制着严格的准入制度。反过来，如果一家银行被批准了，那么意味着银行需要协助政府来完成一些政治目标，例如优先贷款给政府扶持的行业。如果在腐败的政治环境下，银行甚至可能只贷款给政府和银行内部人士开办的公司。作为经济补偿，政府也往往需要给银行兜底以减少其风险，同时利用暴力机器维护银行的利益。作者以墨西哥为例，详细讨论了墨西哥银行业的发展历史。正是由于威权政府治下的银行通常只照顾政府直接或间接指定的企业，范围之外的企业难以获得信贷，所以没有特殊关系的民营企业很难利用银行提供的信贷来发展和壮大。 看到这里想起在去年底之前，我一直以为银行的主要业务时吸纳储户的存款，并把存款放贷出去收取利息。这种想法真是“很傻很天真”，实际上现代商业银行体系承担着“发行”货币的职能。可以看看知乎上这个问题的回答。简而言之你存一张100元的人民币到银行，在现在的制度下，银行大致能放贷700元。你存3年定期的利息可能只有3.2%左右，而贷款的利率大概在6%左右。在各行各业中，想必能如此做生意的也就只有银行了吧。那么是谁允许它们这样做的呢？ 民主政治看起来要好的多，但仍然避免不了政治和银行业的勾结而导致恶果。作者以美国为例，从银行业的角度介绍了08年次贷危机的前因后果。虽然次贷危机在08年爆发，但早在90年代就已经埋下伏笔。随着全球化的发展，美国的贫富分化加剧，出现了一个批聚集在城市的低收入者，导致社会矛盾不断尖锐。 图2:次贷危机中的四方同盟 随着人数增多，这些低收入者逐渐形成了一股政治势力。在民主选举的国家里人数众多就能产生足够的政治影响力。进而国会选举时，选举人也不得不考虑得到他们的支持。但如何才能得到支持呢？政治家无法直接为他们提供高福利，因为那需要动用财政收入（即税收），直接侵害其他纳税人的利益。于是“聪明”的政治家们想到了一个绝妙的主意——为他们提供廉价的贷款来让他们过上好日子。然而，代表着政治家的国会无法直接迫使银行来做这种看起来低回报高风险的生意，所以还需要一个机会。这个机会就是上世纪八九十年代的美国银行业扩张。 美国本土的银行业在早期的状况是全国各地遍布小银行，却没有形成全国性的大银行，而这源于美国人根深蒂固的一些观念。《美联储的诞生》这本书里就反复的在强调美国人对全国性的银行极度反感，认为这会侵害所有人的利益。而这一问题在八九十年代发生了改变，政府开始允许银行间的兼并，但前提是要得到银行当地社区的同意。银行希望一些低收入社区批准兼并计划，而低收入者则希望获得廉价贷款。这似乎与国会没有关系，当然不是，因为银行是不可能自己来承受这种高风险的生意。于是政府拉入了第四个博弈方，即两房（房利美和房地美），由两房来购买银行的贷款从而降低银行自身的风险。当然，在国会的支持下，两房享受着很多政治特权，例如可以提供更低的保证金和加更高的杠杆。就这样，一个四方同盟形成了。 在次贷危机爆发的前几年里，这个同盟在不断的强化自己。住房贷款从最初的要求20%首付和收入证明变成仅需3%的首付且不需要收入证明。房地产的泡沫不断加剧，监管部门对此则视而不见。并非他们道德败坏，或愚蠢到看不到泡沫的存在，而是在那个时间点，任何反对政策都很难被执行，因为那样做似乎会损害所有同盟成员的利益。看到这里，就好像所有人都在狂欢，冷静的人只能悄悄离场而无法提醒别人。如果所有人都不愿意停止这场狂欢，最后会由谁来买单呢？答案是隐藏在背后的纳税人。如果一个企业已经能做到“大而不倒”，那么就意味着所有纳税人都需要为它买单，不管你愿意还是不愿意。 同样的事情不会发生两次，但类似的事情却在不断重复，次贷危机中这种资产是房地产，而下一次则可能是股票。因此，要做的并不是去空谈所谓水满则溢、月盈则亏，因为那没有任何可操作性。当然，对于无法承受任何风险的人来讲，永远不要触碰投资或许是最为稳妥的策略，但那仍然无法摆脱你被动为一些“大而不倒”的东西买单，同时你也失去了利用这种坏的机遇的机会。就好比中国的房价，即使你认定中国的房价不合理，也完全没有必要带有情绪去声讨，如果你能看清楚房价上涨的本质，并且你有能力早一点买房，至少你应该明白这可以抵御通胀造成的货币贬值。按照达里奥（Ray Dalio）的意思，在整体环境加杠杆的过程中，如果你不加则会有机会成本，而且物价上涨也会导致你实际收入下降。但是，如果在降杠杆的过程中，你还要逆势加杠杆，恐怕就是自找麻烦了。如果理性看待，泡沫的破裂其实是自由市场自我修复的一个重要手段，因为人性是贪婪的，所以做到理性的及时调整就变得非常困难，所以不得不借助这种阵痛来实现动态平衡。 如果能顺应这种趋势，即使不能赚到很多钱但也至少能抵抗不必要的损失。当然，如何能洞察这些经济潜流呢？我也不知道，但我知道每一个趋势都不是一两天，甚至不是一两年能形成的。好比次贷危机经历了90年代末的积累，在08年才真正爆发出来。而在这漫长的过程中，如果理性观察应该是能发现这些问题的。但是，如果你沉迷于自己的思维习惯而停止学习，那么即使一些显而易见的现象摆在你眼前，你也会熟视无睹的。","categories":[{"name":"Reading","slug":"Reading","permalink":"liqul.github.io/blog/categories/Reading/"}],"tags":[]},{"title":"Raft协议实现学习之—etcd/raft库的使用","slug":"etcd_raft_2","date":"2018-09-13T07:50:03.000Z","updated":"2018-10-08T08:32:21.000Z","comments":true,"path":"etcd_raft_2/","link":"","permalink":"liqul.github.io/blog/etcd_raft_2/","excerpt":"Etcd中的Raft协议实现Etcd里Raft协议的实现的在这里。etcd并不是Raft的唯一实现，事实上Raft协议有许多实现，还有我简单尝试过的atomix。选择etcd/raft是因为个人对etcd和go语言比较感兴趣，而且Raft论文里提到的实现方法恰好比较适合用go语言的goroutine来实现。 另外的一个原因正如etcd/raft在github上的readme所说： To keep the codebase small as well as provide flexibility, the library only implements the Raft algorithm; both network and disk IO are left to the user. Library users must implement their own transportation layer for message passing between Raft peers over the wire. Similarly, users must implement their own storage layer to persist the Raft log and state.","text":"Etcd中的Raft协议实现Etcd里Raft协议的实现的在这里。etcd并不是Raft的唯一实现，事实上Raft协议有许多实现，还有我简单尝试过的atomix。选择etcd/raft是因为个人对etcd和go语言比较感兴趣，而且Raft论文里提到的实现方法恰好比较适合用go语言的goroutine来实现。 另外的一个原因正如etcd/raft在github上的readme所说： To keep the codebase small as well as provide flexibility, the library only implements the Raft algorithm; both network and disk IO are left to the user. Library users must implement their own transportation layer for message passing between Raft peers over the wire. Similarly, users must implement their own storage layer to persist the Raft log and state. etcd/raft采取了一种极简的实现方式，只有最核心的状态转移逻辑，不包含网络通信和磁盘读写，所以比较方便对照论文来梳理代码的实现。 Etcd/raft与用户系统的关系图1描述了基于etcd/raft完成一个分布式存储系统的一般结构。每个节点分为两部分，左边是用户需要自己实现的部分，右边代表etcd/raft库。在用户实现的逻辑中，首先是系统状态（State machine）。系统状态是与应用相关的，例如对于key-value store来说，它的系统状态可以描述为一个key-&gt;value的映射关系。正是由于系统状态是应用相关的，所以它必须由用户来实现，同时用户还必须实现系统状态的序列化和反序列化用于快照。 图1. Raft实现模块关系示例实现的是一个key-value store，而其主要的结构体如下 需要特别注意系统状态并不是Raft协议的一部分。在基于Raft实现分布式系统的时候，可以采用这样的思路：先实现一个单机的功能齐备的系统，然后再利用Raft协议将它扩展到多节点模式。显然在这个单机系统中，系统状态及其维护都需要用户自己来实现，所以它并不是Raft协议的一部分。当然，并不是所有的单机系统都可以直接依赖Raft协议扩展到多个节点的，因为Raft协议本质上是多节点日志备份系统，这要求系统状态必须能描述为一组日志序列。 网络模块（Net）和持久化存储（Persistent store）分别提供节点间通信能力和持久化能力。Raft协议需要节点间通信来协同操作，而etcd/raft库本身不实现通信功能，而是每当节点间需要通信时把消息交给用户实现的网络模块发送到其它节点。同样，etcd/raft也不提供持久化能力，而节点需要将一些有用信息保存在持久化存储中，以便能在程序意外退出恢复时读取这些信息。 Etcd/raft库里与用户实现交互的逻辑由node完成（代码在node.go）。Node接收来自用户实现逻辑的大部分输入（除来自其它节点的网络消息外），此外还负责将raft状态机（代码在raft.go）输出的行动通过Ready结构发送到用户实现逻辑。用户逻辑与Raft之间共享一个存储结构Storage（代码在storage.go），对于两边都要读写的结构，需要锁来避免读写冲突。 一个可运行的示例代码库里已经有一些关于如何使用的示例，但并没有一个完整的例子。而官方给出的raftexample例子由于依赖了etcd里的rafthttp和wal，引入了一些不利于梳理的内容。所幸这篇blog里作者提供了一个可以运行的最简单的示例，方便用作学习，对应的代码库在这里。由于作者实现的时间较早，代码引用的是旧的库地址 1234567\"github.com/coreos/etcd/raft\"\"github.com/coreos/etcd/raft/raftpb\"//改成下面\"go.etcd.io/etcd/raft\"\"go.etcd.io/etcd/raft/raftpb\" 主体的数据结构如下： 12345678910type node struct &#123; id uint64 //raft节点的id ctx context.Context //context pstore map[string]string //用来保存key-&gt;value的map store *raft.MemoryStorage //raft需要的内存结构 cfg *raft.Config //raft需要的配置 raft raft.Node //前面提到的node ticker &lt;-chan time.Time //定时器，提供周期时钟源和超时触发能力 done &lt;-chan struct&#123;&#125; &#125; 上面的代码示例里pstore实际上就是key-value store的存储结构。为了使用etcd/raft库，按照github里的readme所说，用户需要完成下面一些功能： First, read from the Node.Ready() channel and process the updates it contains. These steps may be performed in parallel, except as noted in step 2. 1.. 2.. 3.. 4.. 用户需要监听和处理Ready消息，上面引用里的4点内容较多在此省略。其中的要点是在执行任何操作之前都需要先持久化一些状态，这与上一篇文章中图2的工作原理是对应的。 Second, all persisted log entries must be made available via an implementation of the Storage interface. The provided MemoryStorage type can be used for this (if repopulating its state upon a restart), or a custom disk-backed implementation can be supplied. 用户需要实现Storage接口来存储必要的信息。MemoryStorage是库提供的一个基于内存的实现，本身并不能进行持久化。用户能自己增加逻辑来实现MemoryStorage的持久化。 Third, after receiving a message from another node, pass it to Node.Step(). 如果接收到来自其它节点的消息，通过Step方法传递到raft的状态机。 Finally, call Node.Tick() at regular intervals (probably via a time.Ticker). Raft has two important timeouts: heartbeat and the election timeout. However, internally to the raft package time is represented by an abstract “tick”. 提供一个周期性的时钟定时触发Tick方法。 在示例实现里完成上述逻辑的代码如下（以First、Second、Third、Finally与上面的描述对应）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445// Ready的处理逻辑func (n *node) run() &#123; for &#123; select &#123; case &lt;-n.ticker: // Finally n.raft.Tick() case rd := &lt;-n.raft.Ready(): //First n.saveToStorage(rd.HardState, rd.Entries, rd.Snapshot) //First.1 n.send(rd.Messages) //First.2 if !raft.IsEmptySnap(rd.Snapshot) &#123; n.processSnapshot(rd.Snapshot) //First.3 &#125; for _, entry := range rd.CommittedEntries &#123; // 对于key-value store这个应用，用户真正需要关心的只有下面这行 n.process(entry) //First.3 if entry.Type == raftpb.EntryConfChange &#123; var cc raftpb.ConfChange cc.Unmarshal(entry.Data) n.raft.ApplyConfChange(cc) &#125; &#125; n.raft.Advance() //First.4 case &lt;-n.done: return &#125; &#125;&#125;// Second. 作者并未实现Storage的持久化，所以这里只有MemoryStore的操作func (n *node) saveToStorage(hardState raftpb.HardState, entries []raftpb.Entry, snapshot raftpb.Snapshot) &#123; n.store.Append(entries) if !raft.IsEmptyHardState(hardState) &#123; n.store.SetHardState(hardState) &#125; if !raft.IsEmptySnap(snapshot) &#123; n.store.ApplySnapshot(snapshot) &#125;&#125;// 作者并未实现真正的网络模块，只是模拟了节点间的收发消息func (n *node) receive(ctx context.Context, message raftpb.Message) &#123; n.raft.Step(ctx, message) //Third&#125; Etcd/raft库的使用大致如代码示例里描述。当然，这里做了不少简化，例如： 没有实现节点间的网络通信； 没有实现可持久化的存储； 没有实现快照的生成和处理逻辑； 用户Application是单线程的； 这些将在后续的文章中逐步梳理和学习。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"}]},{"title":"Raft协议实现学习之—概览","slug":"etcd_raft_1","date":"2018-09-13T07:50:03.000Z","updated":"2018-09-26T02:31:59.000Z","comments":true,"path":"etcd_raft_1/","link":"","permalink":"liqul.github.io/blog/etcd_raft_1/","excerpt":"Raft协议概览Raft是什么？Raft是一种共识协议。与Raft完成相同任务的系统有Chubby和Zookeeper，以及一些系统内置的完成类似功能的组件，例如Elasticsearch里的Zen-Discovery。简而言之，共识协议的目的是让一组节点在响应外界输入时能表现的像一个节点一样。共识系统往往应用于有主备结构的存储系统中，如果缺少这种协议，就无法避免数据错误。","text":"Raft协议概览Raft是什么？Raft是一种共识协议。与Raft完成相同任务的系统有Chubby和Zookeeper，以及一些系统内置的完成类似功能的组件，例如Elasticsearch里的Zen-Discovery。简而言之，共识协议的目的是让一组节点在响应外界输入时能表现的像一个节点一样。共识系统往往应用于有主备结构的存储系统中，如果缺少这种协议，就无法避免数据错误。 共识协议非常重要，而且实现起来往往很容易犯错，因此大部分系统都依赖已经久经考验的系统，比如Zookeeper。选择自己实现共识组件需要莫大的勇气，看看这里就知道Elasticsearch曾经出现过许多一致性问题bug，而且还有一些没有被fix。Raft的提出是针对Paxos的，目的是为了提出一种容易被理解的共识协议。容易被理解也就意味着便于维护，不会有许多看起来”黑魔法“一样的技巧。 之前在这篇文章里已经有一些关于Raft的讨论。这次重新看Raft是希望基于etcd/raft的实现来详细梳理一遍其中的细节。Raft对一些需要实现强一致性的操作是如此重要，值得反复学习。 这个系列里一些重要参考的来源： Raft thesis：Raft作者的论文 etcd/contrib/raftexample和etcd/raft：etcd里Raft的实现代码 Raft:a first implementation：一篇有关如何简单使用etcd/raft库的blog 节点的三种状态在一个正常运行状态（即集群能正常处理外部操作请求的情况）下的Raft集群中，节点分为leader和follower两种角色。其lLeader有且仅有一个，负责处理外部的操作请求；follower有多个，它们从属于leader，负责处理来自leader的指令。 图1. Raft节点的状态转移图（原图Figure 3.3） 一个Raft节点有三种状态：follower、candidate和leader。图1中描述了三种状态之间的转移条件。正常运行状态下的节点要么处于follower状态，要么处于leader状态，而candidate属于follower和leader之间的中间状态，也是节点在非正常运行状态下所处的状态。 节点的初始状态是follower。当一个节点启动后，其状态即为follower。此后受到不同事件触发的影响会变更自己状态。Raft集群中的节点都会采用定期”广播“自己的状态。节点处于follower状态下，如果定期能接收到来自leader广播的消息，那么它会一直处于follower状态，而不会转移到candidate状态。 多个节点竞选leader的过程称为leader election，这是Raft协议中最重要的过程之一，将在后续分析代码时再详细梳理。需要稍微注意图1并不是完全的状态转移图，例如图1中并未包含节点在follower状态下接收到广播消息或选举消息的状态变化情况。此外，还有一个细节问题，若某个异常节点主动发起选举，那么其它节点监听到的选举消息里的term比自己当前的更大，那么其它节点是否应该参与选举呢？答案在Raft thesis的4.2.3章。所以图1只是一个基本的状态转移概览。 LogTerm是Raft中用来计量时间的概念，可以理解为阶段。当一个节点被选举为leader后，Raft系统进入稳定运行状态，随后如果leader节点停机则会进入下一轮选举。这里从上一次选举成功到下一次选举之间的时间即一个term。节点初始化的时候term=0,随后term越大也就意味着状态越新，term是单调增加的。处在正常运行状态下的所有Raft节点都有相同的term，所以如果一个节点监听到的来自其它节点的消息里包含的term比自己的更大，那么说明它自己处于异常状态；反之如果监听到消息里的term比自己的更小，那么这样的消息可以直接忽略掉。 图2. Raft协议中的日志备份（原图Figure 3.5） 每条log对应一次请求，也唯一对应一个term。除了term以为，每条log在每个节点上还有一个唯一的index。如图2中所示，index是单调连续增加的自然数，这意味着任意节点上都不会出现index空缺。在稳定情况下，follower的log可以比leader落后，但不会出现不一致。例如，如果leader的index=4对应的log内容是x&lt;-2且term=2，那么在所有follower上index=4的log都应该与leader相同。如果发生leader切换，在短期内有可能出现不一致的情况，但随后leader会要求所有follower完全复制自己的log（不一致的部分将被删除），那么等到稳定状态下，大家的log又会回到一致的状态。 请求处理工作流Raft最核心的工作是使一组节点对来自外界的请求作出一致的反应。在一个正常运行的Raft集群里，只有leader能回应外部请求。这一点需要牢记，如果当前的leader因为停机而失效，那么必须等选举出下一个leader之后集群才能对外服务。在选举的过程中，集群是无法服务的，这会影响服务的HA，但又是得到强一致性操作必须的代价。图2是Raft集群的工作过程示意图。 图3. Raft集群的工作过程（原图Figure 2.1） 在图3中，client发出外部写入请求(1)，Raft集群的leader接收该请求后将请求内容写入本地log，同时将请求内容转发给所有follower(2)。每个follower在接收到请求后将内容写入自己本地log，并通知leader自己已经完成操作（这个步骤图2中没有标出）。leader一旦接收到大多数follower已经完成备份则把请求内容commit到本地的状态机（state machine），完成该操作后回复client写入完成(4)。随后在leader的广播里会包含最新的被commit的log序号，各follower监听到以后将写入操作commit到本地状态机。 图4精确描述了Raft基本协议中各角色的事件处理状态机。一些国外课程以Raft来作为课程作业，比如这里，提到： In fact, Figure 2 is extremely precise, and every single statement it makes should be treated, in specification terms, as MUST, not as SHOULD. 文中的Figure 2就是图4。这张图算是对Raft基本协议作了一个非常精确又详尽的总结。 图4. Raft基本协议（原图Figure 3.1)","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"}]},{"title":"Raft协议实现学习之—初始化和Leader Election过程","slug":"etcd_raft_3","date":"2018-09-13T07:50:03.000Z","updated":"2019-03-06T14:46:57.886Z","comments":true,"path":"etcd_raft_3/","link":"","permalink":"liqul.github.io/blog/etcd_raft_3/","excerpt":"实验代码和输出实验代码是基于作者原来的代码稍微修改而来，main函数如下所示:12345678910111213141516171819var ( nodes = make(map[int]*node))func main() &#123; // start a small cluster nodes[1] = newNode(1, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[1].run() nodes[2] = newNode(2, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[2].run() nodes[3] = newNode(3, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[3].run() // Wait for leader election for &#123; time.Sleep(100 * time.Millisecond) &#125;&#125;","text":"实验代码和输出实验代码是基于作者原来的代码稍微修改而来，main函数如下所示:12345678910111213141516171819var ( nodes = make(map[int]*node))func main() &#123; // start a small cluster nodes[1] = newNode(1, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[1].run() nodes[2] = newNode(2, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[2].run() nodes[3] = newNode(3, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) go nodes[3].run() // Wait for leader election for &#123; time.Sleep(100 * time.Millisecond) &#125;&#125; 在上面的这段代码里首先创建了3个节点，然后程序进入休眠等待节点竞争leader。程序的输出如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# 阶段1：节点初始化数据结构17:40:03 INFO: 1 became follower at term 017:40:03 INFO: newRaft 1 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]17:40:03 INFO: 1 became follower at term 117:40:03 INFO: 2 became follower at term 017:40:03 INFO: newRaft 2 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]17:40:03 INFO: 2 became follower at term 117:40:03 INFO: 3 became follower at term 017:40:03 INFO: newRaft 3 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]17:40:03 INFO: 3 became follower at term 1# 阶段2：节点处理配置的peer列表17:40:03 node 1: processing entry: &#123;1 1 EntryConfChange [8 0 16 0 24 1] []&#125;17:40:03 node 1: processing entry: &#123;1 2 EntryConfChange [8 0 16 0 24 2] []&#125;17:40:03 node 1: processing entry: &#123;1 3 EntryConfChange [8 0 16 0 24 3] []&#125;17:40:03 node 2: processing entry: &#123;1 1 EntryConfChange [8 0 16 0 24 1] []&#125;17:40:03 node 3: processing entry: &#123;1 1 EntryConfChange [8 0 16 0 24 1] []&#125;17:40:03 node 3: processing entry: &#123;1 2 EntryConfChange [8 0 16 0 24 2] []&#125;17:40:03 node 3: processing entry: &#123;1 3 EntryConfChange [8 0 16 0 24 3] []&#125;17:40:03 node 2: processing entry: &#123;1 2 EntryConfChange [8 0 16 0 24 2] []&#125;17:40:03 node 2: processing entry: &#123;1 3 EntryConfChange [8 0 16 0 24 3] []&#125;# 阶段3：节点1开启新一轮leader election17:40:19 INFO: 1 is starting a new election at term 117:40:19 INFO: 1 became candidate at term 2# 阶段4：节点1向节点2和3发送MsgVote17:40:19 INFO: 1 received MsgVoteResp from 1 at term 217:40:19 INFO: 1 [logterm: 1, index: 3] sent MsgVote request to 2 at term 217:40:19 INFO: 1 [logterm: 1, index: 3] sent MsgVote request to 3 at term 2# 阶段5：节点2和3处理来自节点1的MsgVote请求17:40:19 1-&gt;2 MsgVote Term:2 Log:1/317:40:19 1-&gt;3 MsgVote Term:2 Log:1/317:40:19 INFO: 2 [term: 1] received a MsgVote message with higher term from 1 [term: 2]17:40:19 INFO: 2 became follower at term 217:40:19 INFO: 2 [logterm: 1, index: 3, vote: 0] cast MsgVote for 1 [logterm: 1, index: 3] at term 217:40:19 2-&gt;1 MsgVoteResp Term:2 Log:0/017:40:19 INFO: 1 received MsgVoteResp from 2 at term 2# 阶段6：节点1成功成为leader17:40:19 INFO: 1 [quorum:2] has received 2 MsgVoteResp votes and 0 vote rejections17:40:19 INFO: 3 [term: 1] received a MsgVote message with higher term from 1 [term: 2]17:40:19 INFO: 1 became leader at term 217:40:19 INFO: 3 became follower at term 217:40:19 INFO: raft.node: 1 elected leader 1 at term 217:40:19 INFO: 3 [logterm: 1, index: 3, vote: 0] cast MsgVote for 1 [logterm: 1, index: 3] at term 2# 阶段7：节点1成为leader后向其它节点广播MsgApp17:40:19 1-&gt;2 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 3-&gt;1 MsgVoteResp Term:2 Log:0/017:40:19 INFO: raft.node: 2 elected leader 1 at term 217:40:19 1-&gt;3 MsgApp Term:2 Log:1/3 Commit:3 Entries:[2/4 EntryNormal \"\"]17:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/4 # 这里2汇报已经保存了新的entry17:40:19 INFO: raft.node: 3 elected leader 1 at term 217:40:19 1-&gt;2 MsgApp Term:2 Log:2/4 Commit:4 # 在这里commit从3变成417:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/4 17:40:19 node 1: processing entry: &#123;2 4 EntryNormal [] []&#125; # 由于已经确认大部分节点都保存成功，可以apply到state machine17:40:19 1-&gt;3 MsgApp Term:2 Log:2/4 Commit:417:40:19 2-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 2: processing entry: &#123;2 4 EntryNormal [] []&#125; # 2在接收到来自1的MsgApp后得知commit=4，可以apply到本地的state machie17:40:19 3-&gt;1 MsgAppResp Term:2 Log:0/417:40:19 node 3: processing entry: &#123;2 4 EntryNormal [] []&#125;# 阶段8：进入心跳阶段17:40:20 1-&gt;2 MsgHeartbeat Term:2 Log:0/0 Commit:417:40:20 1-&gt;3 MsgHeartbeat Term:2 Log:0/0 Commit:417:40:20 2-&gt;1 MsgHeartbeatResp Term:2 Log:0/017:40:20 3-&gt;1 MsgHeartbeatResp Term:2 Log:0/017:40:21 1-&gt;2 MsgHeartbeat Term:2 Log:0/0 Commit:417:40:21 1-&gt;3 MsgHeartbeat Term:2 Log:0/0 Commit:417:40:21 2-&gt;1 MsgHeartbeatResp Term:2 Log:0/017:40:21 3-&gt;1 MsgHeartbeatResp Term:2 Log:0/0 节点的基本数据结构正常情况下，初始化节点的代码在/raft/node.go里的StartNode方法。每个Node结构都关联到一个raft结构（定义在/raft/raft.go里）。其中Node负责与应用线程交互，而raft负责实现Raft协议的状态机，根据Node的输入raft产生相应的输出。 SoftState和HardState在实现里有两种状态，稍微说明一下。在Raft作者的论文里，图3.1里列举了不同节点需要保存的状态，分为Persistent state和Volatile state。其中Persistent state包括currentTerm、votedFor和log[]，这三个数据的前两个定义在HardState结构里（/raft/raftpb/raft.pb.go），而log[]则在Storage里维护，例如MemoryStorage里的ents数组。与论文不同的地方，在HardState里还包含了Commit，即论文图3.1里Volatile state里的commitIndex（todo:理解这里区别的含义）。SoftState与图中的Volatile state没有直接关系，其中包含了节点当前leader的id，以及节点当前的状态（leader还是follower等）。 此外，观察etcd/raft的实现发现，在MemoryStorage的初始化过程中，会向ents里写入一个dummy entry；而在ApplySnapshot方法里会在压缩完历史的entry之后初始化ents时写入一个特殊的entry，仅包含snapshot中最后一个entry的term和index。因此，在任何时刻在ents数组中的第一个元素都不是真正的entry，在一些处理过程中需要注意这一点（例如MemoryStorage的FirstIndex接口实现）。 Log的状态完整的管理entry（或称为log）的结构是raftLog（参考/raft/log.go）。其中主要的属性， 属性 说明 unstable 记录所有收到但未成功复制到多数节点的日志，或尚未持久化的snapshot committed 记录最大的已经被复制到多数节点的日志 applied 记录最大的已经被应用到状态机的日志 storage 记录已经持久化的日志，和最新的已经持久化的snapshot 每个entry都应该按照顺序先是unstable，然后是committed，最后才是applied，这个顺序不能乱。unstable和storage分别记录了未持久化和已持久化的entry和snapshot。 Peer列表Raft论文图3.1里，作为leader，还需要维护两个状态，即nextIndex[]和matchIndex[]。这两者的实现在/raft/progress.go里，其中的主要结构体Progress用于记录每个follower节点的进度。其中match和next的定义分别是： match：follower与leader间一致的最大的entry next：leader下一个要复制到follower的entry 代码库里专门有一个markdown文件来说明这些概念及其使用方式，在/raft/design.md。正常情况下，follower与leader的entry列表应该完全一样，但由于leader处理新接收到的entry；节点故障而导致新一轮选举；有新节点加入，follower与leader之间出现不一致。这时候leader首先要了解follower的进度与自己进度的区别，此时的follower处于probe状态。如果follower接收并成功复制来自leader的entry，那么follower进入replicate状态，leader一次可以发出大于一条entry以提高传输效率。 在未了解follower的进度时，leader的行为是设置match=0和next=lastIndex+1。这样做是假定follower已经复制了所有leader上的日志。下一次leader向其follower发起复制的时候，如果follower实际上落后一些，会reject新的日志，并会告知leader自己的当前状态，leader根据情况再协调后续的发送。这里的逻辑参考/raft/raft.go里的handleAppendEntries实现。 snapshot相关的过程留在后续分析。 增加peer节点通常情况下在集群初始化时都会有多个节点，例如在 1nodes[1] = newNode(1, []raft.Peer&#123;&#123;ID: 1&#125;, &#123;ID: 2&#125;, &#123;ID: 3&#125;&#125;) 里在初始化节点1时就向它的构造方法里传入了三个节点ID。考察StartNode方法（/raft/node.go）可以清楚的看到，对每一个ID，当前都会往日志里主动增加一条ConfChangeAddNode消息。这其实是在模仿节点从网络接收到了增加节点的配置消息，这两者后续的处理逻辑是相同的。 Raft节点的线程逻辑在node.run方法里。Run的逻辑大致是： 等待新的触发事件，例如接收到来自用户线程的写入请求；接收到来自网络的消息等等。 被触发后经过raft.Step来驱动Raft协议状态机，输出一些变化（由containsUpdates来检验）。这些变化可能包括节点状态变化，或者有新的消息要发送，或者有新的log要持久化等等。 如果有新的变化，那么组织一个Ready结构发送给用户线程，具体的网络和持久化操作都在用户线程完成。 回到增加peer节点的情况，以advancec和readyc（run方法开头定义的临时变量）为线索来整理执行的流程。下图梳理了用户线程（app.run）和Raft线程（node.run）的交互逻辑： 图1. 节点初始化peer列表过程中用户线程和Raft线程之间的交互过程 由于在初始化时已经指定了3个节点，所以在检查containsUpdates时会发现更新。后续的过程就如图1所示，在交互过程中用到的一系列channel也标记在图中的箭头上。在初始化的过程中，节点均处于follower角色，所以增加节点仅仅是向peer列表里增加了一些记录。 成为Leader的过程在完成初始化之后，选举的过程是由超时来触发的。注意在初始化过程中，会执行becomeFollower（raft.go)将节点角色设置为follower。其中 123456r.step = stepFollower // 设置step逻辑r.reset(term)r.tick = r.tickElection // 设置tick逻辑r.lead = leadr.state = StateFollowerr.logger.Infof(\"%x became follower at term %d\", r.id, r.Term) 包含设置step和tick逻辑的代码。在follower角色下，节点的超时行为是触发下一次选举；对应的，如果处于leader角色中，超时则是触发发送心跳消息。如果follower节点超时了，会给自己“发送”一个MsgHup消息，进而开始竞选leader。在竞选前的一些逻辑包括： 1234567891011// campaignr.becomeCandidate()voteMsg = pb.MsgVoteterm = r.Term// becomCandidater.step = stepCandidate // 更改step逻辑r.reset(r.Term + 1) // 增加termr.tick = r.tickElection // 处于candidate角色下节点超时触发下一轮选举r.Vote = r.idr.state = StateCandidate 随后，竞选节点需要邀请其它节点提名自己。当然，如果集群是只包含一个节点这种特殊情况，则不需要经过其它节点提名的过程而直接当选。发送邀请的过程如下： 123456789101112131415for id := range r.prs &#123; if id == r.id &#123; continue &#125; // ... // 这里可以与论文原图Figure 3.1里的RequestVote RPC对照理解 // Term: candidate的term // To: candidate的id // Type: MsgVote // Index: candidate最新一条log的index，这里的log包含unstable // LogTerm: candidate最新一条log的term，这里的log包含unstable r.send(pb.Message&#123;Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx&#125;)&#125; 论文3.6章讨论的是在选举过程中的安全问题，即如何避免一个log落后的节点竞选成为leader。作者提出了一个需要保证的性质——Leader Completeness Property，说明如下： If a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered terms. 也就是一旦一条log确认被committed了，那么它一定要出现在下一个竞选成功的leader的日志里。这句话有两个问题： 如何判断一条log已经committed？按照定义，committed的标准是在多数节点上复制成功。具体考虑一次写入操作，首先log通过网络复制到多数节点的日志；然后leader获得复制情况，确定完成复制后在本地commit日志；最后通过心跳消息通知其它节点commitment信息，接收到消息的其它节点在本地commit日志。从这个过程来看，leader和follower角色下的节点对同一条log是否已经被commit的认识时机是不同的，leader先于follower知晓该信息。如果leader已经知晓commitment但没来得及通知其它节点就掉线了，那么这条本来已经被commit的消息会怎么样呢？按照论文的讨论，由于其它节点并不知晓这条log已经被commit，所以如果一个没有包含这条log的节点当选为leader，那么这条本已经复制到多数节点的消息将被抹掉。所以，个人认为更为精确的说法是 一条log被commit，意味着它已经被复制到多数节点，并且多数节点已经知晓来自leader的commit的信息。 如何保证committed的log出现在后续竞选成功的leader？按照上一个问题的逻辑，在MsgVote RPC里每个节点都应该包含自己已知的committed的日志的term和index，这样大家比较以后就能自然得出谁更加“up-to-date”一些。但是对比代码的实现细节，与论文的描述有所区别。在上面的代码片段里，raftLog.lastIndex和raftLog.lastTerm对应了index和term的值。而观察它们的实现，这里的“最后一条log”实际上包含了unstable结构里的数据，也就是包含没有被commit的日志。这样修改有什么影响呢？个人觉得这不会打破Leader Completeness Property，但会影响协议的行为。考虑一条日志被复制到多数节点但没有完成commitment，这时如果leader掉线，下一个被选举的leader一定包含这条尚未被commit的日志，因为那些尚未复制这条日志的节点无法得到足够多节点的支持。 节点接收到MsgVote后判断candidate的日志是否足够新的逻辑在raftLog.isUpToDate。 如何防止节点扰乱选举解决了这两个问题，还有可能出现一种异常情况：如果一个失联节点不断增大自己的term，然后邀请其它正常工作状态下的节点参与选举，会扰乱集群的执行秩序。这个问题在论文的4.2.3章节讨论。 基本思想是如果一个节点能够接收到来自其leader的心跳，那么它不会参与选举。这个逻辑的实现可以查看raft.go下的inLease变量定义。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"}]},{"title":"Reading \"State Management in Apache Flink\"","slug":"flink","date":"2018-09-09T14:47:09.000Z","updated":"2018-09-15T14:40:48.922Z","comments":true,"path":"flink/","link":"","permalink":"liqul.github.io/blog/flink/","excerpt":"Updated on 2018-09-09 I’m playing with Flink (1.6) and Structured Streaming (2.3.1) recently. I’m no expert for either framework, so my opinion is based on my very short experience with each of them.","text":"Updated on 2018-09-09 I’m playing with Flink (1.6) and Structured Streaming (2.3.1) recently. I’m no expert for either framework, so my opinion is based on my very short experience with each of them. In Flink, stream processing is the first class application. There are a lot of nice features to make it easy and flexible to support various streaming scenarios. You can do complex event processing (CEP) and co-processing with multiple input sources. One great feature is that you can attach user defined state to amost any operator. From version 1.6, they provide the so called “broadcast state“ which is very useful for streaming applications with dynamic configurations. With support of transactional write by Kafka, now Flink is able to achieve the end-to-end exactly-once semantic. Here is the blog explaining how they use two-phase commit to implement this. Spark is originally designed for batch processing. Even they now support streaming by introducing Spark Streaming and Structured Streaming, their advantages remains still for batch oriented applications. The data model is tightly coupled with DataSet which makes it more friendly for ETL like data manipulations, but not so nice for operations like event processing. I’m not saying that Spark has no advantage compared with Flink. For example, Spark can scale dynamically to the traffic load. Flink is currently missing this feature due to its more complicated state management (the answer here says that it is a coming feature). Also, I note that in the newly introduced “contineous model”, dynamic scaling is not supported due to similar reason as in Flink. If I want to pick one for ETL only where latency is not the major concern, Spark is actually a good candidate. However, if my main application is event processing, Flink is definitely the better choice. Actually, Flink provides a very nice UI to monitor the application status. In contrast, the Spark UI is way too complicated for stream processing applications. In conclusion, Flink is generally more suitable in stream processing applications. But now, if your application is with highly dynamic traffic load, and latency is not your major concern, pick Spark. Updated on 2018-02-05 I recently read an excellent blog about exactly-once streaming processing. It details typical solutions for exactly-once processing used by various open source projects. No matter if the solution is based on streaming or mini-batch, exactly-once processing incurs a inevitably latency. For example in Flink, the state at each operation can only be read at each checkpoint, in order not to read something that might be rollbacked during a crash. === I recently read the VLDB’17 paper “State Management in Apache Flink”. In one sentence, The Apache Flink system is an open-source project that provides a full software stack for programming, compiling and running distributed continuous data processing pipelines. For me, Flink sounds yet another computation framework alternative to Spark and Mapreduce with a workflow management tool. However, In contrast to batch-centric job management which prioritizes reconfiguration and coordination, Flink employs a schedule-once, long-running allocation of tasks. How exactly does a streaming-centric framework differ from a batch-centric framework? Conceptually, there is no fundamental difference between the two. Any batch processing framework can work “like” a streaming processing framework by reducing the size of each batch to 1. However, in practice, they are indeed different. A batch-centric framework usually involve a working procedure such as 1234567891011batch 1 startdo some jobbatch 1 endupdate some statebatch 2 startdo some jobbatch 2 endupdate some state... Note that the job is started and ended within each batch. In contrast, for a streaming-centric framework, 123456789101112131415start a jobreceiving a new dataprocess the dataupdate some statepass the data to the next jobreceiving a new dataprocess the dataupdate some statepass the data to the next job...end the job This comparison is clear. A job in the streaming-centric framework usually work continuously without being started/stopped multiple times as in a batch-centric framework. Starting and stopping a job usually incur some cost. Therefore, a batch-centric framework usually performs less efficiently compared to a streaming-centric one. Additionally, if the application is mission critical (e.g., malicious event detection), processing data in batch usually means high latency. However, if the task is batch-by-batch in nature, a batch-centric framework usually performs as efficiently as a streaming-centric one. Another problem is about snapshotting. Snapshotting is a key capability for a processing pipeline. A snapshot is consist of both the state and data. The global state of the pipeline is composed of the sub-state of each operator. Each state is either a Keyed state or a Operator state. The former represents all type of states indexed by the key from data (e.g., count by key); the latter is more an operator-aware state (e.g., the offset of data). Snapshotting the data is tricky where Flink assumes that Input data streams are durably logged and indexed externally allowing dataflow sources to re-consume their input, upon recovery, from a specific logical time (offset) by restoring their state. This functionality is typically provided by file systems and message queues such as Apache Kafka Each operator snapshots the current state once processing a mark in the dataflow. With the marks and the snapshotted states of each operator, we can always restore the system state from the last snapshot. One should note that the keyed state is associated with an operator, and therefore, the data with the same key should be physically processed at the same node. Otherwise, there should be a scalability issue. Consequently, there should be a shuffle before such operators, or the data is already prepared to ensure data with the same key is processed at a single node. In conclusion, Flink is great as streaming-centric frameworks have some fundamental advantages over batch-centric frameworks. However, since batch-centric frameworks such as Mapreduce and Spark are already widely applied, there should be really strong motivations to migrate existing systems to this new framework. Moreover, the implementation quality and contributor community are two very important facts for the adoption of a new born framework, while Spark has been a really popular project. Maybe, a higher level project such as the Apache Beam is a good direction. Beam hides the low-level execution engine by unifying the interface. Any application written in Beam is then compiled to run on an execution engine such as Spark or Flink.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"flink","slug":"flink","permalink":"liqul.github.io/blog/tags/flink/"}]},{"title":"Things about replication in Elasticsearch","slug":"things-about-replication-in-elasticsearch","date":"2018-04-18T13:33:09.000Z","updated":"2018-04-18T13:33:55.000Z","comments":true,"path":"things-about-replication-in-elasticsearch/","link":"","permalink":"liqul.github.io/blog/things-about-replication-in-elasticsearch/","excerpt":"Updated on 2018-04-18 Elasticsearch is evolving fast in the past few years. There have been quite some discussions on data loss during node crashes, which can be found here and here. Most of the issues have been fixed as described here. However, since Elasticsearch carried out a major upgrade to version 5+, some serious issues still remain for low versions, e.g., the stale replica problem described here.","text":"Updated on 2018-04-18 Elasticsearch is evolving fast in the past few years. There have been quite some discussions on data loss during node crashes, which can be found here and here. Most of the issues have been fixed as described here. However, since Elasticsearch carried out a major upgrade to version 5+, some serious issues still remain for low versions, e.g., the stale replica problem described here. I already discussed in the original article about the two node issue. I recently carried out an experiment with 3 nodes which is actually the recommended minimum size for an Elasticsearch cluster. With 3 nodes, the quorum size is 2 and the minimum master nodes is 2 (discovery.zen.minimum_master_nodes). Therefore, there is always an overlap where some nodes have the latest state. Let me explain this with an example. The nodes are A, B, and C. We go through the following test steps: Create a new index with 2 replicas, i.e., 3 copies in total; Shut down A; Index 1 document on index B and C successfully; Shut down B and C; Turn on A; What about the state for the index? The replica on A will not be allocated as the primary shard since there is only one alive node less than the minimum master nodes 2. Now, we turn on B. As B has the latest state, B propagate the latest state to A. Most of open sourced distributed system rely on a mature consensus approach such as Raft or Zookeeper. However, Elasticsearch decided to invent its own. This actually leads to most of those serious issues. This really drive me crazy :( So far as I know, the most close setting to make elasticsearch strong consistent in a 3 node cluster consists of: A minimun master nodes = 2 write consistency = quorum write/read/search preference = primary check if index succeeds on &gt;= 2 nodes always refresh before search (assume search is an infrequent operation) ======== Replication is a key feature for Elasticsearch from two aspects: (1) When some machines fail, the alive ones can still serve requests; (2) Replication boosts the read speed since read can retrieve data from multiple nodes simultaneously. Elasticsearch follows a primary-secondary fashion for replication. When a write request (e.g., create, index, update, delete) arrives, it is first forward to the primary replica. The primary replica finishes the request, and then, concurrently forward the requests to all alive secondary replicas. There are a few details about this process. First, there is a concept of write consistency level in Elasticsearch, with available options one, quorum, and all. This concept is a bit different from what we normally find for other systems such as Kafka. It barely forces the primary replica to check if there are enough alive replicas available receiving a write request. For instance, suppose we have a cluster of 3 nodes with replica number 2, i.e., each shard is with one primary replica and 2 secondary replicas. If we set the write consistency level to quorum, when the primary replica receives a index request, it checks if there are at least 2 replicas available (i.e., &gt;=replicas/2+1). If the check passes, the primary replica will start the action of index, after which it forward the request to all replicas. One should note that the consistency level is only for the check. This means there is a chance when a replica fails after the check, and right before the action. Second, we need to answer the question: when shall the primary replica respond to the client? It turns out that there are two modes, sync and async as discussed here. The sync mode means the primary replica only responds the client if “all” secondary replicas have finished the request. Note that the “all” here, which has nothing to do with the selected write consistency level. Under the async mode, the primary replica responds to the client right after itself finishing the request. The request is then forward to other replicas in an async way. This accelerate the response timing for the client, which however may lead to overload for the Elasticsearch cluster. Mean while, since the request propagates eventually to the replicas, there will be no read-write consistency guarantee even inside the same session if the read preference is not set to primary. In normal case, there is only one primary replica for each shard. Once the primary replica fails, a secondary replica is elected to serve as primary. In some special situations, the primary replica may lose connection to other replicas, leading to multiple primary replicas in the system, which is called the split brain problem as discussed here. The cue to this problem is by setting the discovery.zen.minimum_master_nodes to &gt;= half of nodes + 1. For example, if you have 3 nodes, the minimum_master_nodes should be set to 2. By setting the minimum_master_nodes we ensure that the service is only available if there are more than minimum_master_nodes living nodes within one master domain. In other words, there can not be two masters in the system. Finally, I want to discuss the problem of stale shard which I read recently from here. Let’s start by use a concrete example. Say if we have two nodes and each shard has two replicas (one primary and the other secondary). We first index 10 documents with the secondary shard node turned off. Then, we turn off the primary shard node, and bring up the secondary shard node. The question here is whether the secondary shard will be promoted to primary? If it is, how about the 10 documents we indexed before? According to this blog, with Elasticsearch v5+, the primary shard will not only do the index, but also inform the master about in-sync shards. In this case, the answer to our questions are no. Because the secondary shard is not in in-sync state after being brought up. I didn’t experiment it myself about this since I don’t have a Elasticsearch v5+ environment. I only tested this with Elasticsearch 2.4.5 where I found different answer. After secondary shard node was brought up, the secondary shard was indeed promoted to primary, and the 10 documents were lost if I then brought up the previous primary shard node. This is indeed a problem if such special situation happens, which however should be quite rare in practice especially if you have more than 2 nodes, and with quorum write consistency level.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"consistency","slug":"consistency","permalink":"liqul.github.io/blog/tags/consistency/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"liqul.github.io/blog/tags/elasticsearch/"}]},{"title":"Notes on HBase","slug":"hbase","date":"2018-04-17T14:47:09.000Z","updated":"2018-04-17T15:05:03.000Z","comments":true,"path":"hbase/","link":"","permalink":"liqul.github.io/blog/hbase/","excerpt":"So far as I know, HBase is the first open source “table” style storage in the big data scope. It is an implementation of the BigTable paper presented by Google. If you read the paper or the reference guide, HBase does not look like a table. The paper tells you that it is a sparse, distributed, persistent, multidimensional sorted map.","text":"So far as I know, HBase is the first open source “table” style storage in the big data scope. It is an implementation of the BigTable paper presented by Google. If you read the paper or the reference guide, HBase does not look like a table. The paper tells you that it is a sparse, distributed, persistent, multidimensional sorted map. There are so many details in this definition. If you want to find out what each of these terms means, you can go through this article. After reading it, you’d rather call HBase a map, instead of a table, because the data structure is (row key, column family:qualifier, timestamp) -&gt; value From the perspective of RDBM, each table in HBase could still be thought of as, for example, a table in MySQL. Data is organized into rows and each row is composed of columns (grouped into column families). A cell is specified by its row key and column name. So far, everything is the same with RDBM. You can insert rows, manipulate fields within a row, retrieve a row by its row key, and so on. The timestamp is actually an internal concept, which should not be used by the user or even put in the data model. In fact, timestamp is used as a version number. Therefore, to the user, there are still two dimensions (row and column) like in an ordinary RDBM table. HBase has been out there for quite a few years, so it is not hard to find a good introduction about what is going on inside. This blog has really nice diagrams. It covers comprehensively some of the most important building blocks of HBase, which I’d like to elaborate a few below. Data OrganizationEach row in a HBase table is break into cells for persistence. And each cell is in fact a key-value pair, where the key is (row key, column family:qualifier, timestamp). Data is first written to a memory cache called MemStore. Once the MemStore accumulates enough data, the data is flushed into a persistent file called HFile. Both data in the MemStore and in the HFile are sorted by their keys. Each flush may generate more than one HFiles. Each row of data is break down by defined column families. For instance, if the table is with two column families, each flush will get two separate HFiles. A row is not stored as a whole, especially if the row has been manipulated later after insertion. Therefore, each time the client read a row, HBase goes through the MemStore and maybe, a few HFiles to bring together all fields to assemble the row. This is called the read amplification problem. HBase keeps its metadata inside a special HBase table called METADATA. The metadata maintains a set of pointers where the key is a three-value tuple [table, region start key, region id] and the value is the RegionServer. If the client wants to find a specific row key, it will go through the metadata to find the right RegionServer. Then, the RegionServer go through its managed regions to find the required data. A HFile is with multiple layers of index, bloom filters, and time range information to skip as much unnecessary data as possible. Data WriteWriting data into HBase has three steps. Firstly, the data is written to a WAL on HDFS. Secondly, the data is written to the MemStore. Finally, when the MemStore size reaches a threshold, the data is flushed into HDFS. In fact, once the data is recorded into the WAL, the write is already successful, though the data is invisible at that moment. If the RegionServer crashes before writing to the MemStore, the data can still be loaded into MemStore once the server recovers. HBase does not support cross row transactions. However, it does support atomic intra-row operations, and we know that a row is actually a set of key-value pairs. Therefore, I think the set of key-value pairs are grouped into a transaction. HBase provides strong consistency for read and write, which means every read gets exactly the same result. The result depends solely on the timestamp the request is received by HBase. In the HBase architecture, strong consistency is achieved based on HDFS. In other words, the HBase provides a consistent global view to the WAL for all RegionServers. Compaction and SplitHDFS is designed for batch process, and therefore, huge number of small files can cause a unacceptable memory footprint to the NameNode. There are two kinds of compaction, the minor compaction and the major compaction. A minor compaction collects a set of HFiles (belonging to the same column family) and merge them into one. A major compaction merge all HFiles belonging to the same column family into one. Since a major compaction consumes quite a lot of resources, it is recommended to be carried out carefully. From this point of view, HBase is not really suitable for use cases where data is manipulated frequently. A region is a consecutive range of row keys. Once a region reaches a certain size (e.g., 1G), the region is split into two. This is to keep the size of a region from being too big. Big region is bad since region is the unit of parallal access in HBase. For instance, a mapreduce program treat each region a split. The region is split in an automatic way in default. But the user could also predefine the split boundaries when creating the table. If the regions are split automatically, one need to be careful about the row key design. For instance, if the row key is mono-increasing numbers, the new arriving data will always be inserted to the newly split region, creating write hotspot. In generally, the row keys should be random enough to prevent both read hotspot and write hotspot. Finally, I’d like to give my two cents on HBase: Better documentation: The current document is not well written. The text contains a lot of references to Jira issues and external articles which is really disrupting. As discussed above, HBase is really about tables, not some complicated maps. I understand the differences, but as a beginner, I prefer concepts closer to my existing mindset. SQL interface: Providing a SQL-like interface is much more friendly than raw Java apis. Such an interface helps users to play with HBase much more easily. Predefined partition: Predined partition feels more controllable than the automatic split. Though there is a way of defining the split boundaries which is still quite indirect. In practice, people usually know how to partition their data to achieve the best performance. This is the well adopted use case for RDBMs. Local filesystem: HDFS is designed for batch access which HBase can be used for random access, even with frequent manipulations. Local filesystems might be a better choice. I also read about Kudu and MapR-DB. From the architecture point of view, they actually share a lot of design patterns. Only, Kudu and MapR-DB are built on the shoulder of HBase, so they can avoid pitfalls. Anyway, HBase is still a very good designed software which provide a very good study case. Thanks to the community and the contributors.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"HBase","slug":"HBase","permalink":"liqul.github.io/blog/tags/HBase/"}]},{"title":"投资的心法","slug":"invest","date":"2018-04-12T13:45:21.000Z","updated":"2019-03-06T14:56:16.649Z","comments":true,"path":"invest/","link":"","permalink":"liqul.github.io/blog/invest/","excerpt":"为什么要投资？自从货币与黄金脱钩以后，整体的发展趋势总是超发的。政治家还经常会以各种借口来增发货币，比如“量化宽松”这个听起来不知所云的名词，本质就是货币超发。你辛苦工作后换来的货币随着时间流逝，其内在价值在逐渐变少。怎么能尽量减少由于一些不确定性造成的经济损失呢？一般人除了投资似乎没有别的办法。","text":"为什么要投资？自从货币与黄金脱钩以后，整体的发展趋势总是超发的。政治家还经常会以各种借口来增发货币，比如“量化宽松”这个听起来不知所云的名词，本质就是货币超发。你辛苦工作后换来的货币随着时间流逝，其内在价值在逐渐变少。怎么能尽量减少由于一些不确定性造成的经济损失呢？一般人除了投资似乎没有别的办法。 巴菲特的估值核心：护城河 + 安全边际基本概念有两个， 护城河：企业有足够强的竞争优势甚至是垄断，并且企业在不断加大这种优势，同时企业本身的业务是具有长久的生命力的 安全边际：只有企业的股价与其内在价值相比足够便宜（如5折）的情况下的购买才是相对安全的 护城河用一句话来概括： “如果你不能拥有一支股票10年，那么就不要拥有它10分钟”。护城河实际上比起安全边际更加重要（除非现在的价格已经严重背离其内在价值），因为一家伟大的公司即使现在落魄，将来还是很有希望回归的。 ETF的优势和劣势ETF的优势在于其天然具备良好的“护城河”，因为你永远不用担心一个ETF会破产，或者ETF对应的企业不是行业里优秀的（因为通常ETF都会有一定的优胜劣汰机制）。其劣势是由于ETF足够“宽”，所以也意味着你获得的收益不如某些个股表现突出。但对于一般投资者来说，要准确找出这些个股的难度太大了，而历史上真正长期（10年以上）做到这一点的专业投资人凤毛麟角，所以投资ETF的实际收益要远远好于投资个股。另一方面，ETF的价格相对个股比较稳定，从而也更难买到足够便宜的，除非市场出现重大的波动。 永远不要动用生活费只有在不动用生活费的前提下才能使自己的损失更少。股指的短期波动是难以预料的，如果短期的下跌并不影响你的生活，那么为什么要着急把股票卖掉呢？而如果并不影响你的生活，你又为什么那么在意股指在短期内的波动呢？如果你坚信你所购买股票的公司是经过你认真分析的（前面提到的护城河），不会在短期内倒闭（如果是ETF那根本不会发生），那么请把你的钱留在股市里，因为“健全”的股市迟早有一天会回到这样的高度。你担心股市永远不会回到这样的高度？如果是那样的话，必然已经有一些更加令人担心的事情出现了（比如战争），那这些钱是在股市还是银行真的有区别吗？","categories":[{"name":"Finance","slug":"Finance","permalink":"liqul.github.io/blog/categories/Finance/"}],"tags":[{"name":"investment","slug":"investment","permalink":"liqul.github.io/blog/tags/investment/"}]},{"title":"Eventual Consistency vs. Strong Consistency","slug":"consistency_model","date":"2018-03-16T07:05:39.000Z","updated":"2018-04-12T13:33:38.000Z","comments":true,"path":"consistency_model/","link":"","permalink":"liqul.github.io/blog/consistency_model/","excerpt":"Here is a very good explanation about eventual consistency and strong consistency. I’d like to borrow the two figures on that page below: Fig. 1 figure for eventual consistency","text":"Here is a very good explanation about eventual consistency and strong consistency. I’d like to borrow the two figures on that page below: Fig. 1 figure for eventual consistency In this example above, Node A is the master, which replicate X to its followers Node B and C. Suppose the time when X is successfully writen to Node A is t_1, and the time when X is replicated to Node B is t_2. Any time between t_1 and t_2, if a client reads from Node A, it gets the latest value of X. But if the client reads from Node B, it gets an old version of X. In other words, the result of a read depends on which Node the client reads from, and therefore, the storage service presents an inconsistent global view for the client. In contrast, if the storage service provides a strong consistency semantic, the client should always read the same result. This figure below illustrates an example of strong consistency. Fig. 2 figure for strong consistency The single difference between Fig. 1 and Fig. 2 is that before X has been successfully replicated to Node B and C, a read request of X to Node B and C should be blocked. How about reading from Node A before all replications done? It should be blocked as well, and therefore, there is a missing ‘lock’ symbol in Fig. 2. The full picture should has the following steps: A client issues a write request of X to Node A; Node A locks X globally to prevent any read or write to X; Node A store X locally, and then replicate X to Node B and C; Node B and C store X locally and send Node A a response; After receiving from Node B and C, Node A release the lock of X and respond to the client; These steps are only used to understand the basic idea of strong consistency, which is not necessary a best practice. If you want to know more details, research some real systems such as Spanner or Kudu. While sounds more understandable for developers, strong consistency trades Availability for Consistency. In the instance shown in Fig. 2, a client may need to wait for a while before it reads the value of X. If the networking fails apart (for example, Node C is partitioned from Node A and B), any write requests to Node A will fail if each value is forced to have 3 replications. In addition, if the global lock service fails, the storage service will also be unavailable. In general, a storage service with strong consistency has much higher requirements to the infrastructure in order to function well, and therefore, is more difficult to scale compared to one with eventual consistency. AWS S3’s consistency model.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"distributed","slug":"distributed","permalink":"liqul.github.io/blog/tags/distributed/"},{"name":"consistency","slug":"consistency","permalink":"liqul.github.io/blog/tags/consistency/"}]},{"title":"Notes on MR memory issues","slug":"experience_with_mr_memory_parameters","date":"2018-02-05T03:29:09.000Z","updated":"2018-04-12T13:37:19.000Z","comments":true,"path":"experience_with_mr_memory_parameters/","link":"","permalink":"liqul.github.io/blog/experience_with_mr_memory_parameters/","excerpt":"Updated on 2018-02-05 I recently encountered several OOMs from mapper tasks reading parquet files. The yarn container is killed due to running out of physical memory. Since I already set the JVM memory to 0.8 of the container size, I’m pretty sure that this is due to off-heap memory allocation issues. I found the two jira issues here and here, pointing me to the snappy codec used by parquet for decompression. There aren’t so much I can do except allocating more memory beside the JVM.","text":"Updated on 2018-02-05 I recently encountered several OOMs from mapper tasks reading parquet files. The yarn container is killed due to running out of physical memory. Since I already set the JVM memory to 0.8 of the container size, I’m pretty sure that this is due to off-heap memory allocation issues. I found the two jira issues here and here, pointing me to the snappy codec used by parquet for decompression. There aren’t so much I can do except allocating more memory beside the JVM. === I recently experienced two OOM problems running a mapreduce application. The MR application reads from a group of parquet files, shuffles the input rows, and writes into parquet files, too. The first OOM is thrown by the mapper with error logs look like following 123452017-06-22 09:59:10.978 STDIO [ERROR] [WORKER] [129] Container [pid=14638,containerID=container_e26_1495868456939_0784_01_000066] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 1.5 GB of 2.1 GB virtual memory used. Killing container.Dump of the process-tree for container_e26_1495868456939_0784_01_000066 : |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE |- 14638 14632 14638 14638 (bash) 0 0 17096704 774 /bin/bash -c /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242 1&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stdout 2&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stderr |- 14655 14638 14638 14638 (java) 4654 290 1616650240 272880 /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242 After some investigation, I realized this is due to a misconfiguration of the mapper container memory limit (mapreduce.map.memory.mb) and the mapper JVM memory limit (mapreduce.map.java.opts). Basically, the latter should be smaller than the former, because the mapper container consumes some memory itself. After setting mapreduce.map.java.opts = mapreduce.map.memory.mb * 0.8, the OOM problem is gone. I note that this also applies for the reducer, which has two corresponding parameters (mapreduce.reduce.java.opts and mapreduce.reduce.memory.mb). This article explains nicely. The second OOM issue is much harder to address, which comes with the shuffle phase. I saw error logs like following 12345678910111213141516172017-06-21 20:22:42.294 STDIO [ERROR] [WORKER] [100] Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56) at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:514) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193) This is not an old problem which could be found in here and here. Most of the solutions suggest tuning the three parameters: mapreduce.reduce.shuffle.input.buffer.percent (default 0.7): how much memory shuffle can use to store data pulled from mappers for in-memory sort. mapreduce.reduce.shuffle.memory.limit.percent (default 0.25): how much memory each shuffle thread uses for pulling data from mappers into memory. mapreduce.reduce.shuffle.parallelcopies (default 10): the number of shuffle thread can run in parallel Some solutions claims that we should have 1mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent * mapreduce.reduce.shuffle.parallelcopies &lt; 1 which is actually not correct. MergeManager allocates memory to shuffle threads which is used for copying mapper output into memory. Each time a shuffle thread applies for a copy action, the MergeManager determines if the application is granted by checking (1) if the appliedMemory size is more than the max memory each shuffle thread can have. This is controlled by mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent. Suppose the reducer JVM has 3.5G heap size, each shuffle can apply no more than 3500*0.7*0.25=612M with default settings. (2) if the usedMemory is more than memoryLimit. The used memory accounts for memory used by shuffles and in-memory merge. The memory limit is calculated by 3.5*0.7 = 2.45G with 3.5G JVM heap size. Now, if the usedMemory is 2.44G and appliedMemory is 612M, the real memory used by shuffle could be more than 3G !!! This is not a bug, since there is a detailed comments in MergeManagerImpl.reserve. The comments explain why the actually used memory could be one shuffle larger than the limit. From the other side, this could cause OOM. Due to this issue, there’s no 100% safe way to fix the OOM by tuning the parameters. We can only mitigate this problem by reducing mapreduce.reduce.shuffle.input.buffer.percent and/or mapreduce.reduce.shuffle.memory.limit.percent. One should carefully calculate these parameters according to the real workload. Especially, the memory each shuffle can use limit the max size of output from each mapper. For example, if the mapper produces a 300M intermediate file, the shuffle should be able to allocate memory more than 300M. Otherwise, all sort will be done on disk. One more thing is about the parquet format. It is a highly compressed format, and therefore the decompressed mapper output is much larger than the input split size. I think this is why OOM happens more frequently for parquet files than other file formats.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"Mapreduce","slug":"Mapreduce","permalink":"liqul.github.io/blog/tags/Mapreduce/"},{"name":"OOM","slug":"OOM","permalink":"liqul.github.io/blog/tags/OOM/"}]},{"title":"Understanding the SSD","slug":"ssd","date":"2017-12-07T12:47:09.000Z","updated":"2018-04-12T13:37:53.000Z","comments":true,"path":"ssd/","link":"","permalink":"liqul.github.io/blog/ssd/","excerpt":"Reading the chapter 13.5 “Arranging data on disk” in the book “DATABASE SYSTEM: IMPLEMENTATION” makes me think of a question: How data should be arranged on a SSD (Solid-State Drive)? This is indeed an old question, so after doing some research with Google, I find some very good explanations. An Overview of Pages, Blocks and FTLs in a Solid-State Drive (SSD) How Do SSDs Work?","text":"Reading the chapter 13.5 “Arranging data on disk” in the book “DATABASE SYSTEM: IMPLEMENTATION” makes me think of a question: How data should be arranged on a SSD (Solid-State Drive)? This is indeed an old question, so after doing some research with Google, I find some very good explanations. An Overview of Pages, Blocks and FTLs in a Solid-State Drive (SSD) How Do SSDs Work? The two articles above describes how SSD works differently from a HDD. Some key points to take away are: The minimum read/write unit for a SSD is a page. A block is made up of a set of pages. A dirty page (with data) can not be overwritten before being erased. The minimum erase unit for a SSD is a block. Each block has a finite program/erase cycles (P/E cycles). Within a SSD, data can only be erased by block. Garbage collection need to run to reclaim logically deleted pages (e.g., due to update). Therefore, data in blocks with deleted pages are packed and rewrite to another empty block. A piece of data might be rewritten over and over again, which is called the write amplification problem. This also leads to the fact that data is moving constantly which is quite different from data stored within a HDD. Tables and indexes vs. HDD and SSD This article above discussed about the strategy of storing table data and indexes on HDD vs. SSD. The results are clearly shown by those charts. Also, the discussion in the comments is worthwhile for reading. Coding for SSDs Finally, I found a very interesting serial of blogs “Coding for SSDs”. The author built a key-value store optimized for SSDs. There are quite a lot of insights in these blogs. In conclusion, SSDs outperform HDDs from almost every aspects today, except the price per bit. However, I envision that in the near future, the price could be made low enough to replace most HDDs. SSDs are almost drop-in replacement for HDDs. However, to get the best performance from SSDs, developers do need to take care about the data access characteristics of SSDs.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"ssd","slug":"ssd","permalink":"liqul.github.io/blog/tags/ssd/"}]},{"title":"读《洪业：清朝开国史》有感","slug":"ming_lessons","date":"2017-10-28T12:47:09.000Z","updated":"2018-11-01T10:32:55.000Z","comments":true,"path":"ming_lessons/","link":"","permalink":"liqul.github.io/blog/ming_lessons/","excerpt":"读《洪业：清朝开国史》关于崇祯的一些感受。","text":"读《洪业：清朝开国史》关于崇祯的一些感受。 魏忠贤问题 上策：保持互相制衡，两方敲打，改革弊政 中策：无所作为 下策：杀魏忠贤导致文臣势力过大 皇太极的问题 上策：联络岱善，内部瓦解后金统治阶级 中策：同意与皇太极议和，攘外必先安内，剿灭李自成 下策：同时面对两股敌人 用人的问题 上策：黑猫白猫，不在意细节，以能力取人 中策：坚持用人时间长一点，不随意更替 下策：动辄得咎，反复无情，换人如流水 自杀的问题 上策：尽早自杀，传位于太子 中策：南下或北上。南下学宋高宗，虽末世无法与南宋比肩，但仍有一战的实力；北上主动联系后金，一同剿灭李自成，有崇祯在后金不那么容易南侵，何况还有吴三桂 下策：自杀身死，连太子也没有放过 总结：第一次感受到了一个人对历史产生了如此大的作用。","categories":[{"name":"Book Reading","slug":"Book-Reading","permalink":"liqul.github.io/blog/categories/Book-Reading/"}],"tags":[{"name":"洪业","slug":"洪业","permalink":"liqul.github.io/blog/tags/洪业/"}]},{"title":"Quorum in Amazon Aurora","slug":"quorum_in_amazon_aurora","date":"2017-10-27T08:12:09.000Z","updated":"2019-03-10T03:57:52.774Z","comments":true,"path":"quorum_in_amazon_aurora/","link":"","permalink":"liqul.github.io/blog/quorum_in_amazon_aurora/","excerpt":"I recently read a serial of posts about the quorum mechanism in Amazon Aurora, which is a distributed relational database. These posts are: post1: quorums and correlated failure. post2: quorum reads and mutating state. post3: reducing costs using quorum sets. post4: quorum membership.","text":"I recently read a serial of posts about the quorum mechanism in Amazon Aurora, which is a distributed relational database. These posts are: post1: quorums and correlated failure. post2: quorum reads and mutating state. post3: reducing costs using quorum sets. post4: quorum membership. Besides, there is actually a Sigmod’17 paper about Amazon Aurora which could be found here. I only briefly went through that paper which spends most of words talking about the basic architecture. I like this serial of posts which is a very good tutorial if you want to learn practical usage of quorum. By definition, a quorum model is Formally, a quorum system that employs V copies must obey two rules. First, the read set, Vr, and the write set, Vw, must overlap on at least one copy. Second, you need to ensure that the quorum used for a write overlaps with prior write quorums, which is easily done by ensuring that Vw &gt; V/2. At the heart of this model is that each read/write to the cluster of nodes overlaps at least one node with each other. While it is cool to enjoy the replication benefit with the quorum model, there comes cost for both read and write. For read, a client may need to consult multiple nodes (i.e., the read set) in order to ensure reading the latest state. For write, the multiple copies need to be materialized in order to maintain the quorum model. The author introduced the basic ideas of solving these two problems in post2 and post3. Especially, for the read penalty, the master maintains a cache of the status of all successful replicas, including their latency estimations. Therefore, a client need only to find information from the master in order to read the latest information. Membership management is discussed in post4 where they use the approach of overlapping quorums to solve the node failure problem. One nice feature is that this approach is robust given new failures happening right during the handling process. Finally, I’d like to end up with the following sentence from the posts: State is often considered a dirty word in distributed systems—it is hard to manage and coordinate consistent state as you scale nodes and encounters faults. Of course, the entire purpose of database systems is to manage state, providing atomicity, consistency, isolation, and durability (ACID).","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"quorum","slug":"quorum","permalink":"liqul.github.io/blog/tags/quorum/"},{"name":"aurora","slug":"aurora","permalink":"liqul.github.io/blog/tags/aurora/"}]},{"title":"Reading the New Apache HBase MOB Compaction Policy","slug":"new_apache_hbase_mob_compaction_policy","date":"2017-08-29T02:19:09.000Z","updated":"2018-04-12T13:40:03.000Z","comments":true,"path":"new_apache_hbase_mob_compaction_policy/","link":"","permalink":"liqul.github.io/blog/new_apache_hbase_mob_compaction_policy/","excerpt":"In case you want to understand more on MOB (Moderate Object Storage), you may refer to this issue. Basically, hbase was first introduced with capability of storing mainly small objects (&lt;100k). Moderate objects stand for files from 100k to 10m. Recently, there is a blog introducing the new compaction policy for MOB files. The problem with the initial approach is multiple compaction. For instance, the goal is to compact the objects created in one calendar day into one big file. The compaction process starts after the first hour. The objects created in the first hour are compacted into a temporal file. Then, the objects created in the second hour, and the temporal file created for the first hour are compacted into a new temporal file…","text":"In case you want to understand more on MOB (Moderate Object Storage), you may refer to this issue. Basically, hbase was first introduced with capability of storing mainly small objects (&lt;100k). Moderate objects stand for files from 100k to 10m. Recently, there is a blog introducing the new compaction policy for MOB files. The problem with the initial approach is multiple compaction. For instance, the goal is to compact the objects created in one calendar day into one big file. The compaction process starts after the first hour. The objects created in the first hour are compacted into a temporal file. Then, the objects created in the second hour, and the temporal file created for the first hour are compacted into a new temporal file… In this way, finally, all objects created in one day is compacted into one file. However, the objects in the first hour is compacted quite a few of times, wasting IO. The new method is based on partition. For instance, we may compact the objects in each hour of day, which is the first stage. Then, the temporal files in each hour are compacted into the final file, which is the second stage. This saves a lot of IO in comparison with the initial approach. Actually, this improvement is quite straightforward. What I found really insightful is about the compaction partitioned by the created time. Note that the creation time of each object is never changed during its life time. Therefore, suppose a set of objects is compacted into a big file which say contains objects between 2017-08-23 ~ 2017-08-24. After a while, some objects in that set may be deleted (with tombstone in hbase), or replaced with newer versioned metadata. However can we remove these objects physically? The answer is easy. We search for all objects created between 2017-08-23 ~ 2017-08-24, which should result in a subset of the original set of objects. We then extract the remain objects into a new big file, and delete the old big file. There are two other essential points to achieve the clear process described above: (1) the metadata should be 1:1 mapping with the objects. In other words, there should be no more than 1 metadata pointing to the same object. (2) the creation time and the pointer to file should be always updated atomically.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"hbase","slug":"hbase","permalink":"liqul.github.io/blog/tags/hbase/"},{"name":"compaction","slug":"compaction","permalink":"liqul.github.io/blog/tags/compaction/"}]},{"title":"Understanding Chain Replication","slug":"notes-chain-replication","date":"2017-07-28T10:05:39.000Z","updated":"2019-03-10T03:56:51.575Z","comments":true,"path":"notes-chain-replication/","link":"","permalink":"liqul.github.io/blog/notes-chain-replication/","excerpt":"I learned the idea of chain replication from hibari, Hibari is a production-ready, distributed, ordered key-value, big data store. Hibari uses chain replication for strong consistency, high-availability, and durability. Hibari has excellent performance especially for read and large value operations.","text":"I learned the idea of chain replication from hibari, Hibari is a production-ready, distributed, ordered key-value, big data store. Hibari uses chain replication for strong consistency, high-availability, and durability. Hibari has excellent performance especially for read and large value operations. The term “strong consistency” indeed caught my attention as I already know a few key-value storage services with only eventually consistency, e.g., openstack swift. I read its doc to find out the key tech sitting in the core is called “chain replication”. I did some investigation about this concept which actually back to very early days in 2004 in a OSDI paper. The idea is actually very easy to understand. The service maintains a set of chains. Each chain is a sequence of servers, where one server is called the head, and one is called the tail; all servers in between are middle servers. The figure in the very beginning shows such an example with two middle servers. Each write request is directed to the head server, and the update is pipelined from the head server to the tail server though the chain. Read requests are directed to only tail servers. What a client can read from the chain is definitely replicated across all servers belonging to the chain, and therefore, strong consistency is guaranteed. Though the idea sounds straightforward, there are few practical issues. First of all, the traffic load at tail servers is higher than other servers, since they handle both write and read traffics. A load balancing aware chain organization algorithm is needed to balance the load across all servers. For instance, one server may be middle server of one chain and meanwhile tail server of another chain (see Fig. 3 in the Hibari paper). Another problem is failure handling. There should be a way of detecting failed servers, which turns out to be non-trivial in such distributed world. There are also plenty of issues about recovering from failures, replication, and migration. In conclusion, this “simple” idea comes with a bunch of tough issues. There are only few open source projects based on chain replication, such as Hibari and CorfuDB. One fundamental reason may be the cost paid for strong consistency is too high. One killer application for object storage is handling highly massive objects such as user data in social network companies. However, the chain can never cross data centers in order for low latency. The idea of using chained servers is not really new. HDFS also use a pipeline to optimize data transfer latency while achieving strong consistency. Therefore, if the number of files is not a issue, storing them directly on HDFS might be a reasonable choice, given the advantage of naive integration with other Hadoop components.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"replication","slug":"replication","permalink":"liqul.github.io/blog/tags/replication/"}]},{"title":"Spark学习笔记","slug":"notes-learning-spark","date":"2017-07-07T11:19:09.000Z","updated":"2019-03-10T03:57:01.139Z","comments":true,"path":"notes-learning-spark/","link":"","permalink":"liqul.github.io/blog/notes-learning-spark/","excerpt":"Spark与Scala在学习Spark之前务必对Scala有所理解，否则面对完全陌生的语法是很痛苦的。 Scala的一种入门方式是： 学习Scala 函数式程序设计原理。这是Scala作者自己开的课程。没什么比语言作者更加能理解这门语言的了，是切入Scala编程的最好入门方式。课程习题参考了《计算机程序的构造和解释》一书，非常经典。 阅读《Scala in depth》一书，对一些Scala的重点概念有更加详细的讨论。 根据特定的topic，Google各种网络资料。","text":"Spark与Scala在学习Spark之前务必对Scala有所理解，否则面对完全陌生的语法是很痛苦的。 Scala的一种入门方式是： 学习Scala 函数式程序设计原理。这是Scala作者自己开的课程。没什么比语言作者更加能理解这门语言的了，是切入Scala编程的最好入门方式。课程习题参考了《计算机程序的构造和解释》一书，非常经典。 阅读《Scala in depth》一书，对一些Scala的重点概念有更加详细的讨论。 根据特定的topic，Google各种网络资料。 RDD (Resilient Distributed Datasets)RDD的含义RDD是spark中用于记录数据的数据结构。根据具体的RDD类型，数据有不同的组织形式。一个RDD包含多个partition，partition是并行的基本单位。RDD可能存在内存中，也可能存在硬盘里，或者两者皆有。一个RDD可以由数据源创建，也可能由其它RDD计算得到，所有参与计算RDD的RDD称为父RDD。若对mapreduce有所了解，可以把partition看作mapper的一个split。 RDD中的窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）若一个RDD中每一条记录仅仅依赖父RDD中唯一一条记录，则其为窄依赖，否则为宽依赖。比如在map中，每一条子RDD中的记录就对应着唯一父RDD中的对应记录。而groupByKey这样的操作中，子RDD中的一条记录，我们并不知道它究竟来自哪个父RDD中的哪个partition。 利用mapreduce的概念来理解，一组连续的窄依赖操作可以用一个mapper来实现，而宽依赖操作则只能依赖reducer。正因如此，一组连续窄依赖中产生的“中间结果”（实际并不需要产生这些中间结果）是没有存在的意义的，只要知道输入、操作就能直接计算输出了。举个具体的例子： 1val resRDD = srcRDD.map(_ + 1).map(_ + 2).filter( _ % 2 == 0) 中的transformation链可以看作mapreduce下的一个mapper，一条记录从左到右执行不依赖其它记录。若把上面例子改为： 1val resRDD = srcRDD.map(_ + 1).distinct().filter( _ % 2 == 0) 其中加入了distinct意味着一条记录从左到右无法利用一个mapper就完成，必须截断加入一个reducer。这里需要理解mapreduce中的一个mapper并不是等价于spark中的一个map操作，而是对应所有窄操作的组合，例如filter、flatMap、union等等。 补充材料：why spark’s mapPartitions transformation is faster than map。其中的一句话讲的非常清楚——you probably know that “narrow” transformations/tasks happen independently on each of the partitions. 即窄操作在单机即可完成，不需要依赖保存在其它主机上的partition。 RDD中persist和checkpoint的逻辑persist的目的是为了加快后续对该RDD的操作；checkpoint的目的是为了减少长执行链失败带来的开销。由于目的不同，如果persist的RDD丢失了，可以重新计算一遍（这就是普通cache的基本逻辑）。反过来，如果checkpoint丢失了，则无法重新计算，因为该checkpoint之前的内容都遗忘了。cache只是persist的一个子操作，其storage level为memory_only。 persist和checkpoint都是异步操作，执行persist或checkpoint命令仅仅给对应的RDD加上一个mark，后续交给block manager完成具体的物化操作（？？？）。persist有多种storage level，包括memory, off heap memory, disk等等。在spark中，block manager负责所有的数据存储管理，包括persist、checkpoint、或shuffle产生的中间数据等。 值得一提的是关于off heap memory的概念说明。简而言之，off heap memory就是不受JVM管控的一块内存空间，由于不受管控所以不存在GC的开销；另一方面由于并非JVM native环境，所以并不能识别其中存储的Java对象这样的结构，需要序列化和反序列化来支持。off heap memory的典型应用场景则是缓存一些较大的静态数据。 重要的方法computedef compute(split: Partition, context: TaskContext): Iterator[T]根据给定的partition计算一个interator，可以遍历该partition下的所有记录。有意思的是partition的名字为split，与mapreduce下mapper的处理单位名字一样。 RDD中的基础transformationmapdef map[U: ClassTag](f: T =&gt; U): RDD[U]返回的RDD为MapPartitionsRDD类型，其compute方法会对其父RDD中的记录执行f映射。 mapPartitionsdef mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]与map的区别在于映射f的作用对象是整个partition，而不是一条partition中的记录。在一些初始化代价较高的场景下，mapPartition比map更加合理和高效。 补充材料：why spark’s mapPartitions transformation is faster than map。 flatMapdef flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]与map类似，仅仅将对iterator的map操作换成flatMap操作。这里f映射的输出类型为TraversableOnce，表示只要能完成单次遍历即可，可以是Traversable或Iterable。 filterdef filter(f: T =&gt; Boolean): RDD[T]与map类似，仅仅将对iterator的map操作换作filter操作。 distinctdef distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]首先这个方法存在一个implicit的参数ord，类型为scala.math.Ordering。Ordering中实现了各种基础类型（Int, Long, Double, String等）的比较方法，这意味着如果T是一种基础类型则无须实现自己的比较方法，只需要import scala.math.Ordering即可。 与前几种transformation最大的不同在于distinct依赖reduce，即它是一种宽依赖操作。其具体实现代码如下： 1map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1) 可见其首先将一条记录映射为一个pair，然后执行reduceByKey的操作。这里reduceByKey方法并非RDD所有，之所以可以调用是因为object RDD里定义了从RDD转换为PairRDDFunctions的implicit方法。这种针对特定情况下的RDD增加操作的抽象方式可以学习。reduceByKey中给出了合并两个value的方式，即把相同的key的alue合并为一个（在此为null），然后根据给定的numPartitions数量进行hash partition。最终结果通过map仅保留key即可。 与mapreduce一致，这里的合并会发生在本地和reducer处，类似mapreduce中的combiner。在调用reduceByKey后的调用逻辑为： 12reduceByKey((x, y) =&gt; x, numPartitions)combineByKeyWithClassTag(x=&gt;x, (x,y)=&gt;x, (x,y)=&gt;x, new HashPartitioner(numPartitions)) 在combineByKeyWithClassTag中会根据传入的三个映射分别创建createCombiner、mergeValue和mergeCombiner。其中，createCombiner用于产生合并的初始值；mergeValue用于合并两条记录；mergeCombiner用于将mergeValue得到的结果再次合并。上述三者组成一个Aggregator对象。 coalescedef coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null) : RDD[T]连接的作用是重新整理原有的RDD。有两种情况：（1）若shuffle\\=\\=false，表示一种虚拟的RDD分区变化，此时numPartitions应该比原来的少，否则无意义。注意此时是不会发生真实的IO的；（2）若shuffle\\=\\=true，表示要做一次真实的shuffle，即会带有真实的数据IO。对于第二种情况，在coalesce方法内部会做一次随机的mapping操作，把每个元素与结果RDD中的partition做一次mapping。在第二种情况下，numPartitions可以比父RDD的分区数量更多。 虽然前一种情况只是虚拟的分区变化，但究竟把哪些父partition分入同一个子partition是可以考虑locality因素的，CoalescedRDD的balanceSlack参数用来控制locality在分配父partition时起的权重。 看代码中12345// include a shuffle step so that our upstream tasks are still distributed new CoalescedRDD( new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition), new HashPartitioner(numPartitions)), numPartitions).values 这段话比较难懂，而实际上是做了几件事：首先，在ShuffledRDD中根据随机生成的key将父RDD各partiton中的数据分散到子RDD的各partiton中；然后，隐式转换为PairRDDFunctions的values方法转换成普通的RDD。 sampledef sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = withScope对当前RDD的每个partition进行一次sample。withReplacement用于控制是否可出现重复sample，fraction控制sample的比例，seed即随机种子。 randomSplitdef randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]给定一组weights，例如Array(2.0,3.0,5.0)，将父RDD按这样的比例划分，得到一个子RDD数组。示例：12345678910111213scala&gt; val rdd = sc.makeRDD(1 to 10,10)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27scala&gt; rdd.collectres1: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) scala&gt; val randomSplittedRDD = rdd.randomSplit(Array(2.0, 3.0, 5.0))randomSplittedRDD: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[12] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[13] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[14] at randomSplit at &lt;console&gt;:29)scala&gt; randomSplittedRDD.foreach(x =&gt; println(x.collect.mkString(&quot; &quot;)))9 102 4 81 3 5 6 7 其内部实现实际上是利用了BernoulliCellSampler完成的，每次把父RDD的某个partition做一次sample得到一个子partition，通过一个MapPartitionsRDD实现从父RDD到子RDD的映射。但由于产生的是一组子RDD，因此每多一个子RDD就需要把父RDD做一次sample。由于每次调用时random seed是在内部保持不变的，所以即使多次sample，也不会导致某个元素被分到不同的子RDD里去。这一点是开始一直想不通的，因为我一直以为只需要sample一遍就能完成整个过程。 takeSampledef takeSample(withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T]返回指定数量的sample。 union（同++）def union(other: RDD[T]): RDD[T]获取两个RDD的并集，若一个元素出现多次，并不会通过union操作去重，因此union本身属于窄依赖。根据partitioner的情况，分两种情况处理：（1）如果两个RDD的partitioner都定义了且相同，那两RDD的partition数量一样，得到的并集RDD也有相同数量的partition。在考虑locality时，会按照多数原则处理，即如果大多数属于某个并集partition的父partition都倾向某个locality选择，那么就以此多数为准；（2）如果不满足（1）的情况，则并集RDD的partition数量为两父RDD的数量之和，即简单的合并关系。 keyBydef keyBy[K](f: T =&gt; K): RDD[(K, T)]根据映射f抽取原RDD中每条记录的key，使结果RDD中每条记录为一个kv二元组。 sortBydef sortBy[K](f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]对RDD排序，key由映射f抽取。这个方法的实现比较有趣，如下123this.keyBy[K](f) //生成一个基于kv二元组的RDD .sortByKey(ascending, numPartitions) //sortByKey是OrderedRDDFunctions中的方法，由隐式转换rddToOrderedRDDFunctions支持 .values //排好序的RDD再退化由原来的元素组成，也是隐式转换支持 实现过程经过两次隐式转换，非常有scala的特色，这种隐式转换往往发生在特殊的RDD之上。排序的具体过程参考Shuffle一节。 intersectiondef intersection(other: RDD[T]): RDD[T]计算两个父RDD的交集，得到子RDD，交集元素无重复。实现如下：123this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null))) //map成kv二元组后，隐式转换PairRDDFunctions调用其cogroup方法得到(k, (v1, v2))的结构 .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125; //把两边都不是空的情况筛选出来 .keys //退化为普通的RDD 其中cogroup依赖shuffle，所以是宽依赖操作。intersection操作还有一些重载，但基本实现是相同的。 glomdef glom(): RDD[Array[T]]将原来的RDD变成新的RDD，其原有的每个partition变成一个数组。例如：1234scala&gt; val a = sc.parallelize(1 to 9, 3)scala&gt; a.glom.collectres66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9)) 这篇文章把glom的作用讲的非常清楚。其中的例1和例2都是在处理一个数组要比挨个处理每个元素好很多的时候。当然，这消耗的内存要更大（TODO: 具体使用情况如何？是否会导致OOM？），是一个折衷。 cartesiandef cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]生成当前RDD与另一个RDD的笛卡尔积，即列举所有a in this和b in other而组成的(a,b)的集合。生成的新RDD的partition数量等于原两个RDD各自的partition数量的乘积。 groupBydef groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])]将当前RDD中的元素按f映射的key做group操作，结果RDD可根据传入的partitioner来进行分区。源代码中有如下注释：123456* Note: This operation may be very expensive. If you are grouping in order to perform an* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.** Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any* key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]]. 其中指出当前实现中一个key的所有value会需要保存在内存中，从而可能导致OOM，这可能是combine的过程中必须将所有value保存在内存中有关（推测）。另外，聚合或reduce可以解决大部分问题，而不需要groupBy，依此推测这个操作仅用于一些value较少又不得不获取这个中间结果的场景。 这篇文章很好的讲述了groupBy引入的内存问题的原因。 pipedef pipe(command: String): RDD[String]pipe类似于mapreduce中的streaming，即能通过stdin来把数据发往外部进程，在通过stdout把结果读回来。这篇文章讲的非常清楚。但是这似乎只是map的过程，并不能包括reduce。 其内部实现实际上就是把参数中包含的command启动一个进程，然后通过stdin/out来完成上述算子操作过程。 zipdef zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]将当前RDD与other组合成一个新的包含二元组的RDD，要求两个RDD包含相同数量的partition，且每对partition包含相同数量的元素。 zipPartitionsdef zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V]与zip的关系类似map与mapPartitions的关系，但又不完全一样。zip要求对应的partition里包含的元素数量也完全一样，但这里f映射并不需要两个partiton里元素数量相同。但显然可以利用zipPartitions来实现zip的功能，且与zip比较起来应该有更好的效率。 subtractdef subtract(other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]得到在当前RDD中且不在other中的元素组成的RDD，由于需要按元素做key，属于宽依赖。 DataFrame.repartion vs. DataFrameWriter.partitionBydef repartition(numPartitions: Int, partitionExprs: Column*): DataFramedef partitionBy(colNames: String*): DataFrameWriter这里的讨论非常清楚。repartition的参数是numPartitions和partitionExprs，partitionExprs将指定的列做hash后对numPartitions求模，得到对应的partition的index。这样得到的最终分区数量是numPartitions，但实际上如果numPartitons大于分组数量，可能有一些partition是空的；反之，如果numPartitions小于分组数量，有一些partiton里包含多个分组。partitionBy是把每个partition按照指定的列拆分为一到多个文件。 一个应用实力：如果希望输出的文件里，每个文件有且仅有一个分组，那么就可以dataframe.repartiton(n, columns).write.partitionBy(columns).csv(xxx)。其中n可以控制并发的数量，跟实际的数据分布有关。 zipWithUniqueIddef zipWithUniqueId(): RDD[(T, Long)]为了解决zipWithIndex带来的性能问题，这里放松了条件，只要求id是唯一的。zipWithUniqueId只是个算子，第k个partition的元素对应的id分别为k, k+n, k+2n, …，这里的n是partition的数量。 RDD中的actionsforeachdef foreach(f: T =&gt; Unit): Unit将映射f应用到每个元素上。 foreachPartitiondef foreachPartition(f: Iterator[T] =&gt; Unit): Unit将映射f应用到每个partition上。 collectdef collect(): Array[T]将RDD中所有元素作为一个数组返回。注意不要将collect作用于一个过大的RDD，否则会抛出内存异常，可先利用take和takeSample只取一个子集。 reducedef reduce(f: (T, T) =&gt; T): T执行映射f对应的reduce操作。其操作基本步骤是：（1）每个partition执行f映射对应的reduce过程；（2）在driver的host机器上执行基于f映射的reduce过程，输入来自各个partition的输出。步骤（2）的复杂度与partition的数量呈线性增加。 treeReducedef treeReduce(f: (T, T) =&gt; T, depth: Int = 2): T为了改进reduce里步骤（2）的瓶颈问题，对各partition的输出先逐层聚合，最后再到driver处生成最终结果，类似一棵树的聚合过程。在文章里有详细的描述。reduce和treeReduce的关系类似aggregate和treeAggregate的关系。 folddef fold(zeroValue: T)(op: (T, T) =&gt; T): T将映射op应用到每对元素上面。在实现过程中，spark不限定元素之间的执行顺序，实际上是先在partition内部做，然后再在partition之间，所以不能保证一个预先设定好的顺序来执行。因此，fold算子适用于那种不需要考虑左右操作元素的顺序，例如max。 aggregatedef aggregateU: ClassTag\\(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U与fold的不同在于aggregate可以返回一个新的类型U，而不是原来的类型Ｔ。从定义的角度，fold是aggregate的一种特例。例如： 123456scala&gt; val a = sc.parallelize(1 to 9, 3)scala&gt; a.fold(0)&#123; _ + _ &#125;res0: Int = 45scala&gt; a.aggregate(0) ( _ + _, _ + _ )res1: Int = 45 treeAggregatedef treeAggregateU: ClassTag(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U, depth: Int = 2): Uaggregate与treeAggregate和reduce与treeReduce的关系类似。 countdef count(): Long计算整个RDD中元素的个数。 countApproxcountApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] 在给定timeout期限的情况下，返回RDD中元素个数的估计。其中confidence是认为评估结果符合高斯分布的假设条件下估算的置信度，而不是结果的可信度。其核心代码如下： 1234567891011121314151617override def currentResult(): BoundedDouble = &#123; if (outputsMerged == totalOutputs) &#123; new BoundedDouble(sum, 1.0, sum, sum) &#125; else if (outputsMerged == 0) &#123; new BoundedDouble(0, 0.0, Double.NegativeInfinity, Double.PositiveInfinity) &#125; else &#123; val p = outputsMerged.toDouble / totalOutputs val mean = (sum + 1 - p) / p val variance = (sum + 1) * (1 - p) / (p * p) val stdev = math.sqrt(variance) val confFactor = new NormalDistribution(). inverseCumulativeProbability(1 - (1 - confidence) / 2) val low = mean - confFactor * stdev val high = mean + confFactor * stdev new BoundedDouble(mean, confidence, low, high) &#125; &#125; 其中totalOutputs是partition的个数。上面代码的逻辑是：如果已经计算了所有partition，则返回的结果是100%准确的；如果一个partition都未完成，那么结果完全不可信；否则，按比例计算mean，variance跟已返回比例有关，越多则variance越小，其low/high都是根据confidence和mean算出来的。 countByValuedef countByValue()(implicit ord: Ordering[T] = null): Map[T, Long]实际上就是一个map + reduce的过程，而所得结果因为需要转化为Map，需要把所得内容完全载入driver的内存，所以只适合不同的value的数量比较小的情况。 countByValueApproxdef countByValueApprox(timeout: Long, confidence: Double = 0.95)(implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]]与前面提到的countApprox实现类似。 zipWithIndexdef zipWithIndex(): RDD[(T, Long)]获得一个新的RDD，其中每个元素都是一个二元组，其中value是元素所在RDD中的全局index。该操作不保证重复时index的顺序不变。这个操作表面上是一个算子，但实际上会触发一个spark job，因为在执行之前需要知道每个partition的起始index，而这只能通过count每个partition来得到。 takedef take(num: Int): Array[T]take的作用是从一个RDD中获取给定数量num个数的元素，得到一个数组。实现的基本思路是，首先尝试读一个partition，然后根据得到的元素数量与num的比较决定是否需要再探索其它的partition，以及探索的partition数量。这个探索数量的策略似乎比较heuristic，大体上是每次探索的partition数量小于等于已探索的4倍，而具体的值跟已探索到的元素数量与num的关系来定。从实现上看，take返回的所有元素都保存在一个数组内，所以如果num数量过大会引起内存问题。 takeOrdereddef takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]takeOrdered除了获取num个元素外，还要求这些元素按照ord给出的排序方式排序。其实现的核心代码如下： 1234567891011121314val mapRDDs = mapPartitions &#123; items =&gt; // Priority keeps the largest elements, so let&apos;s reverse the ordering. val queue = new BoundedPriorityQueue[T](num)(ord.reverse) queue ++= util.collection.Utils.takeOrdered(items, num)(ord) Iterator.single(queue) &#125; if (mapRDDs.partitions.length == 0) &#123; Array.empty &#125; else &#123; mapRDDs.reduce &#123; (queue1, queue2) =&gt; queue1 ++= queue2 queue1 &#125;.toArray.sorted(ord) &#125; 首先，对每个partition需要得到一个BoundedPriorityQueue，其大小固定为num。若partition内元素少于num个，则queue不满。随后，在一个reduce中，把每个partition得到的queue拼接为一个queue。BoundedPriorityQueue的拼接会按照每个元素插入队列。根据这个实现，每次takeOrdered或top操作都需要对所有partition排序，然后在结果里拼出一个大小为num的队列，代价是比较大的。 常见的RDD派生类Spark Architecture http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/ ShuffleShuffle的目的是把key相同的记录发送到相同的parition以供后续处理。Mapreduce中同样存在shuffle阶段。回顾mapreduce中shuffle的过程：（1）mapper将数据分为多个partition，然后parition内按照key排序（实际分两步完成），这些partition一部分写入磁盘，一部分缓存在内存里；（2）mapper输出的partition分发到对应的reducer；（3）reducer对已经排好续的记录再次进行合并排序；（4）key相同的记录被group为一个iterable交给reduce方法处理。 补充材料：《Hadoop: The Definitive Guide》英文版，197页 Shuffle的两种方法Spark中shuffle“目前”有两种实现，分别是基于hash和sort。 基于hash的方式在spark 1.2.0以前是默认的方式。其实现思路非常简单，对于任意输入RDD中的partition，根据hash结果产生N个文件。N表示“reducer”的数量。由于没有排序，每条记录经过hash后直接写入文件，因此速度较快。对于后续处理不需要排序的情况，基于hash的shuffle性能较好。其缺陷是产生的文件数量较大。 基于sort的方式达到的效果与mapreduce里的shuffle一样，但实现上有较大的差异。首先，从“mapper”写出的数据是不做本地排序的，只有在“reducer”从远端获取数据时才会触发排序过程。这里需要了解spark中的AppendOnlyMap的数据结构。简单来说，在数据量足够小的情况下，“mapper”输出的数据会保存在内存一个AppendOnlyMap中。如果数据较多，则会将AppendOnlyMap变换为一个priority queue，按key排序后保存到外部文件中。这样一来，一次map操作的所有数据会保存在一个内存里的AppendOnlyMap加若干外部的文件。当“reducer”请求数据的时候，这些数据分片会被组织成一个最小堆，每次读取一个key最小的记录，从而实现了排序的功能。“Reducer“拿到各个数据分片后，采用TimSort来对所有数据排序，而不是mapreduce中的合并排序。 补充材料：Spark Architecture: Shufflespark的外排:AppendOnlyMap与ExternalAppendOnlyMap Block ManagerBlock Manager在spark中作为一层存储抽象层存在。RDD的iterator方法里有读取缓存的partition的入口getOrCompute，其中block的id定义为： 12345val key = RDDBlockId(rdd.id, partition.index)case class RDDBlockId(rddId: Int, splitIndex: Int) extends BlockId &#123; override def name: String = &quot;rdd_&quot; + rddId + &quot;_&quot; + splitIndex&#125; 从实现上看每个RDD的partition都有一个唯一的key，用于blockmanager存储的键值。一个partition应该与一个block一一对应的。Block的存储完全由block manager来管理。 关于block size不能超过2g限制的issue tracker 不错的参考资料Spark源码分析之-Storage模块Spark缓存机制分析Apache Spark源码走读之6 – 存储子系统分析 DAGDAGSchedulerAdaptive execution in Spark DataFrame可以反复学习的bloghttp://dataknocker.github.io","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"spark","slug":"spark","permalink":"liqul.github.io/blog/tags/spark/"}]},{"title":"Notes on Two-phase Commit","slug":"two-phase-commit","date":"2017-06-09T06:32:00.000Z","updated":"2018-03-06T10:52:32.000Z","comments":true,"path":"two-phase-commit/","link":"","permalink":"liqul.github.io/blog/two-phase-commit/","excerpt":"","text":"I recently came across a good description of two-phase commit from actordb’s document. I decide to borrow it as a note. The following is copied from actordb’s document: 3.2.3 Multi-actor transactionsMulti-actor transactions need to be ACID compliant. They are executed by a transaction manager. The manager is itself an actor. It has name and a transaction number that is incremented for every transaction. Sequence of events from the transaction manager point of view: Start transaction by writing the number and state (uncommitted) to transaction table of transaction manager actor. Go through all actors in the transaction and execute their belonging SQL to check if it can execute, but do not commit it. If actor successfully executes SQL it will lock itself (queue all reads and writes). All actors returned success. Change state in transaction table for transaction to committed. Inform all actors that they should commit. Sequence of events from an actors point of view: Actor receives SQL with a transaction ID, transaction number and which node transaction manager belongs to. Store the actual SQL statement with transaction info to a transaction table (not execute it). Once it is stored, the SQL will be executed but not committed. If there was no error, return success. Actor waits for confirm or abort from transaction manager. It will also periodically check back with the transaction manager in case the node where it was running from went down and confirmation message is lost. Once it has a confirmation or abort message it executes it and unlocks itself. Problem scenarios: Node where transaction manager lives goes down before committing transaction: Actors will be checking back to see what state a transaction is in. If transaction manager actor resumes on another node and sees an uncommitted transaction, it will mark it as aborted. Actors will in turn abort the transaction as well. Node where transaction manager lives goes down after committing transaction to local state, but before informing actors that transaction was confirmed. Actors checking back will detect a confirmed transaction and commit it. Node where one or more actors live goes down after confirming that they can execute transaction. The actual SQL statements are stored in their databases. The next time actors start up, they will notice that transaction. Check back with the transaction manager and either commit or abort it.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"transaction","slug":"transaction","permalink":"liqul.github.io/blog/tags/transaction/"}]},{"title":"摄影笔记","slug":"notes-on-photography","date":"2017-05-04T01:58:00.000Z","updated":"2018-03-06T10:52:25.000Z","comments":true,"path":"notes-on-photography/","link":"","permalink":"liqul.github.io/blog/notes-on-photography/","excerpt":"","text":"焦段选择的一些感想：广角（&lt;35mm) 场面干净：由于广角会摄入较广的场景，所以必须保证其中不要有不希望被包括的主体 中心突出：没有中心的广角构图是非常失败的，这比其它焦段更加要求中心突出 线条整齐对称：没有细密整齐的线条，广角会非常乏味，这些线条可以是建筑、地面的纹路、天际线等等 身临其境：广角照片给人的印象是身历其境，所以角度一般不能太平庸，要么居高临下，要么自底向上 多元素：元素可以多一点但最好是能够相互呼应的 中焦(35mm~70mm) 现实感：由于其呈现的效果更加接近人眼所以能给人一种“旁观”的感觉，更加适合拍摄纪实的题材，其带来的震撼感要高于其它焦段 距离变化：在这个焦段范围中，一点点变化都能对拍摄距离产生较大影响 长焦(&gt;70mm) 微距：把较远处的主体拍到眼前是长焦的主要作用之一 压缩场景：由于长焦会把多个主体间的距离弱化，很像中国画的感觉，体现的是一种平面的美感 少元素：元素尽量少一点，画面简单一点，弱水三千只取一瓢 虚化加成：由于长焦带来的虚化加成，在稍微大一点的光圈下能达到所谓“空气切割”的感觉 场景 vs. 焦段 苏州园林：原本是为了人眼优化的布景，更加适合中焦和长焦 城市建筑：广角更能呈现出震撼的感觉，加上建筑的线条在广角中更具有表现力；一些广场上的建筑由于没有遮挡，在没人的时候也可以用长一点的焦段 人像：跟场景有关，在场景杂乱的地方就老实用中长焦大光圈虚化；在户外视场景而定广角可以突出人与宏达场景的相映，中焦更接近生活，长焦可以捕捉一些在无干扰情况下的活动，总而言之还是跟背景有关系 一些原则 色彩尽量少一点，不要给人一种杂乱的感觉 一定要有主体，不然没有着眼点 场景中的元素除非必要尽量不要包括进来","categories":[{"name":"Photography","slug":"Photography","permalink":"liqul.github.io/blog/categories/Photography/"}],"tags":[{"name":"photography","slug":"photography","permalink":"liqul.github.io/blog/tags/photography/"}]},{"title":"Setup SBT Development Environment","slug":"setup-sbt-development-environment","date":"2017-04-10T02:52:39.000Z","updated":"2018-03-06T10:52:05.000Z","comments":true,"path":"setup-sbt-development-environment/","link":"","permalink":"liqul.github.io/blog/setup-sbt-development-environment/","excerpt":"","text":"Setup JDK following Oracle guidance. Setup SBT No matter which platform you are on. I recommend downloading the zip archive directly. Put the following into ~/.sbt/repositories: 12345678[repositories]#localpublic: http://maven.aliyun.com/nexus/content/groups/public/typesafe:http://dl.bintray.com/typesafe/ivy-releases/ , [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnlyivy-sbt-plugin:http://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext]sonatype-oss-releasessonatype-oss-snapshots Run sbt and sbt console. If you see all downloads from aliyun, you’ve setup it successfully. Test creating a new SBT project in intellij to see if everything ok.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"sbt","slug":"sbt","permalink":"liqul.github.io/blog/tags/sbt/"},{"name":"scala","slug":"scala","permalink":"liqul.github.io/blog/tags/scala/"}]},{"title":"Notes on Multi-versioned Storage","slug":"non-blocking-read","date":"2017-03-31T06:25:39.000Z","updated":"2018-03-06T10:51:28.000Z","comments":true,"path":"non-blocking-read/","link":"","permalink":"liqul.github.io/blog/non-blocking-read/","excerpt":"","text":"I recently read the Spanner paper. I realized that I cannot understand the idea of TrueTime and Non-blocking read well. Therefore, I did some research by googling the concept of non-blocking read, and came across this mysql document. After reading it, I realized that my understanding of multi-versioned storage is incorrect. So I decide to put some notes here. The key points of multi-versioned storage are: version -&gt; the creation wall clock time of the object (or a vector time) timestamp -&gt; the query time of the object the association between the metadata and content should never be changed Each object is associated with a set of metadata. The metadata contains a timestamp field indicating the version of this object. For a concrete example, let’s define a storage model as follows: Each metadata contains three fields Field Description name name of the object. objects with the same name are deem to be the same object ctime creation time deleted 1 means a tombstone extra some extra metadata for this object Let’s use typical operations to clarify the usage of this data model. PutPut is adding a new versioned object into the storage. In the figure above, the first “obj1” is inserted at 2017-04-01 11:30:12. Inserting another object with the same name is simply adding a new entry to the table pointing to the new content. When an object is requested from client, only the one with the latest timestamp is returned. Therefore, from 2017-04-01 11:30:12 to 2017-04-02 11:31:25, the first obj1 is visible. After 2017-04-02 11:31:25, the client see only the second obj1. With this data model, there is no need to block writes during reading this object, since each operation is based solely on its timestamp. DeleteDelete is simply by putting a tombstone for a certain object. Like the third obj1 in the example. No content is presented for this entry. The objects with a tombstone as the latest entry will be filtered out if requested from clients. UpdateIn this context, update is different from putting new contents into the object, but altering the object’s metadata (e.g., the extra field in the table). Updating the fields beyond the unique key is less a problem. If we need to update even the name of the object, we need to perform two steps: (1) delete the original object; (2) put a new object with the new metadata. One need to keep these two steps in an atomic transaction. SearchSearch is usually based on metadata to find a set of objects. The difference with multi-versioned storage is that we only return an object with the latest timestamp. This could be achieved with a select clause like 1SELECT t.* FROM (SELECT * FROM table WHERE xxx AND ctime &lt; NOW() ORDER BY ctime DESC) t GROUP BY t.name We then filter out tombstones reside in the result set. GetGet is first searching for the latest object before the operation time. Then, the pointer of content is handed over to the client. The client should read the content as soon as possible. Otherwise, the content may be deleted due to recycling. CompactCompact refers to merge a set of contents into a big one. This is useful for example for HDFS to relief the name node’s burden of storing a large number of small files. The implementation of compaction is a bit tricky. One first merge the target set of contents into a big file. Then, insert a new entry for each related object within an atomic transaction. For clarification, let’s suppose we need to compact obj2 and obj3 in our example. We first create a new content with both contents from the original obj2 and obj3. Then, we add new entries for obj2 and obj3, respectively. The tricky part is that the ctime for these two entries are just 1 second larger than the original two. This may sounds weired at first. But think about the situation that a new entry for obj2 is put after we started but not finished compaction. In this case, the new content could be covered by the compaction if the ctime is larger than the new entry. We add 1 to the ctime. So, the new entry will either be later, or conflict with the compaction, leading to a failure. Upon such failure, the client simply retry to submit the new content. This should not be a real problem since compaction is a rare operation. One may wonder why not simply updating the pointers in the original entries for obj2 and obj3. This actually breaks the third key points we mentioned at the beginning. It is important not changing the association. For example, if we want to get an object during compaction, reading may fail since the old contents may be deleted. Also, stale contents may be produced. More importantly, we may need very complicated transaction controls. RecycleRecycle is used to delete all deleted objects. The deleted objects could be find by searching for tombstones in the table. If a tombstone is detected, all entries before that can be deleted physically, including the tombstone itself. Delete a single content is straightforward. However, if a content is merged into a big file, we can carry out a similar process like compaction to delete the content physically. Old-versioned entries can also be recycled. Both recycling for deleted and old-versioned entries follows a certain policy. We can delete an entry once it has been out dated for a few days. This duration shouldn’t be too soon. Otherwise, we may recycle content being or to be read. One can see that, with this multi-versioned storage model, all operations are much simpler without dependency to a locking system. We even do not rely on transaction, if compaction is not necessary. Let’s return to the three key parts at the beginning of this note. With a timestamp field, we already get the sense of version. As discussed above, the operation timestamp is critical as we need it to determine which object should be put into the result set. If data is stored across multiple machines, we need to synchronize their clock precisely. TrueTime is a API expose the uncertainty of time among different machines, and thus is critical for such large scale storage implementation. Finally, the association should not be broken, otherwise, we need complicated mechanisms to fix the issues it incurs.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"storage","slug":"storage","permalink":"liqul.github.io/blog/tags/storage/"},{"name":"MVCC","slug":"MVCC","permalink":"liqul.github.io/blog/tags/MVCC/"},{"name":"non-blocking","slug":"non-blocking","permalink":"liqul.github.io/blog/tags/non-blocking/"}]},{"title":"Notes on Implementing A Lock System","slug":"implementing-a-lock-system","date":"2017-03-31T06:25:39.000Z","updated":"2018-03-06T10:51:07.000Z","comments":true,"path":"implementing-a-lock-system/","link":"","permalink":"liqul.github.io/blog/implementing-a-lock-system/","excerpt":"","text":"Recently, I got the job of building a locking utility. I started by learning from existing locking services, i.e., zookeeper and mysql, since they already have been widely accepted. Zookeeper has this page introducing how to build locks. I also find this page details how lock implemented for mysql innodb table rows. After reading these materials, as well as some others. I found a locking utility could be simple, and also could be quite complicated. So I decided to make a note here to summarize my understandings. A lock is essentially a flag claiming the right of a deterministic resource. In mysql, the locking of a range is mapped to a set of data pages, which is thus deterministic as well. The simplest form of lock is a single write lock of one resource, e.g., a file. One could implement this based on zookeeper by creating a node with a certain name. For example, if I want to lock the file “/home/liqul/file1”, I’d put a node with the same name of the file into zookeeper. If the creation returns with success, I got the lock for the file. In contrast, if I failed in creating the node, it means the lock of the file is already held by someone else. If I cannot obtain the lock, I may backoff for a while and retry afterwards. The one holding the lock should release it explicitly. Otherwise, the file can never be used by others any more. I note that this is much simpler compared with the lock recipe in this page. The zookeeper lock recipe brings two useful features: blocking and session-aware. Blocking means that if I failed to obtain the lock now, I will wait until the lock being released by all peers queuing before me. Session-aware means that if I crash, my application for the lock also disappears. The latter is useful to avoid forgetting to release the lock as discussed in the previous paragraph. To implement these two features, one should setup a client-server architecture. Also, to achieve blocking lock, one need a queue for each resource. Sometimes, we however prefer non-blocking locks, which is not discussed in the zookeeper recipe. Read-write lock is an extension of the write-only lock. Zookeeper also has a dedicated recipe for it here. Implementing a read-write lock is non-trivial. Let’s explain this problem with a concrete example. Suppose A wants to acquire a read lock of a file F, while B wants a write lock to F too. A naive implementation may be as follows: 1234561 *A* checks if there&apos;s any write locks to *F*2 *B* checks if there&apos;s any read locks to *F*3 *A* finds no write locks to *F*4 *B* finds no read locks to *F*5 *A* put a read lock of *F*6 *B* put a write lock of *F* The “check and then lock” pattern simply doesn’t work correctly. In zookeeper, they rely on sequential nodes and watcher to work around this problem, where each peer always first inserts a node and then check if itself obtains the lock. If not, the peer put a watcher onto the current holder of the lock. Another workaround is to first obtain a global write pre-lock before any operation. With a pre-lock the above procedure becomes: 123456781 *A* acquires the pre-lock2 *A* obtains the pre-lock3 *A* checks if there&apos;s any write locks to *F*4 *B* acquires the pre-lock5 *B* failed to obtain the pre-lock6 *A* finds no write locks to *F*7 *A* put a read lock of *F*8 *A* release the pre-lock One obvious drawback of the second workaround is that even two locks have no conflict, they still need to perform one after another. To avoid this problem, the lock utility need to maintain a conflict matrix for each pair of locks being processed or pending (should be marked separately). If a pending lock is not conflicting with any lock processed, it obtains the pre-lock right away. Otherwise, it is put into a queue waiting for all conflicting locks being clear. A short version is to only consider read and write locks separately. Another extension is to achieve atomicity for a set of locks. For this purpose, one need to treat the whole set as “one” lock. And the handling of it is quite similar with what we have discussed above. For instance, if you want to implement with zookeeper, you may need to insert all the locks and then set watchers for a set of conflicting locks. Only after all conflicting locks being clear, you obtain the locks. Without zookeeper, one can also use the pre-lock solution as described above. A conflict matrix is necessary to avoid deadlock if you want to process the sets of locks in parallel. In general, zookeeper is quite ready for customizing into your own locking service. However, it does has its own drawbacks. For example, it is not clear how to implement non-blocking read-write locks. If you have metadata for your locks, and you want to search in the metadata, zookeeper may be painful. At this time, using a mysql database may be a good choice, though you need to avoid some pitfalls discussed in this article.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"lock","slug":"lock","permalink":"liqul.github.io/blog/tags/lock/"}]},{"title":"Notes on Using \"Select ... For Update\" for Uniqueness in Mysql","slug":"using-select-for-update-for-uniqueness-in-mysql","date":"2017-03-31T06:25:39.000Z","updated":"2018-03-06T10:51:55.000Z","comments":true,"path":"using-select-for-update-for-uniqueness-in-mysql/","link":"","permalink":"liqul.github.io/blog/using-select-for-update-for-uniqueness-in-mysql/","excerpt":"","text":"I encountered a deadlock recently. Similar questions have been asked on StackOverflow, e.g., this and this. But the answers didn’t really explain why this happens. The situation is quite easy to reproduce @ Mysql 5.7.17 (also tested on other versions in 5.5 or 5.6): 12345678CREATE TABLE `test` ( `id` bigint(11) NOT NULL AUTO_INCREMENT, `val` varchar(255) NOT NULL, PRIMARY KEY (`id`), KEY `search` (`val`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;insert into test set val='pre-lock'; session112start transaction;select * from test where val='pre-lock' for update; session212start transaction;select * from test where val='pre-lock' for update; session11insert into test set val='/a/b/c'; session21ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction The result of show engine innodb status: 12345678910111213141516171819202122232425262728293031323334353637LATEST DETECTED DEADLOCK------------------------2017-04-06 23:54:03 0x7000057db000*** (1) TRANSACTION:TRANSACTION 1333, ACTIVE 18 sec starting index readmysql tables in use 1, locked 1LOCK WAIT 2 lock struct(s), heap size 1136, 1 row lock(s)MySQL thread id 5, OS thread handle 123145394155520, query id 62 localhost root Sending dataselect * from test where val='pre-lock' for update*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 24 page no 4 n bits 72 index search of table `test_tnx`.`test` trx id 1333 lock_mode X waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 8; hex 7072652d6c6f636b; asc pre-lock;; 1: len 8; hex 8000000000000001; asc ;;*** (2) TRANSACTION:TRANSACTION 1332, ACTIVE 29 sec insertingmysql tables in use 1, locked 14 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 1MySQL thread id 62, OS thread handle 123145394434048, query id 63 localhost root updateinsert into test set val='/a/b/c'*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 24 page no 4 n bits 72 index search of table `test_tnx`.`test` trx id 1332 lock_mode XRecord lock, heap no 1 PHYSICAL RECORD: n_fields 1; compact format; info bits 0 0: len 8; hex 73757072656d756d; asc supremum;;Record lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 8; hex 7072652d6c6f636b; asc pre-lock;; 1: len 8; hex 8000000000000001; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 24 page no 4 n bits 72 index search of table `test_tnx`.`test` trx id 1332 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 2 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 8; hex 7072652d6c6f636b; asc pre-lock;; 1: len 8; hex 8000000000000001; asc ;;*** WE ROLL BACK TRANSACTION (1) My objective is to use select ... for update as a uniqueness check for a following sequence of insertions. I expected that Tnx 2 would wait until Tnx 1 released the lock, and then continue its own business. However, Tnx 2 is rolled back due to deadlock. The innodb status looks quite confusing. Tnx 1 is holding and waiting for the same lock. After some research, though I still cannot figure out the root cause, my perception is that the insertion in Tnx 1 acquires a gap lock which is somehow overlapping with the gap lock by the select ... for update. And therefore, this create a deadlock where Tnx 1 waits for Tnx 2 and Tnx 2 waits for Tnx 1. During my research, I found that the right use case for select ... for update is as follows: 1234start transaction;select * from [table] where [condition] for update;insert into [table] values [belongs to condition];delete from [table] where [belongs to condition]; The rows being mutated should be explicitly locked by the select ... for update. Also, the condition should be as clear as possible. For example, put only an unique key in the condition. This is to make the gap lock with a simple and clear range, in order not to cause deadlocks. Generally, using select ... for update is non-trivial since the underlying locking mechanism seems quite complicated. For my scenario, I got two workarounds: Disable gap locks by setting the isolation level to READ COMMITTED. Apply select ... for update on a row from another table, which avoid possible lock overlap.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"lock","slug":"lock","permalink":"liqul.github.io/blog/tags/lock/"},{"name":"Mysql","slug":"Mysql","permalink":"liqul.github.io/blog/tags/Mysql/"},{"name":"Deadlock","slug":"Deadlock","permalink":"liqul.github.io/blog/tags/Deadlock/"},{"name":"Uniqueness","slug":"Uniqueness","permalink":"liqul.github.io/blog/tags/Uniqueness/"}]},{"title":"Notes on The Raft Consensus Algorithm","slug":"the-raft-consensus-algorithm","date":"2017-03-31T06:25:39.000Z","updated":"2018-03-15T02:34:55.000Z","comments":true,"path":"the-raft-consensus-algorithm/","link":"","permalink":"liqul.github.io/blog/the-raft-consensus-algorithm/","excerpt":"","text":"What’s consensus? It allows a collection of machines to work as a coherent group that can survive the failures of some of its members. It means not only a group of machines reach a final decision for a request, but also the state machine is replicated across these machines, so that some failures do not affect the functioning. Raft is a consensus algorithm seeking to be correct, implementable, and understandable. The thesis is very well written. It is much more comprehensive compared to the NSDI paper. Implementing Raft based on the thesis shouldn’t be too difficult (of course, also not trivial). The author also built a website putting all kinds of helping things there. I read the paper and decide to take some notes here. There are two key parts sitting in the core of the algorithm: Leader election The election is triggered by a timeout. If a server failed to detect heartbeats from the current leader, it start a new term of election. During the term, it broadcast requests to collect votes from other servers. If equal or more than majority of servers reply with a vote, the server becomes the leader of this term. The “term” here is a monotonically increasing logic time. From the perspective of a server receiving the vote request, it decides whether to give the vote based on a few considerations. First of all, if the sender even falls behind the receiver in terms of log index, the receiver should not vote for it. Also, if the receiver can still hear the heartbeats from current leader, it should not vote too. In this case, the requester might be a disruptive server. In other cases, the receiver should vote for the sender. Log replication Once a server becomes the leader, it’s mission is simply replicate it’s log to every other follower. The replication means make the log of a follower exactly the same as the leader. For each pair of leader and follower, the leader first identify the highest index where they reach an agreement. Starting from there, the leader overwrite its log to the follower. The leader handles all requests from clients. Once it receives a new request, it first put the request into its own log. Then, it replicate the request to all followers. If equal or more than majority followers (including the leader itself) answer the replication request with success, the leader apply the request into its state machine (this is called commit). The leader put the new log index into its heartbeats, so followers know if the request has been committed, after which each follower commit the request too. More formal introduction of the core Raft could be found in Fig. 3.1 in the thesis paper. There are also a few extensions to make the algorithm practical to be used in production systems, such as the group management. I also found Fig. 10.1 a very good reference of architecture. There are quite a lot of implementations of Raft, which could be found here. I also find a project named Copycat, with code here and document here. Copycat is a full featured implementation of Raft in java. Building your own application based on Copycat shouldn’t be too difficult. They provide an example of implementing a KV store based on Copycat in their source code here, which is used as the “Get Started“ tutorial. Another very important reason, why I think Copycat a good reference, is that it emphases the abstraction of state machine, client, server, and operations. Therefore, going through it’s document enhanced my understanding of Raft. If you don’t want to build your own Raft, may be Copycat is worthwhile a try, though I haven’t any real experience beyond a toy project. The annotated thesis could be found here. A go-through case for understanding A typical request handling process is as follows: The client sends a request to the cluster; The leader handles the request by putting it to a WAL; The leader sends the request to all followers; Each follower puts the received request to its WAL, and responds to the leader; Once the leader has heard a majority number of responses from its followers, the leader commit the request by applying the WAL to its state machine; The leader inform the client that the request has been handled properly, and then, put the index of the request into its heartbeat to let all followers know the status of each request; Once the follower knows that the request has been committed by the leader, the follower commit the request too by applying it to its own state machine. There are a few key points to understand in the process above: 1.Does the client always know if its request has been handled properly? No. If the leader commits the request and then crashes, the client will not know if the request has been actually successfully handled. In some cases, the client will resend the request which may lead to duplicated data. It leaves for the client to avoid such kind of duplication. 2.How about the leader crashes before inform its followers that the request has been committed? If the leader crashes, a follower will be elected to be the next leader. The follower must have the latest state according to the mechanism of Raft. Therefore, the next leader definitely has the WAL for the request, and the request has definitely been replicated across a majority number of hosts. Therefore, it is safe to replicate its state to all followers. 3.Key feature of a consensus algorithm (or strong consistency)? Under normal situations, if there’s a state change, the key step changing the state should be always handled by a certain node. The state changing should be replicated to a majority number of followers before informing the requester a success. Each read request goes to that certain node as well. Once there’s node failures or networking partitions, the service stop working until returning to the normal situation again.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"liqul.github.io/blog/tags/Raft/"},{"name":"Consensus","slug":"Consensus","permalink":"liqul.github.io/blog/tags/Consensus/"},{"name":"Copycat","slug":"Copycat","permalink":"liqul.github.io/blog/tags/Copycat/"}]},{"title":"An Algorithm Deduplicating File Locks","slug":"an-algorithm-deduplicate-file-locks","date":"2017-03-26T13:02:44.000Z","updated":"2018-03-06T10:50:53.000Z","comments":true,"path":"an-algorithm-deduplicate-file-locks/","link":"","permalink":"liqul.github.io/blog/an-algorithm-deduplicate-file-locks/","excerpt":"","text":"In one of my recent project, I need to implement a lock service for files. It is quite similar to the lock service of existing file systems. I need to support two modes: write (exclusive) and read (shard). For instance, “R/a/b/c” means a read lock for the file with path “/a/b/c”. Likewise, “W/a/b/c” stands for a write lock. The interface is pretty straightforward. The user provide a set of locks, e.g., {R/a/b/c, R/a/b/d}, to be locked by our service. One challenge we face is to deduplicate the locks given by the user. For instance, the user may provide a set of locks {R/a/b/c, W/a/b/c}. In this example, we know the write lock is stronger than the read lock, and therefore, the set is equivalent to {W/a/b/c}. We tend to reduce the number of locks, since it is beneficial for checking less conflicts in the steps afterwards. So far, the deduplication problem sounds quite simple, since we only need to consider locks with exactly the same path. However, we also need to support a wildcard notation, because we have a large number of files under one directory. If we want to lock all files in this directory, we need to generate a huge list of locks, each for a file. With a wildcard, we lock all files under “/a/b/“ with “R/a/b/*”. The wildcard makes the deduplication problem much more complicated. We first clarify the coverage of locks by a concrete example: 1W/a/b/* &gt; W/a/b/c &gt; R/a/b/c Given a large set of locks, a very simple algorithm is that we compare each pair of locks. If one is stronger than the other, the weaker one is throw away. The result set contains only locks where no one is stronger than other locks. This algorithm’s complexity is O(n*n) which is slow if the given set is large. So, I try to find a faster algorithm. After some thoughts, I developed an algorithm with complexity O(n), which constructs a tree on the set of locks. Each tree node has the following attributes: 12345678class Node &#123; String name; //the name of the node String mode; //&quot;W&quot; or &quot;R&quot; Node parent; //the parent of the node boolean mark = false; //mark=true if there&apos;s at least one W node in its subtree Map&lt;String, Node&gt; children = new HashMap&lt;&gt;(); //the children of this node ...&#125; We now explain how to construct a lock tree with a concrete example. In the figure above, we show inserting a set of locks into an empty lock tree. Each node is represented with a string “mode:name:mark”. Step 1~3: Inserting the first three locks {R/a/b/c, R/a/b/d, R/a/e/f} is straightforward.Step 4: Then, inserting the 4th lock, i.e., “W/a/b/c”, has two effects: (1) upgrade the original “R” lock of /a/b/c to “W” mode; (2) mark the path of “/a/b/c” from 0 to 1.Step 5: The 5th lock “R/a/b/e” is the same with the first three.Step 6~7: Then, the 6th lock “R/a/b/*“ contains a wildcard. Having a “R” wildcard means replacing all unmarked nodes (“R:e:0” and “R:d:0”) with a “R:*:0” node, in its parent’s subtree (the parent is “R:b:1” in this case). Similarly, inserting “R/a/*/*“ first removes all unmarked nodes in “R:a:1”‘s subtree, i.e., “R:e:0”, “R:*:0”, and “R:f:0”, and then insert new nodes with wildcard.Step 8: Finally, inserting a “W” mode lock with wildcard means deleting all nodes in the subtree of “R:a:1” (the wildcard node’s parent) and then insert the new nodes. After constructing the lock tree, each path from root to leaf is a lock in our deduplicated result set. In practice, a hashmap may already solve most duplicated cases. We rarely encounter extreme cases as shown in the example above. The algorithm briefly described above is only for fun :). I didn’t mention paths with different lengths since they could be put into different trees, which is therefore not a real problem. I didn’t elaborate all cases in the example above, which is more complicated. A sample implementation of the algorithm could be find here. The output is as follows: 123456789101112131415161718192021222324252627R/a/b/cROOT: abc[R]R/a/b/dROOT: abc[R]ROOT: abd[R]R/a/e/fROOT: abc[R]ROOT: abd[R]ROOT: aef[R]W/a/b/cROOT: abc[W]ROOT: abd[R]ROOT: aef[R]R/a/b/eROOT: abc[W]ROOT: abd[R]ROOT: abe[R]ROOT: aef[R]R/a/b/*ROOT: abc[W]ROOT: ab*[R]ROOT: aef[R]R/a/*/*ROOT: abc[W]ROOT: a**[R]W/a/*/*ROOT: a**[W]","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"lock","slug":"lock","permalink":"liqul.github.io/blog/tags/lock/"}]},{"title":"Notes on Designing an Admission Control System","slug":"a-role-based-admission-control-system","date":"2017-03-07T07:50:03.000Z","updated":"2018-03-06T10:50:32.000Z","comments":true,"path":"a-role-based-admission-control-system/","link":"","permalink":"liqul.github.io/blog/a-role-based-admission-control-system/","excerpt":"","text":"An admission control system takes care of two parts: action and resource. As a concrete example, let’s assume you are a sales manager. A typical business is viewing the orders made by all sales man in your group. Here, viewing the orders is the “action” part, while the orders made by your group is the “resource” part. Given a user account, an admission control system tells which actions the user could take on which resources. For a website, the action is usually represented with a URI. For instance, 123&quot;/orders/view&quot; -&gt; view the orders&quot;/user/delete&quot; -&gt; delete a user&quot;/user/update&quot; -&gt; update a user&apos;s profile A typical way of managing the actions is through a layered representation of users, roles, and permissions. A user belongs to a role, and a role is composed of a set of permissions. Each permission is linked to a URI for instance. For convenience reasons, people usually add additional layers to the three-layer structure. For example, a “permission group” layer makes it easier for the manager to assign permissions when the number of permissions is too large. 1user -&gt; role -&gt; permission Representing the resource is nontrivial. Before that, you need domain knowledge to understand the structure of the resource. In our example of sales manager, the resource are orders which could be managed in a tree. The manager can see all orders within her subtree, while a sales man cannot see orders from her siblings. In this case, the resource is put into a tree as follows. 1234567manager -- sales man 1 | -- sales man 2 | -- sales man 3 -- agent 1 | -- agent 2 You may maintain multiple trees, each for a logic organization. For instance, one tree for sales and another for operators. Each order is linked to a node on the tree. When a action arrives, e.g., “/orders/view/1” where “1” is the order id, we check if the linked node of this order entry in DB. If the node is within the user’s subtree, the admission is granted, otherwise denied. Last but not least, be sure to use a white list for the super user. In other words, the super user should not go through the admission control system. Otherwise, when the admission control system is down, you can do nothing at all without permission.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"admission control","slug":"admission-control","permalink":"liqul.github.io/blog/tags/admission-control/"}]},{"title":"乌合之众","slug":"the-crowd","date":"2017-02-25T14:31:47.000Z","updated":"2018-03-06T10:50:26.000Z","comments":true,"path":"the-crowd/","link":"","permalink":"liqul.github.io/blog/the-crowd/","excerpt":"","text":"准确的说《乌合之众》是指“The Crowd——A Study of the Popular Mind”，作者是Gustave Le Bon，一位1931年离世的法国作家。我所读的版本是由广西师范大学出版社出版的，由冯克利翻译的版本。有意思的是这本书正文只有32开的183页，而其中1-29页是Robert K. Merton撰写的读后感，所以在阅读正文之前已经有非常详尽的对于书的评论了。 读后感里对《乌合之众》的评论是非常纠结的，既赞扬了作者深刻的观察力，也批评了作者受其根深蒂固的观念的影响。相信赞扬的成分还是要比批评的成分大得多的，其实一部书里哪怕揭示了一点潜藏的知识，那这部书就已经很了不起了，哪怕附带许多明显的错误。《乌合之众》就是这样，在短短的一百多页里，作者其实只想传达一个观点——“群体的思维和个人是不同的”。更准确的说，是群体的思维是混乱的、富含想象力的、可传染的、极端的、低俗简单的、扩大化的、矛盾的，跟文明社会里的个人相比，群体就好比是来自野蛮社会的个体。 《乌合之众》里有非常多关于上面我列举的各个形容词的描述，例如：“打动群体心灵的是神话中的英雄，而不是一时的真实英雄”、“群体感情的一致倾向会立刻变成一个既定事实”、“群体总是落后于博学之士和哲学家好几代人”。尤其经典的是“群体推理的特点，是把彼此不同、只在表面上相似的事物搅在一起，并且立刻把具体的事物普遍化”，比如“受雇主剥削的苦力，立刻便认为天下所有的雇主都是剥削他们的人”。 这本书两个世纪前就已经完成了，但它所描绘的现象却不断重现着。近代近两百年来中国发生的各项运动，以及参与这些运动的农民、学生、工人、商人等等，无不是《乌合之众》里描绘的角色。他们富有激情，时刻以民族、道德为口号党同伐异，采用最野蛮和暴力的方式。他们看似有无穷的力量，推动社会和历史前进，而实际上他们不过是任人摆布的玩偶，为达到某些目的而布施的棋子。 可恨的是直到今天这样的场景仍然此起彼伏，人们并没有比一两百年前的先辈更加冷静和明白，而一些野心家还试图利用群体的这种特点，一旦风吹草动人们仍然会像暴民一样无差别摧毁一切（很多时候包括这些活动的制造者）。作为个人，在群体面前显得太势单力薄了，我们无法改变什么，甚至无法保护自己，我们能够做的，仅仅是躲避群体带来的灾难——这种灾难是自然灾难无法比拟的！","categories":[{"name":"Book Reading","slug":"Book-Reading","permalink":"liqul.github.io/blog/categories/Book-Reading/"}],"tags":[]},{"title":"Running Mapreduce over Openstack Swift","slug":"running-mapreduce-over-openstack-swift","date":"2017-02-13T02:19:09.000Z","updated":"2018-03-06T10:55:14.000Z","comments":true,"path":"running-mapreduce-over-openstack-swift/","link":"","permalink":"liqul.github.io/blog/running-mapreduce-over-openstack-swift/","excerpt":"","text":"I recently got a chance of experimenting mapreduce over several opensource object storages. Openstack swift is definitely one well known object storage service. However, I found it non-trivial to set it up for evaluation. Therefore, I put some notes below. This article is only for os=Ubuntu 14.04 and swift=liberty. I noticed that the newer versions of swift is with much better documentation, which is much easier to follow. The appendix contains my early trials of installing swift from source. Read the followings http://stackoverflow.com/questions/19004503/the-relationship-between-endpoints-regions-etc-in-keystone-openstack Part 1. Fundamentals and Architecture in “Openstack Swift - Using, Administering, and Developing for Swift Object Storage” Make sure you have some sense for concepts including user, role, tenant, project, endpoint, proxy server, account server, object server, rings, and etc. To my understanding, they are: user: a real user, or a service role: role of users, corresponding to a set of permissions tenant = project: a set of users endpoint: the entry point urls of openstack services proxy server: handling user requests in swift account server: handling user account in swift, also used as domain or primary namespace object server: handling object storage in swift rings: the consistent hashing algorithm used by swift keystone: the authentication service in openstack. Key concepts on keystone can be found here Setup keynote and swiftInstall dependencies Install MariaDB (or MySQL) and memcache following page Install keystone following page1 and page2. Note that if you want to run mapreduce over swift, you can not use the TempAuth approach. Read this page for more details. Install swift following page1, page2, and page3. You can start the swift service with 1swift-init all start Setup Hadoop Setup Hadoop with version &gt;= 2.3 and configure it following page1 and page2. Make sure SwiftNativeFileSystem is in the classpath, read this page for any problem you find. Configure etc/hadoop/core-site.xml，add following contents: 1234567891011121314151617181920212223242526272829303132333435363738&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.swift.impl&lt;/name&gt; &lt;value&gt;org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.auth.url&lt;/name&gt; &lt;---SwiftTest is the name of the service &lt;value&gt;http://127.0.0.1:5000/v2.0/tokens&lt;/value&gt; &lt;---the ip:port should point to keystone. Make sure having &quot;/tokens&quot; there. &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.auth.endpoint.prefix&lt;/name&gt; &lt;value&gt;/AUTH_&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.http.port&lt;/name&gt; &lt;value&gt;8080&lt;/value&gt; &lt;--- the same with that in proxy-server.conf &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.region&lt;/name&gt; &lt;value&gt;RegionOne&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.public&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.tenant&lt;/name&gt; &lt;value&gt;admin&lt;/value&gt; &lt;--- name of the project, not the role! &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.username&lt;/name&gt; &lt;value&gt;admin&lt;/value&gt; &lt;--- user name &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.swift.service.SwiftTest.password&lt;/name&gt; &lt;value&gt;adminpassword&lt;/value&gt; &lt;--- password &lt;/property&gt;&lt;/configuration&gt; Verify all setup create a container (swift post nameofcontainer). Important! the name should be only consist of letters. No ‘_’ is allowed. upload a file (swift upload nameofcontainer nameoffile). hdfs dfs -ls swift://nameofcontainer.SwiftTest/. This should show you files you uploaded previously. hadoop distcp nameoffile swift://mycontainer.SwiftTest/nameoffile Appendix: Install swift from sourceDependenciesFrom the swift book12apt-get install git curl gcc memcached rsync sqlite3 xfsprogs \\git-core libffi-dev python-setuptools 1234apt-get install python-coverage python-dev python-nose \\python-simplejson python-xattr python-eventlet \\python-greenlet python-pastedeploy python-netifaces \\python-pip python-dnspython python-mock Install the latest liberasurecode (as per some post, may not be necessary)123456789sudo apt-get install build-essential autoconf automake libtoolcd /opt/git clone https://github.com/openstack/liberasurecode.gitcd liberasurecode./autogen.sh./configuremakemake testsudo make install Install the Swift CLI1234cd /optgit clone https://github.com/openstack/python-swiftclient.gitcd /opt/python-swiftclient; sudo pip install -r requirements.txt;python setup.py install; cd .. I run into a problem of invalid format of the requirement.txt. You may need to do some editing in that case. Install the Swift123cd /optgit clone https://github.com/openstack/swift.gitcd /opt/swift ; sudo python setup.py install; cd .. The above is from the swift book. But I realized I still need to run “sudo pip install -r requirements.txt” before the setup* Another issue I found is that it takes forever to pip install cryptography in the requirements.txt. I searched in Google where some guy said it took 20 min for building. For me, after 20 min, it still hanged there. I tried to install all dependencies manually, as follows without the cryptography. After that, it was installed successfully: 123456789Requirement already satisfied (use --upgrade to upgrade): cryptography in /usr/local/lib/python2.7/dist-packagesRequirement already satisfied (use --upgrade to upgrade): idna&gt;=2.0 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): pyasn1&gt;=0.1.8 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.4.1 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): setuptools&gt;=11.3 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): enum34 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): ipaddress in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): cffi&gt;=1.4.1 in /usr/local/lib/python2.7/dist-packages (from cryptography)Requirement already satisfied (use --upgrade to upgrade): pycparser in /usr/local/lib/python2.7/dist-packages (from cffi&gt;=1.4.1-&gt;cryptography) Copy in Swift configuration files12345678910mkdir -p /etc/swiftcd /opt/swift/etccp account-server.conf-sample /etc/swift/account-server.confcp container-server.conf-sample /etc/swift/container-server.confcp object-server.conf-sample /etc/swift/object-server.confcp proxy-server.conf-sample /etc/swift/proxy-server.confcp drive-audit.conf-sample /etc/swift/drive-audit.confcp swift.conf-sample /etc/swift/swift.confchown -R swift:swift /etc/swift In this setup, we rely on tempauth for authentication. Config the swift.confSetting the hash path prefix and suffixThese two random strings are used to determine the hash result of the partitions on the rings. They are supposed to be confidential. 12swift_hash_path_suffix = random_32_length_string1swift_hash_path_prefix = random_32_length_string2 You can use 1head -c 32 /dev/random | base64 to get one random 32 length string. The storage policyYou can custermize the storage policy as per the swift book, but it is optional. Build the ringsThe 3 parameters are part_power, replicas, min_part_hours part_power: the number of partitions in the storage cluster. The typical setting is log_2 ( 100 maximun number of disks ). For this setup, it is log_2 (100 1) which is close to 7. replicas: in this setup, there is only one drive. Therefore, only 1 replica is allowed. min_part_hours: the default is 24 hours. Tune it as per the swift book or the official document. 1234cd /etc/swiftswift-ring-builder account.builder create 7 1 1swift-ring-builder container.builder create 7 1 1swift-ring-builder object.builder create 7 1 1 Prepare the drivesYou can use a disk or chop off disk space from existing disk to provide storage for swift. Follow the step 2 of this article. Copied here: Attach a disk which would be used for storage or chop off some disk space from the existing disk.Using additional disks:Most likely this is done when there is large amount of data to be stored. XFS is the recommended filesystem and is known to work well with Swift. If the additional disk is attached as /dev/sdb then following will do the trick: 12345# fdisk /dev/sdb# mkfs.xfs /dev/sdb1# echo &quot;/dev/sdb1 /srv/node/partition1 xfs noatime,nodiratime,nobarrier,logbufs=8 0 0&quot; &gt;&gt; /etc/fstab # mkdir -p /srv/node/partition1# mount /srv/node/partition1 Chopping off disk space from the existing disk:We can chop off disk from existing disks as well. This is usually done for smaller installations or for “proof-of-concept” stage. We can use XFS like before or we can use ext4 as well. 12345# truncate --size=2G /tmp/swiftstorage# DEVICE=$(losetup --show -f /tmp/swiftstorage)# mkfs.ext4 $DEVICE# mkdir -p /srv/node/partition1# mount $DEVICE /srv/node/partition1 -t ext4 -o noatime,nodiratime,nobarrier,user_xattr In either case, you need to 1chown -R swift:swift /srv/node/partition1 Also, you need to mount automatically after system reboot. So put the mount command line into a script, e.g., /opt/swift/bin/mount_devices. Then add a file start_swift.conf under /etc/init/ with content 1234description &quot;mount swift drives&quot;start on runlevel [234]stop on runlevel [0156]exec /opt/swift/bin/mount_devices Make sure to make the script runnable 1chmod +x /opt/swift/bin/mount_devices Add the drives to the ringsThe single parameter 100 is the weight for load balancing. Note that the partition1 in the command is in the path of /srv/node/partition1. If you change the path, you need to change both places. It is not the name of the device in /dev/sda, for example. Make sure about the IPs and the PORTs. They are the address of the processes (account, container, and object). I was blocked by the port for quite a while, simply because the default ports in the conf files are different from the ones used below (not sure why…). 123swift-ring-builder account.builder add r1z1-127.0.0.1:6002/partition1 100swift-ring-builder container.builder add r1z1-127.0.0.1:6001/partition1 100swift-ring-builder object.builder add r1z1-127.0.0.1:6000/partition1 100 Rebalance the rings to create them actually. 123swift-ring-builder account.builder rebalanceswift-ring-builder container.builder rebalanceswift-ring-builder object.builder rebalance You will find the *.ring.gz files and a backup folder. Configure the logsput a file named 0-swift.conf under /etc/rsyslog.d directory, containing only one line: 1local0.* /var/log/swift/all.log And then create the directory and set the right permissions. 12345mkdir /var/log/swiftchown -R syslog.adm /var/log/swiftchmod -R g+w /var/log/swiftservice rsyslog restart Start the proxy server1swift-init proxy start If you see warning messages as discussed in this post. You can follow the solutions there, copied here: 12345curl -o libisal2_2.15.0-3~bpo14.04+1_amd64.deb http://mitaka-trusty.pkgs.mirantis.com/debian/pool/trusty-mitaka-backports/main/l/libisal/libisal2_2.15.0-3~bpo14.04+1_amd64.deb sudo dpkg -i libisal2_2.15.0-3~bpo14.04+1_amd64.debgit clone https://bitbucket.org/kmgreen2/pyeclib.git sudo python setup.py installsudo apt-get uninstall python-pyeclib Configure tempauth in proxy-server.confWe rely on tempauth for authentication in this setup. If you are using keystone, follow this post or this one. First, start the memcache: 1service memcached start Add your users to proxy-server.conf under the tempauth block. 12345678910[filter:tempauth]use = egg:swift#tempauth# You can override the default log routing for this filter here:...# &lt;account&gt; is from the user_&lt;account&gt;_&lt;user&gt; name.# Here are example entries, required for running the tests:user_admin_admin = admin .admin .reseller_adminuser_test_tester = testing .adminuser_test2_tester2 = testing2 .adminuser_test_tester3 = testing3 The syntax is 1user_$SWIFTACCOUNT_$SWIFTUSER = $KEY [group] [group] [...] [storage_url] which means you can configure the user for each storage url. Then, modify these two options: 12allow_account_management = trueaccount_autocreate = true Start the serversCreate the cache directory 12mkdir -p /var/swift/reconchown -R swift:swift /var/swift/recon Start the services after making sure the conf files are right, especially the ip and port. The ip is typically set to 0.0.0.0 and the port should be the same as adding the drives. 123swift-init account startswift-init container startswift-init object start Then, restart the proxy server 1swift-init proxy restart Verify the setupSend in the authentication (note that I’m using the admin user defined in the proxy-server.conf with group admin and password admin): 1curl -v -H &apos;X-Auth-User: admin:admin&apos; -H &apos;X-Auth-Key: admin&apos; http://localhost:8080/auth/v1.0/ The above will get you the X-Auth-Token if everything is right. For instance, 12345678910111213141516171819202122* Hostname was NOT found in DNS cache* Trying 127.0.0.1...* Connected to localhost (127.0.0.1) port 8080 (#0)&gt; GET /auth/v1.0/ HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: localhost:8080&gt; Accept: */*&gt; X-Auth-User: admin:admin&gt; X-Auth-Key: admin&gt; &lt; HTTP/1.1 200 OK&lt; X-Storage-Url: http://localhost:8080/v1/AUTH_admin&lt; X-Auth-Token-Expires: 18098&lt; X-Auth-Token: AUTH_tka1a0d192e57746839c1749f238ba5419&lt; Content-Type: text/html; charset=UTF-8&lt; X-Storage-Token: AUTH_tka1a0d192e57746839c1749f238ba5419&lt; Content-Length: 0&lt; X-Trans-Id: txb9682bc6bab448659fd50-005881a50f&lt; X-Openstack-Request-Id: txb9682bc6bab448659fd50-005881a50f&lt; Date: Fri, 20 Jan 2017 05:50:07 GMT&lt; * Connection #0 to host localhost left intact Next, use the token to access the account by 1curl -v -H &apos;X-Storage-Token: AUTH_tka1a0d192e57746839c1749f238ba5419&apos; http://127.0.0.1:8080/v1/AUTH_admin/ the admin in the url AUTH_admin should be the same as the username. You should get something like follows. 1234567891011121314151617181920212223242526* Hostname was NOT found in DNS cache* Trying 127.0.0.1...* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)&gt; GET /v1/AUTH_admin HTTP/1.1&gt; User-Agent: curl/7.35.0&gt; Host: 127.0.0.1:8080&gt; Accept: */*&gt; X-Storage-Token: AUTH_tka1a0d192e57746839c1749f238ba5419&gt; &lt; HTTP/1.1 200 OK&lt; Content-Length: 12&lt; X-Account-Object-Count: 0&lt; X-Account-Storage-Policy-Policy-0-Bytes-Used: 0&lt; X-Account-Storage-Policy-Policy-0-Container-Count: 1&lt; X-Timestamp: 1484824088.30547&lt; X-Account-Storage-Policy-Policy-0-Object-Count: 0&lt; X-Account-Bytes-Used: 0&lt; X-Account-Container-Count: 1&lt; Content-Type: text/plain; charset=utf-8&lt; Accept-Ranges: bytes&lt; X-Trans-Id: txf57c870a9ba146d9947d0-005881a5cd&lt; X-Openstack-Request-Id: txf57c870a9ba146d9947d0-005881a5cd&lt; Date: Fri, 20 Jan 2017 05:53:17 GMT&lt; mycontainer* Connection #0 to host 127.0.0.1 left intact I was blocked here for quite a while simply because the port in the proxy-server.conf is 6202, not 6002. Be careful!!! If you get something wrong, go to the log file (at /var/log/swift/all.log) and see the error message there. You can further verify the setup by creating a container and upload/download a file with 12curl -v -H &apos;X-Storage-Token: AUTH_tka1a0d192e57746839c1749f238ba5419&apos; -X PUT http://127.0.0.1:8080/v1/AUTH_admin/mycontainerswift -A http://127.0.0.1:8080/auth/v1.0/ -U admin:admin -K admin upload mycontainer some_file In the last command line, we use swift client instead of curl. There are other subcommand other than upload. There are delete, download, list, post, and stat. To avoid putting the url and user info in every command, one could put the following into .bashrc 123export ST_AUTH=http://localhost:8080/auth/v1.0export ST_USER=admin:adminexport ST_KEY=admin Start the consistency processesSwift relies on a few other processes for consistency purposes. Edit (or create) the /etc/default/rsync file and add the following line: 1RSYNC_ENABLE=true Then, edit (or create) the /etc/rsyncd.conf file, adding the following: 12345678910111213141516171819uid = swiftgid = swiftlog file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pid[account]max connections = 25path = /srv/node/read only = falselock file = /var/lock/account.lock[container]max connections = 25path = /srv/node/read only = falselock file = /var/lock/container.lock[object]max connections = 25path = /srv/node/read only = falselock file = /var/lock/object.lock Start the rsync service 1service rsync start Now, you can start the processes by 123swift-init account-replicator startswift-init container-replicator startswift-init object-replicator start One useful way of controlling these processes is by 1swift-init all start/stop/restart Setup on multiple nodesThe plan of deployment depends on the hardware. One need to read the part IV of the swift book before that. I found this article very well writen. One can learn the way of deployment for a small cluster. There are a few points one should keep in mind. Copy the swift.conf file to all nodes; Add the drives across all nodes to the rings; Copy the rings to all nodes; References Install a Stand-alone, Multi-node OpenStack Swift Cluster with VirtualBox or VMware Fusion and Vagrant (https://thornelabs.net/2014/07/14/install-a-stand-alone-multi-node-openstack-swift-cluster-with-virtualbox-or-vmware-fusion-and-vagrant.html) OpenStack 101: How to Setup OpenStack Swift (http://blog.adityapatawari.com/2014/01/openstack-101-how-to-setup-openstack_12.html) OpenStack Swift (the swift book)","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"big data","slug":"big-data","permalink":"liqul.github.io/blog/tags/big-data/"},{"name":"mapreduce","slug":"mapreduce","permalink":"liqul.github.io/blog/tags/mapreduce/"},{"name":"openstack","slug":"openstack","permalink":"liqul.github.io/blog/tags/openstack/"},{"name":"swift","slug":"swift","permalink":"liqul.github.io/blog/tags/swift/"}]},{"title":"Implementing Your Own Mapreduce Input Format","slug":"implementing-your-own-inputformat","date":"2017-02-13T02:11:53.000Z","updated":"2018-03-06T10:49:56.000Z","comments":true,"path":"implementing-your-own-inputformat/","link":"","permalink":"liqul.github.io/blog/implementing-your-own-inputformat/","excerpt":"","text":"There are two steps for each input format, namely, getSplit and recordReader. GetSplit transforms the original input data into splits, one split for each mapper. Typically, if a big file is feed into mapreduce, it will be divided into splits of block size (e.g., 64M or 128M). However, if small files are provided, mapreduce will treat each file as a split. RecordReader turn each split into key value pairs, which are used by the mapper later. In this article, I use several examples to explain how an inputformat be implemented. I’m greatly influenced by a few great references, such as this and this. The first example is from book “Hadoop: The Definitive Guide”. WholeFileInputFormat treats each individual input file as a value. We do not need to implement our own getSplit function. Once the isSplitable function returns false, no file will be divided. Each file is treated as a split for the mapper. 12345678910111213public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, BytesWritable&gt; &#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(inputSplit, taskAttemptContext); return reader; &#125;&#125; Since each file is a split, the inputSplit of the record reader is actually a FileSplit. In the example code below, each returned value is the whole content of the file. The key is null in this example, which can also be the name of the file obtained from the fileSplit. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt; &#123; private FileSplit fileSplit; private Configuration conf; private BytesWritable value = new BytesWritable(); private boolean processed = false; public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit) inputSplit; this.conf = taskAttemptContext.getConfiguration(); &#125; public boolean nextKeyValue() throws IOException, InterruptedException &#123; if(!processed) &#123; byte[] contents = new byte[(int)fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); IOUtils.readFully(in, contents, 0, contents.length); value.set(contents, 0, contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true; return true; &#125; return false; &#125; public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; public float getProgress() throws IOException, InterruptedException &#123; return processed? 1.0f : 0.0f; &#125; public void close() throws IOException &#123; //do nothing &#125;&#125; I also think the TeraInputFormat a good example. The key difference from the previous example lies in the input file format of terasort. According to the document, each input element is a 100 byte array, with the first 10 bytes as key and the left 90 bytes as value. This raises a challenge for dividing the input file into splits with exactly multiple of 100 bytes. In the code below, the getSplits simply invokes super.getSplits, ignoring the format issue. Then, in the record reader, the offset is adjusted to the next multiple of 100. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107public class TeraInputFormat extends FileInputFormat&lt;Text, Text&gt; &#123; static final String PARTITION_FILENAME = &quot;_partition.lst&quot;; private static final String NUM_PARTITIONS = &quot;mapreduce.terasort.num.partitions&quot;; private static final String SAMPLE_SIZE = &quot;mapreduce.terasort.partitions.sample&quot;; static final int KEY_LENGTH = 10; static final int VALUE_LENGTH = 90; static final int RECORD_LENGTH = 100; private static MRJobConfig lastContext = null; private static List&lt;InputSplit&gt; lastResult = null; public TeraInputFormat() &#123; &#125; public RecordReader&lt;Text, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException &#123; return new TeraInputFormat.TeraRecordReader(); &#125; public List&lt;InputSplit&gt; getSplits(JobContext job) throws IOException &#123; if(job == lastContext) &#123; return lastResult; &#125; else &#123; long t1 = System.currentTimeMillis(); lastContext = job; lastResult = super.getSplits(job); long t2 = System.currentTimeMillis(); System.out.println(&quot;Spent &quot; + (t2 - t1) + &quot;ms computing base-splits.&quot;); if(job.getConfiguration().getBoolean(TeraScheduler.USE, true)) &#123; TeraScheduler scheduler = new TeraScheduler((FileSplit[])lastResult.toArray(new FileSplit[0]), job.getConfiguration()); lastResult = scheduler.getNewFileSplits(); long t3 = System.currentTimeMillis(); System.out.println(&quot;Spent &quot; + (t3 - t2) + &quot;ms computing TeraScheduler splits.&quot;); &#125; return lastResult; &#125; &#125; static class TeraRecordReader extends RecordReader&lt;Text, Text&gt; &#123; private FSDataInputStream in; private long offset; private long length; private static final int RECORD_LENGTH = 100; private byte[] buffer = new byte[100]; private Text key; private Text value; public TeraRecordReader() throws IOException &#123; &#125; public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; Path p = ((FileSplit)split).getPath(); FileSystem fs = p.getFileSystem(context.getConfiguration()); this.in = fs.open(p); long start = ((FileSplit)split).getStart(); this.offset = (100L - start % 100L) % 100L; this.in.seek(start + this.offset); this.length = ((FileSplit)split).getLength(); &#125; public void close() throws IOException &#123; this.in.close(); &#125; public Text getCurrentKey() &#123; return this.key; &#125; public Text getCurrentValue() &#123; return this.value; &#125; public float getProgress() throws IOException &#123; return (float)this.offset / (float)this.length; &#125; public boolean nextKeyValue() throws IOException &#123; if(this.offset &gt;= this.length) &#123; return false; &#125; else &#123; long newRead; for(int read = 0; read &lt; 100; read = (int)((long)read + newRead)) &#123; newRead = (long)this.in.read(this.buffer, read, 100 - read); if(newRead == -1L) &#123; if(read == 0) &#123; return false; &#125; throw new EOFException(&quot;read past eof&quot;); &#125; &#125; if(this.key == null) &#123; this.key = new Text(); &#125; if(this.value == null) &#123; this.value = new Text(); &#125; this.key.set(this.buffer, 0, 10); this.value.set(this.buffer, 10, 90); this.offset += 100L; return true; &#125; &#125; &#125;&#125; One of the most common reason of customizing your own input format is for combining small files into fewer bigger ones. For this purpose, we still need to disable the split of files. Note that the implemented CombineWholeFileInputFormat extends CombineFileInputFormat which helps to turn the input split into a combine file split (the size of each split could be tuned) which may contains multiple small files. Each individual file could be retrieved with an index as shown in the record reader below. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CombineWholeFileInputFormat extends CombineFileInputFormat&lt;Text, BytesWritable&gt; &#123; @Override protected boolean isSplitable(JobContext context, Path filename) &#123; return false; &#125; @Override public RecordReader&lt;Text, BytesWritable&gt; createRecordReader(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException &#123; return new CombineFileRecordReader&lt;Text, BytesWritable&gt;( (CombineFileSplit)inputSplit, taskAttemptContext, CombineWholeFileRecordReader.class ); &#125;&#125;public class CombineWholeFileRecordReader extends RecordReader&lt;Text, BytesWritable&gt; &#123; private WholeFileRecordReader reader; private String fileName; public CombineWholeFileRecordReader(CombineFileSplit split, TaskAttemptContext context, Integer index) throws IOException, InterruptedException &#123; FileSplit fileSplit = new FileSplit(split.getPath(index), split.getOffset(index), split.getLength(index), split.getLocations()); fileName = fileSplit.getPath().toString(); reader = new WholeFileRecordReader(); reader.initialize(fileSplit, context); &#125; public void initialize(InputSplit inputSplit, TaskAttemptContext taskAttemptContext) throws IOException, InterruptedException &#123; &#125; public boolean nextKeyValue() throws IOException, InterruptedException &#123; return reader.nextKeyValue(); &#125; public Text getCurrentKey() throws IOException, InterruptedException &#123; return new Text(fileName); &#125; public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return reader.getCurrentValue(); &#125; public float getProgress() throws IOException, InterruptedException &#123; return reader.getProgress(); &#125; public void close() throws IOException &#123; reader.close(); &#125;&#125; SummaryThis article demonstrated a few sample implementations of input format for a mapreduce job. The basic idea is first divide the whole input data into splits and then read each split with a record reader. Usually, there are already plenty of base input formats that can be leveraged for customizing into more sophisticated ones.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"liqul.github.io/blog/tags/mapreduce/"}]},{"title":"Questions Asked When Applying AI","slug":"questions-applying-ai","date":"2017-02-11T10:08:51.000Z","updated":"2019-03-10T03:57:36.557Z","comments":true,"path":"questions-applying-ai/","link":"","permalink":"liqul.github.io/blog/questions-applying-ai/","excerpt":"","text":"Artificial Intelligence (AI) or Human Intelligence (HI)?It is not always obvious to determine using AI or HI. If you feel confident to have sufficient data to train sophisticate models, you may prefer using AI. Otherwise, using HI based rules is a more straight way tackling the problems. In practice, a hybrid solution combining both AI and HI is usually a wise choice. Classification or Regression?It all depends on the output of the problem. If the outcome is continuous, use regression, otherwise classification. In the digital world, nothing is really continuous. So, if there are too many categories, use regression is a wiser choice. Do I need preprocessing?It is rare case that no preprocessing is required. Data usually contains a lot of noise. If the raw data is feed into the learning algorithm, unpredictable outcome may be obtained. Filtering the extraneous data before doing the learning, and most important, also apply the filter before applying the algorithm in your real system. Once you encounter an extraneous event, use HI to determine the output to your end user. Complex or Less complex?In the world of learning algorithm, the complexity of the problem is important to choose to apply a “legacy” algorithms like random forest, SVM, or to apply a “cutting-edge” algorithm like CNN. Though DNN beats the smartest people in some areas, it is still quite big for some applications, e.g., activity classification on smartphones. Also, keep in mind that those super powerful algorithms need tons of data as learning input. So, before tasting the cutting-edge techniques, be sure to prepare enough “food” for them. How to avoid over-fitting?The cue comes from cross validation. Depending on how much data you have, you make take out 30% data for testing, and the left 70% for training. Or, you may split the whole data set as 10 splits. You take out 1 split for testing and the left 9 splits for training. If you have only few data, use leave one out cross validation. What’s the cost?Learning algorithms typically treat all output result neutrally, which however is rare in practice. So, be sure to consider about your cost function, and apply it to the learning algorithm. There are a few ways of applying a cost function given a learning algorithm as a blackbox. How domain-knowledge comes in?In general, domain knowledge belongs to HI. Here, I simply want to emphasis that domain knowledge may come in multiple ways. For example, the output of a classification algorithm may be smoothed with a HMM smoother powered by domain knowledge. Also, choosing the features requires deep understanding about the problem to solve. Make a stretch algorithm framework for the problem and then feed more and more domain knowledge in brain to make it better. Finally, I want to put a picture of the architecture that I designed for transportation activity detection on smartphones.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"AI","slug":"AI","permalink":"liqul.github.io/blog/tags/AI/"}]},{"title":"Toolbox","slug":"toolbox","date":"2016-12-18T03:42:47.000Z","updated":"2018-03-06T10:49:34.000Z","comments":true,"path":"toolbox/","link":"","permalink":"liqul.github.io/blog/toolbox/","excerpt":"","text":"Particle FilteringParticle filtering is essentially a heuristic continious search process. Each particle represents a probe of the whole search space. Thus, the number of particles represents the size of the search space. There are a few key steps in the search process: initialization, update weight, andd resampling. Each of these steps can be heuristically tuned following the physic world rules. A good example of applying particle filtering can be found in “Robust Indoor Localization on a Commercial Smart-Phone”, CMU TR 2011. In the paper, they adopted Importance Resampling which is a typical approach by selecting these important particles each iteration. Alternatively, another popular approach is to learn the distribution of the particles and re-generate them based on the learnt model. For example, we assume the particles follow a Normal distribution (learnt based on mean and stddev of their positions), and the observation implies another Normal distribution (pre-knowledge), we can then use a joint distribution to generate the new batch of particles. Two must read papers are Magicol and this one. Forward Error CorrectionThe purpose of FEC is to ease the correction of error packets in a packet-based network. Though there are various ways to accomplish this goal, a easy-to-follow paper is “Effective erasure codes for reliable computer communication protocols”, Sigcomm Comput. Commun. Rev. 1997. The key idea is to treat N packets as N unknowns, and generate M (M greater than or equal N) un-collinear equations such that any N equations (among M) can derive the original N packets. There is actually no inherent difference between FEC and network coding. The key idea of network coding is to exploit the broadcast nature of wireless communication to save as much bandwidth as possible. Dina Katabi has been working on this back to a few years ago, which could be found here. Periodicity DetectionThere are basically two mechanisms: short-time FFT and auto-correlation. A very easy-to-follow paper “On Periodicity Detection and Structural Periodic Similarity” SDM’05. The key idea of that paper is to fuse them for robustness. The paper is written in an interesting fashion. This paper also gives a high-level description on FFT. Periodicity detection is actually independent to the sensor. However, with different sensors, the specific approach may vary a little bit. A very good reference using RGB camera sensor is “Real-Time Periodic Motion Detection, Analysis, and Applications” CVPR 2000. The approach in the reference paper is quite intuitive and easy to follow. Though detecting periodic activities has been well studied, it is still challenging to achieve online counting. A good practice for counting is based on a state machine with two states: (1) periodicity detection: where frequency analysis (FFT) is applied to a sliding window and peak energy ratio to tell if a periodic activity is observed; (2) edge detection: the rising/falling edges are counted. There are a few important thresholds. First, the peak energy ratio which describes how “higher” the energy of the peak should be above the ambient. Second, the threshold used to detemine if there’s a rising/falling edge. These choices require definitely domain knowledge and should use real data to learn them. Local RegressionRegression is used to predict the output of a system based on existing observations. Local regression is thus used to account for locality which is a common phenomena in many physic world scenarios. Andrew Ng’s CS229 lecture notes is a good tutorial for understanding this concept. One example of applying local regression is in the Modellet paper, as well as in “Refining WI-FI Based Indoor Positioning” ISCAICT, 2010. The key technology for avoiding overfitting is leave-one-out cross-validation (LOOCV). The key idea of cross-validation is to ensure good performance on an independent test set. More details can be found in reference paper. ModulationA easy-to-follow guide to modulation could be found in here. Rolling ShutterOne widely used exposure control is called rolling shutter where each row has a different exposure starting time. Therefore, if the scene is varying frequently, the captured data may vary from row to row. One brief introduction of rolling shutter can be found in “Energy characterization and optimization of image sensing toward continuous mobile vision” Mobisys’13. The exposure time for each row is exactly the exposure time of the camera which depends on the lighting condition. The difference of the starting times between adjacent rows is called row readout time, which depends on the hardware. Then, to analyse a shining light signal in the frequency domain, the sampling interval is deemed to be the readout time, and the sampling duration is the exposure time. A special case is that if the sampling duration is the multiple of the signal period, the signal is undetectable. Note that the exposure time doesn’t affect the highest detectable frequency, however, the longer the sampling duration, the lower the SNR. Using rolling shutter to detect a shinning LED light source is feasible in theory. However, due to factors like imperfect sensor, exposure time, and background interference, the SNR is typically low. In common office condition, the recommended illumination level is 300 to 500 Lux. Therefore, it is impractical to apply this technique under such conditions. IMU Sensor FusionModern mobile devices ship with low-cost IMU sensors, i.e., accelerometer, gyroscope, and compass. Ideally, they can be used to track the orientation of the rigid body of the device. However, suffering from the drift issue, the sensor outputs cannot be directly used. A common way of dealing with such problems is fusing the readings from various sensors. This in general belongs to the field of Mechatronics and Robotics, and thus the derivation is quite complicated. Fortunately, we can leverage existing work described in the paper “An efficient orientation filter for inertial and inertial/magnetic sensor arrays” by Sebastian Madgwick in 2010. His code is distributed publicly and used by a lot of open source projects like this and this is a port of the algorithm in C. Recently, I found this place does a very good job explaining the key points on sensor fusion, and a copy in pdf is here. Random Sample ConsensusRandom Sample Consensus is usually referred to as RANSAC, which is an iterative method to estimate parameters of a model from a set of observations. The key idea is very straightforward. The algorithm starts by randomly selecting a subset of all observations to train the unknown parameters of the model. Then, it tests the remain observations and count how many of them fits the model. If the ratio is higher than a threshold, the model is considered acceptable. Otherwise, the model is rejected. The algorithm goes through a finite number of iterations to find the optimal model with the highest consensus ratio. There are two key parameters, namely thresholds of consensus and acceptable model. In order to get a good estimation, the user should have solid domain knowledge of the distribution of the data and outliers. Examples and implementation in C++ could be found at here and here. One may wounder why not using all data to train the model. The answer is that, sometimes, it is unnecessary to use all data. One example is when you want to calculate the transform matrix from one point cloud to another. Diff AlgorithmDiff is an important algorithm in various applications such as finding the difference in two code segments. There are various ways of doing this where a representive one is described in the paper “An O(ND) Difference Algorithm and Its Variations”. One salient point of this paper is the formulation of the problem using Edit Graph. A C# implementation and discussions cound be found here. A nicer description can be found here. Kernel Density Estimation (KDE)Kernel density estimation (KDE) is a technique typically used to smooth the discrete density function, or more precisely the histogram, learnt from a finite set of samples. For instance, we observed 10 samples of the RSSI of a Wi-Fi access point, ranging from -40dB to -90dB. Then, we can derive the histogram distributing the samples in to 10dB sized bins. However, due to the limited number of samples, we may result in a coarse-grained shape. KDE is used to deal with such situations. The key idea is to “leak” some probability density into neighboring bins. To achieve this goal, KDE involves a kernel, commonly uniform, triangular, bi-weight, normal, or others, and apply it across the histogram. An intuitive example could be found on the Wikipedia with normal kernel. Thus, to perform KDE, one needs to choose an appropriate kernel, as well as a bandwidth for the kernel. Generally, KDE is an empirical method to make the discrete probability density function more reasonable. However, one definitely need to understand the domain knowledge to ensure that KDE really makes some sense. Apriori AlgorithmApriori is a classic algorithm for mining association rules in a large dataset. One famous application of Apriori is to mine common merchandise itemset bought by customers. There are two key metric in Apriori, i.e., support and confidence. Support quantify the number of transactions containing the itemset of interest. For instance, there are a total of 1000 transactions where 100 of them contains both bread and butter, then the support ratio is 100/1000=0.1. Another important metric is the confidence which measures the reasoning confidence. Use the same example. If 200 transactions contains bread and only 100 of them have butter, then the confidence is 100/200. In other words, we have 50% confidence to say that if someone buy break, she will also buy butter. Before mining the dataset, one need to specify these two metrics. Typically, confidence is set to a very high value like 99% to ensure the derived rules are significant and meaningful. The ratio of support depends heavily to the domain knowledge. In the market transaction example, support needs to be high to motivate the rearrange the goods being aware of the cost. However, in some cases, we focus more on the confidence, where the support could be low. The algorithm works in a bottom up way. Apriori first finds out all one-item sets with support higher than the chosen metric. Then it elaborate all combinations of one-item sets to form two-item sets. The process keeps on until no more higher level set satisfies the support metric. Then, Aporiori applies the confidence metric to filter out invalid itemsets. Apriori is with many implementations which can be found easily.","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"tool collection","slug":"tool-collection","permalink":"liqul.github.io/blog/tags/tool-collection/"}]},{"title":"阳明先生的哲学","slug":"yangming","date":"2016-12-02T16:00:00.000Z","updated":"2018-04-10T14:59:44.000Z","comments":true,"path":"yangming/","link":"","permalink":"liqul.github.io/blog/yangming/","excerpt":"","text":"王守仁，幼名云，字伯安，浙江余姚人。初次接触阳明先生是通过《明朝那些事》，作者当年明月称其为“一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人”。阳明先生对后世的影响应首推其哲学，当时称之为“心学”，可惜《明朝那些事》里讲得更多的是他的事迹，虽然提到“知行合一”但没有解释清楚其内在含义。所以虽然读了《明朝那些事》，阳明先生的思想在我看来还是雾里看花。 阳明先生虽然在哲学界享有盛名，但其知名度并不高。这是个很奇怪的现象，其原因可能有二：其一，阳明先生似乎并不喜欢著书，最为著名的《传习录》也由其弟子收集整理，类似于《论语》。其实大部头的书籍并不符合他的性格，他崇尚简洁，其倡导的观念归纳起来寥寥数句而已，何须长篇累牍。其二，阳明先生所推崇的思想在当代看来是赤裸裸的唯心主义。在这“唯物主义”的意识形态下，这纯粹是封建社会糟粕，不禁止就不错了，怎会大张旗鼓地宣传？当然，这或许也与某些人十分崇拜他有关吧。 我对阳明先生的肤浅理解其实仅仅来源于《传习录》。个人拙见，其思想的根源可以归纳为“人心即天地”，《传习录》有： ‘心犹镜也。圣人心如明镜。常人心如昏镜。近世格物之说，如以镜照物，照上用功。不知镜尚昏在，何能照？先生之格物，如磨镜而使之明。磨上用功。明了后亦未尝废照’ 此类语句在《传习录》中还有很多。 所以，人们真正该用功的地方乃是磨练自己的内心。“知行合一”或“天泉桥话别”为阳明先生解释如何磨练人心的具体方法。其中“知行合一”被许多人认为是阳明先生的思想核心，我认为其原因是：这句话说起来朗朗上口容易引起“共鸣”，没有真正理解其意义的读者往往以为类似“行胜于言”或“言行合一”，其实差之毫厘谬以千里。 明白了阳明先生的哲学，就清楚人的成长没有捷径。人成长的过程即人心接受历练的过程，不经历诸多繁杂事务决不能“动心忍性增益其所不能”。同时，正由于人心即天地，“磨镜而使之明”就成为了唯一和终极的人生目标，看清这一点，才会明白什么成长过程中遇到的障碍不过“磨镜”的石头而已，应敞开胸怀地迎接，而不是畏首畏尾地逃避。 2016-05-01：理解了“知行合一”，就明白为什么说评价一个人不要看他说过什么，而要看他做过什么。他说出来的话不一定代表他最真实的想法，而做的事情必然是最符合他的内心的。","categories":[{"name":"Thinking","slug":"Thinking","permalink":"liqul.github.io/blog/categories/Thinking/"}],"tags":[{"name":"Learning","slug":"Learning","permalink":"liqul.github.io/blog/tags/Learning/"}]},{"title":"Install Octave on OSX EI Capitan 10.11.3","slug":"install-octave-on-osx-ei-capitan-10113","date":"2016-12-02T16:00:00.000Z","updated":"2018-04-10T14:58:23.000Z","comments":true,"path":"install-octave-on-osx-ei-capitan-10113/","link":"","permalink":"liqul.github.io/blog/install-octave-on-osx-ei-capitan-10113/","excerpt":"","text":"Installing Octave on the latest OSX can be error prone. One may download the Octave-Forge bundle from sourceforge here which however is large and also a bit out of date. Alternatively, if you use homebrew to do this, please be ware ofa few traps. Here are the steps I adopted to successfully install Octave.If you do not have Xcode or Homebrew installed yet, please refer to Googleto get them installed properly. 1. Preliminaries Install XQuartz from here. Install gcc with (this is for gfortran). 1$ brew install gcc It may take quite some time to install gcc from source. If you cannot wait, do xcode-select –install before install gcc. This will have mac install a pre-compiled version which is very fast. 2. Import the scientific computing packages with12$ brew update &amp;&amp; brew upgrade$ brew tap homebrew/science If you see any warnings, run 1$ brew doctor and follow any suggestions to fix the problem. And then re-import the packages as follows 12$ brew untap homebrew/science$ brew tap homebrew/science 3. Install Octave1$ brew install octave --without-docs The option –without-docs above is to suppress errors due to missing Tex installation. 4. Install gnuplotAs gnuplot will automatically be installed with octave, but without support for X11. So we need to reinstall it properly. 12$ brew uninstall gnuplot$ brew install gnuplot --with-x To me, I still got the following warnings after the steps above. 12warning: ft_render: unable to load appropriate fontwarning: could not match any font: *-normal-normal-10 It can be fixed by following this stack overflow post. Simply put it here where you should add this line into your ~/.bash_profile. 1export FONTCONFIG_PATH=/opt/X11/lib/X11/fontconfig And run 1$ source ~/.bash_profile to reload the config within your terminal. Alternatively, you could restart your teminal. 5. ConfigurationsPut the following configurations into ~/.octaverc. If there’s no such file, just create one yourself. 1234setenv (&quot;GNUTERM&quot;, &quot;X11&quot;)# optional if you are in favor of a more elegant prompt.PS1(&apos;❯❯ &apos;)","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"octave","slug":"octave","permalink":"liqul.github.io/blog/tags/octave/"}]},{"title":"认知螺旋","slug":"cycling","date":"2016-12-02T16:00:00.000Z","updated":"2018-04-10T15:00:12.000Z","comments":true,"path":"cycling/","link":"","permalink":"liqul.github.io/blog/cycling/","excerpt":"","text":"在认知事物的过程中，一个人的外在表现通常是循环往复的。最常见的情况是，起初对某个领域知之甚少，感觉到自己的无知；随着不断学习，又会在某一阶段认为自己已经掌握了大部分知识；而更进一步的深入，又会觉得在许多地方理解不到位，无知感又随之而来。这种现象，可以用螺旋式台阶来比喻，正如下图的上面部分。 这会产生一个有趣的问题，虽然认识在逐步加深，也就好比沿着台阶不断上升，但将这些台阶投影到一个平面（如图下半部分）会发现不同高度的台阶会投影到同一个区域里，而这意味着什么呢？从内在看，这意味着某些观念或感觉的往复，正如开头的例子里无知感会反复出现；而从外在来看，则是人对外的表现形式有一定的类似。理解了这一点有两方面的好处：首先，在认知事物的过程中，不要因为出现往复感而焦虑，因为这是深入的必经之路，如果没有这种感觉反倒是有问题的；另外，更重要的是我们能够更加理性地观察和分析他人所处的状态。 如果不理解这种螺旋上升和投影的关系，当我们观察到某人的外在表现时，很容易把他们定位在错误的认知程度上。比如在学术上，大师往往能把非常复杂的问题用浅显的语言描述清楚，而一些只知道浅显原理的人也能说出类似的段子，如果不能理性观察，就容易把“浮于表面”错误看成“深入浅出”。再比如在当前的互联网公司里存在许多杂家，他们表面上什么都懂，却无一精通，其理解事物的深度有限，从而对整体的把握也是畸形的，这跟许多从业多年的人比起来，乍一眼看上去是类似的，需要多了解一点才能真正分辨出来。 看《国史大纲》里对战国时期的一系列的描述，包括废井田开阡陌、军人和商人替代贵族等等，总感觉能看到欧洲文艺复兴时期的味道，两者都是封建社会崩溃的过程。但是，为何欧洲随后横扫全球而当年“中国”只是实现了秦朝的大一统？两者类似螺旋上升的不同层次。从战国到文艺复兴，两者相隔千年，生产力在逐渐进步，因此放在战国时期，各国只能对内扩张对外兼并；放在欧洲，则各国有更强的实力去对外扩展和兼并欧洲以外更广大的世界。 2017-04-15更新近年来中国的互联网行业大都沿袭野蛮生长的模式，其中以华为、阿里为成功的典型。虽然大家习惯认为BAT是一个梯队，但BT和A并非同一时代的企业，与那俩相比A要晚的多。野蛮生长的最大特点是少量精英+大量的廉价劳动力（即“铁打的营盘流水的兵”），最大的特点在工资低、无偿加班、强调狼性文化和价值观。原本奉行精英文化的公司（比如外企、百度），掉头学习这种野蛮模式以自救。 从时间轴上看，这只是生产力发展到一定阶段，为了迎合这种生产力而诞生的一种生产关系。随着各种技术的成熟，不再需要精英阶层从事一线生产活动，这些精英转而成为领导层去带领一些廉价劳动力完成过去看似无法完成的生产活动。然而，社会仍旧向前发展，将会进入下一阶段，而这个阶段又与上上个阶段表现出的现象类似。 简而言之，社会发展的方向是不断加深协作的深度，这将无法依赖现在的野蛮生长模式完成进化。在短期的将来，更合理的模式将是中等规模的精英间的协作。在人力资源受限的情况下，必然依赖全球的人才聚集，突破这种限制，一些高质量的开源项目正是在这种背景下完成的。适应这种状态的公司的模式不再大，依赖少量的精英人才分布式自由组合。为什么是少量呢？因为生产力的发展，已经不再需要那么多人了；为什么是分布式自由组合？因为这样的组织效率是最高的。 截止今天，仍然能看到市面上各种沿袭野蛮生长模式的公司，其中很多已经死掉了，或者在苟延残喘依赖风投的钱续命。另一些看似成功的公司，比如滴滴、膜拜、ofo等等等等，它们的命运让人拭目以待。那些近年来靠野蛮生长走向巅峰的公司，它们必将进入一轮刮骨疗毒般的清理门户。在将来的几年里，这些都值得期待~","categories":[{"name":"Thinking","slug":"Thinking","permalink":"liqul.github.io/blog/categories/Thinking/"}],"tags":[{"name":"认知","slug":"认知","permalink":"liqul.github.io/blog/tags/认知/"}]},{"title":"《国史大纲》读书笔记","slug":"guoshidagang","date":"2016-12-02T16:00:00.000Z","updated":"2019-03-14T13:57:44.533Z","comments":true,"path":"guoshidagang/","link":"","permalink":"liqul.github.io/blog/guoshidagang/","excerpt":"一、当信任何一国之国民，尤其是自称知识在水平线以上之国民，对其本国已往历史，应该略有所知。 (否则最多只算一有知识的人，不能算一有知识的国民。) 二、所谓对其本国已往历史略有所知者，尤必附随一种对其本国已往历史之温情与敬意。 (否则只算知道了一些外国史，不得云对本国史有知识。) 三、所谓对其本国已往历史有一种温情与敬意者，至少不会对其本国以往历史抱一种偏激的虚无主义， (即视本国已往历史为无一点有价值，亦无一处足以使彼满意。) 亦至少不会感到现在我们是站在已往历史最高之顶点， (此乃一种浅薄狂妄的进化观。) 而将我们当身种种罪恶与弱点，一切诿卸于古人。 (此乃一种似是而非之文化自谴。) 四、当信每一国家必待其国民具备上列诸条件者比数渐多，其国家乃再有向前发展之希望。 (否则其所改进，等于一个被征服国或次殖民地之改进，对其国家自身不发生关系。换言之，此种改进，无异是一种变相的文化征服，乃其文化自身之萎缩与消灭，并非其文化自身之转变与发皇。)","text":"一、当信任何一国之国民，尤其是自称知识在水平线以上之国民，对其本国已往历史，应该略有所知。 (否则最多只算一有知识的人，不能算一有知识的国民。) 二、所谓对其本国已往历史略有所知者，尤必附随一种对其本国已往历史之温情与敬意。 (否则只算知道了一些外国史，不得云对本国史有知识。) 三、所谓对其本国已往历史有一种温情与敬意者，至少不会对其本国以往历史抱一种偏激的虚无主义， (即视本国已往历史为无一点有价值，亦无一处足以使彼满意。) 亦至少不会感到现在我们是站在已往历史最高之顶点， (此乃一种浅薄狂妄的进化观。) 而将我们当身种种罪恶与弱点，一切诿卸于古人。 (此乃一种似是而非之文化自谴。) 四、当信每一国家必待其国民具备上列诸条件者比数渐多，其国家乃再有向前发展之希望。 (否则其所改进，等于一个被征服国或次殖民地之改进，对其国家自身不发生关系。换言之，此种改进，无异是一种变相的文化征服，乃其文化自身之萎缩与消灭，并非其文化自身之转变与发皇。) 前三章包括序言没有写笔记，一来当时没做，二来其中大量远古事件只有梗概。但并非不重要，也无谓追求完美。 第四章”霸政时期”第三节：“齐桓晋文之霸业” 总观当时霸业，有二大要义：一则为诸夏耕稼民族之城市联盟，以抵抗北方游牧部落之侵略，因此得保持城市文化，使不致沦亡于游牧之蛮族。二则诸夏和平联合以抵御南方的楚国帝国主义之武力兼并，因此得保持封建文化，使不致即进为郡县的国家。 其大势为文化先进诸国逐次结合，而为文化后进诸国逐次征服。同时文化后进诸国，虽逐次征服先进诸国，而亦逐次为先进诸国所同化。其文化落伍诸部族，则逐次消灭，或逐次驱斥。 原来历史书上所说的“春秋五霸”，实际上主要是由于外力作用下，诸国之间形成的，替代没落的周天子的抱团方式。随着所谓戎狄蛮夷的逐渐同化和被驱逐，这种外在的压力最终不复存在，国与国之间就逐渐开始兼并了。 文化是个很有意思的东西，往往先进的文化会被落后的文化征服，似乎越是有文化，就越是骄奢淫逸…而最终实际上要么征服者被先进文化同化，要么由于处理不好这个问题短短数年就被推翻，历史上这样的例子太多太多了。秦国能统一，大概只是因为他地处边缘，最后被中原文化同化，所以在前辈们已经趴在沙滩上动不了的时候上去补了个刀。 第五章“军国斗争之新局面”第二节：“从宗法封建到新军国之种种变迁”书里总结了几个变化： 郡县制替代了贵族世袭的采地，食禄制度兴起和养贤 战争由贵族之间的“游戏”变成残酷杀戮 军民渐趋分治，商业和货币兴起，军人和商人兴起而替换了传统的贵族 废井田开阡陌和税收制度改革 这些变化存在内在的逻辑关联。周代礼崩乐坏，贵族之间为争夺有限的资源而产生激烈竞争，从而类似中世纪骑士们”过家家“似得战车战转变为步卒肉搏战。为了战胜对方，不再依赖养尊处优的贵族阶级，而必须产生专业的杀戮工具———军队，而为了鼓励底层士卒又产生了军功系统。军人有了晋升阶梯，成为新贵族，而他们需要得到土地奖励，导致了废除与传统贵族分封制对应的井田制，进一步产生了新土地制度下的税收体系。贵族的没落，商人从贵族的附属流入民间，进入自由商业时代，从而引发了货币的兴起。 看这一系列的描述，总感觉能看到欧洲文艺复兴时期的味道，两者都是封建社会崩溃的过程。但是，为何欧洲随后横扫全球而当年“中国”只是实现了秦朝的大一统？两者类似螺旋上升的不同层次。从战国到文艺复兴，两者相隔千年，生产力在逐渐进步，因此类似战国时期各国对内的扩展和对外的兼并，欧洲各国有更强的实力去扩展和兼并，因此不仅在欧洲，它们有能力去兼并更广大的世界。 另外，以前从历史课本里知道的秦朝大一统后的政策，实际上只是战国的延续，并非秦朝创立的新制度。 第六章“民间自由学术之兴起”第二节：“儒墨两家之兴起” “兼爱”与“仁”不同。仁非不爱人，特有亲疏等差，故说“孝悌为仁之本”。人决无不能爱其父母而能爱别人者。“兼爱”异于“别爱”，乃一种无分别之爱，亦可说是一种大同之爱，抹杀个人，只就大群着眼。 但是儒、墨两派，有他们共同的精神，他们全是站在全人类的立场，来批评和反对当时的贵族生活。儒家精神比较温和，可说是反对贵族立场的右派；墨家较激烈，可说是左派。以下战国学派，全逃不出儒、墨两家之范围。 正因为孔子、墨子身处不同环境，所以产生的思想也就有差别。孔子认为孝是爱的本源，通过推己及人来把这种爱由自己父母推及其他人。这应该是家族观念的一种延续。而墨子出身寒苦，则没有深刻的家族意识，有的是抱团取暖的底层人民，所以更强调一种类似共产主义的东西。 这里出现了“右派”和“左派”，对应保守和激进。在改革的时候，保守派更加容易容于当时环境，虽然存在一些不彻底的地方，但温和的演变带来的破坏力也就比较小。激进常常伴随着不懂得妥协，一旦成功，其破坏力通常将不好的和好的一并带走。 第五节：“贵族养贤” 唯四公子门下，真士少，伪士多。只见游士气焰之高涨，而不见他们的真贡献。 此非当时之无士，四公子争以养士为名高，动称“门下食客三千人”，何来有如许士？伪滥杂进，则真士不致。 物以类聚、人以群分，这个道理无论在那时还是今天都是成立的。一旦群体里滋生了争名夺利的氛围，有识之士也就只能敬而远之或沉默不语了。 第六节：“平民学者间之反动思想” 老子的理论，其要者，反尚智，反好动，反游士食客，皆针对当时的现象。注：此种现象，皆春秋时代所无。 由此可知老子主要生活在战国时期。 第七章“大一统政府之创建”第二节：“国家民族之传成” 秦以下，虽封建遗形尚未全绝，然终不能再兴。 自此景、武下逮东汉，封建名存实亡，寸土一民，皆统于中央，诸封王唯食邑而已。 唯明初封诸王，欲以封建、郡县相杂，然一、再传即废。 以唐太宗之英武，唐初文、武诸功臣之出众，诚使君臣割地，各自专制一方，…，。所以不能尔者，由国人对于政治意义之认识，久已不许复有贵族世袭封建制度之存在。 观念最难形成，也最难改变。《乌合之众》里说，一个民族有其固有的基因；《晚晴七十年》里说一种制度代替另一种制度要二三百年时间。辛亥革命把民主思想带入人心后，复辟就变得不可能了。可笑明初居然想复古，结果落下一朝的各种残疾，可笑。 战国之秦乃如春秋之楚，不得即此谓秦果夷瞿秋。 中华民族本是各民族融合而成。若怀着无比狭义的民族主义，哪里还有所谓“中华民族”？ 第三节：“第一次统一征服之出现及其覆灭” 始皇曰：“天下共苦战阀不休，以有侯王。天下初定，又复立国，是树兵也”。 秦君臣此番建树，于中国史上政体之促进有大功绩。后人空以专制讥秦，疏欠平允。 “罗辑思维”介绍《秦迷》一书，讲到秦始皇为统一六国的一系列政治手段，不知道是真是假。但如果上面所言不虚，则在那时候始皇果然十分先进。 秦代政治的失败，最主要的在其役使民力之逾量。 创业中都难免犯各种错误，何况第一次形成一个统一的国家。后世不应该以现在的思想去评议古人的决定。 第四节：“平民政府之产生” 项羽、田横之徒皆贵族，而皆不能成事，此可以观世变。 以前没有从这个方面想过。 当时平民政府的第二个反动思想则为“无为而治”。 即如萧何定律，而夷三族、妖言令、携书律等皆存在。至孝惠、高后、文帝时逐渐废除。唯精神上汉则恭俭，秦则骄奢，此其异。 汉初政治并不比秦国来得先进，相似的政治制度，在不同的掌权人手里，也会出现不同的结果。但这种人治的成分必须减弱才是系统能长期良性运行的保证。可惜后代还是有不明白这个道理的浑人当道。 平民政府有其必须完成之两大任务，首先要完成统一，其次为完成文治。 平民政府只能通过暴力革命夺取政权，然后自然对军人要论功行赏，过渡到文治需要相当漫长的时间。 第八章“统一政府文治之演进”第二节：“西汉初年之政府” 如是则当时的政治组成，第一层是宗室，第二层是武人，第三层是富人，第四层是杂途。注：…。文学、儒术亦杂途之一。 这个政治结构或许后代会有所演进，但基本面还是差不多的。这也就算是平民学术兴起带来的结果吧。 第三节：“西汉初年的士人与学术” 秦代焚书，最主要者为六国史记，其次为诗、书古文，而百家言非其所重。 焚书本起于议政冲突，博士淳于越称说诗、书，引据古典，主复封建，李斯极斥之，遂牵连而请焚书。 焚书缘起于朝堂上的政治冲突，但领土统一后消灭人民对故国的怀念，最好的方法似乎也是消灭故国的历史。但任何激进的做法其效果都难以理想，尤其是针对人们心理上的问题。秦刚刚灭六国，六国虽然被灭但人还在，除非政权稳固到反对的声音都安静下来，而这需要非常长的时间，否则只能招致报复和反叛。 第四节：“中央政府文治思想之开始” 注：农民政府之好处在真朴，坏处在无礼貌；可爱初在皇帝、宰相如家人，其弊处则皇帝待宰相如奴仆。 淳朴通常意味着价值观混乱、多重标准。 汉初政治，往往有较秦为后退者，注：此因平民政府缺少学术意味之故。 回想中国历代，每每政治清明国力强盛的时候，其学术活动也是极其活跃的，或体现为诗、辞、歌、赋；每每政治昏暗的时候，其学术活动也陷入低潮，内容晦涩黯淡。虽然这只是一个侧面，但仔细想想也不无道理。 第八节：“王莽受禅与变法” 王莽的政治，完全是一种书生的政治。 王莽的失败，变法让贤的政治理论，从此消失，渐变为帝王万世一统的思想。政治只求保王室之安全，亦绝少注意到一般平民的生活。这不是王莽个人的失败，是中国史演进过程中的一个大失败。 《国史大纲》的论述里谈到，王莽篡位不是一件单纯的个人行为，否则不至于能坚持长达18年的时间。这实际上承袭了儒家一直以来的轮换让贤思想，是一种政治结构的实践。与之对比民国初年张勋复辟正是由于是一种反先进思想的闹剧，所以不能真正在历史上留下什么。从现在来看，似乎觉得那必然是失败的，或许只是因为接受了“万世一统”的观念太深了。这真的是必然吗？当然不是，只是在合适的时机做一件合适的事情会得到好的效果；反过来，哪怕出发点和事情本身是好的，时机不对，得到的效果可能是相反的，而且在人们心理上造成的负面影响可能很久都无法消失。Sigh，这次失败实际上意味着后世再不会有人尝试这种“脑残”的想法了。 第九章“统一政府之堕落”第四节：“外戚参加王室的由来” 西汉初年，宗室、功臣、外戚，为朝廷之三大系。 随着王朝的演进，由功臣而来的荫功集体首先变得越来越弱。政府渐渐独立于王室，而王室则需要外戚来牵制政府的权利。只是宗室逐渐堕落，而导致外戚实力增大而已。不过，只要外戚不至于改朝换代，则没有大碍，因为外戚是不世袭的。 第五节：“宦官参加王室之由来”所谓九卿，是指 太常：掌宗庙礼仪 光禄勋：掌宫殿门户文官 卫尉：掌宫殿门户武官 太仆：掌车马 廷尉：掌刑辟 大鸿胪：掌蛮夷归化 宗正：掌宗属 大司农：掌农货 少府：掌山货 不得不说，虽然名字听起来都很文雅，实际上都是些帝王身边的贴身侍从或“家务官”。只是帝王身边的侍从，后来都演变成其它更实际职能的官员，至古至今都一样的道理。 宦本宦学、仕宦，非恶称也。 宦本是做官与做学问，“本无恶称”，奈何历史上留下骂名的比较多，再加上身体残疾，所以成了一种恶称。比起外戚，宦官的可传递性更弱，所以会被选择成为王室集团的一员，与外朝争夺权力。其实，从宦官当权的本质上来讲，也只是帝王堕落而导致的。 第十章“士族之新地位”第一节：“士族政治势力之逐步膨胀” 博士弟子额之日益增添。 须知要扩大一组织的影响力，并不是要层层设卡减少参加的人数，美其名曰提高质量。当然也不能鱼龙混杂，否则会造成前面提到过的“真士不至”的问题。两者之间要求得一个平衡。 全国各郡县常得平均参加中央政局，对大一统政府之维系，尤为有效。 参与感总是凝聚组织结构的最重要手段。 第三节：“太学清议” 士人在政治、社会上势力之表现，最先则为一种“清议”。 东汉自光武、明、章，虽云崇奖儒业，…，清议为转移，直至东汉末叶，此风弗衰。 这段文字读起来非常不容易懂，尤其是其中一些句子如： 国家喜文法廉吏，以为足以止佞，然文吏习为欺谩，廉吏清在一己，无益百姓流亡，盗贼为害也。 承王莽后，加严猛为政，因以成俗，是以郡国所举，多办职俗吏，不应宽博之选。 王充论衡亦极辩世俗常高“文吏”，贱下“儒生”之非。 这里面提到的“文法廉吏”、“文吏”、“廉吏”、“俗吏”之类的实际上指代一类人，他们有一定学问，被地方长官选为辅助的官吏，处理一些复杂的政务；而对应的“儒生”则对应那些有学问但不是官吏的人。文吏由于长期处理事务，有一定的执政经验，所以在社会上的地位比儒生要高；相对的儒生没有行政经验，所以被认为不能承担责任，遇到事情战战兢兢不知道如何处理。 文吏可以由儒生转变过来，而长期处理杂务会导致忠诚正直方面欠缺。一些儒生因为看到文吏的腐败行为而拒绝加入其行列。这些儒生看到其它儒生为了当官而放弃自身在正直方面的追求而责骂他们，这导致了社会上更加轻谩儒生。针对这种现象，《论衡》里的“程材篇”有很多讨论。 正因为东汉初年这些议论，导致了 稍后郡国查举，渐移趋向。言事者谓郡国查举不以功次，养虚名者累进，故守职者益懈，而吏事陵迟。 既然推荐制度不考虑完成的功绩，而重点看个人的品行和社会上的名望，所以渐渐人们开始只注重名声，因此从政者为保持高尚的名节，很多实际事务便无法处理。从政者注意民间舆论是一种进步，然而过于影响政治推移就不好了。 矫枉过正，这也是一种必然的结局。 第四节：“党锢之狱” 因此宦官之势，乃非外朝士人之力所能摧陷廓清，名士不得不内结外戚，而外戚到底是一种腐败的因袭体，名士遂终与之两败。 国家虽有法度，而有些群体不在约束范围内，或其行为不为外人知晓而实际上无法执行，则法度是不会起作用的。前者可以通过立法来弥补，或者则需打破一些根深蒂固的东西，实际上比立法更难。 依靠一种污浊的势力去打击另一种污浊的势力，此消彼长，最终只能变成一种“循环发生”的现象了，而国家在这个过程中 “人之云亡，邦国殄瘁”。 即身系国家安危的贤人都死了，离亡国就不远了。 黑暗腐败的汉王室，终于倾覆，依附于王室的外戚与宦官，亦同归于尽。 外戚与宦官实际上寄生于王室，皮之不存毛将焉附？ 第五节：“门第之造成” “累世经学”与“累世公卿”，便造成士族传袭的势力，积久遂成门第。 做学问在当时是少数家族专有的权益，而经学又是仕途的入门手段，所以传袭成门第。另一方面，上升途径若只掌握在少数群体手中，则长期以往，那些无法上升群体的矛盾将逐步激化。 自东汉统一政府的倾覆，遂变成变相之封建。长期的分崩离析，而中国史开始走上衰运。 钱老的观点一直是：中国政治体制需在平静中不断演进，而长久的战争只会造成倒退。回顾古代、近代一些运动，无不印证这个观点。钱老亲历一段岁月，作为那个时代的知识分子，身系中华民族文化是否能传承下去，书写《国史大纲》来从中国历史的发展来解答和对抗一系列的“新思想”，不可不称作一位斗士。这个线索不妨听听梁文道在其一千零一夜里对这部书的讨论。 第六节：“东汉士族之风尚” 古者刑不上大夫，故贵族阶级相互有隙，不得直于法庭，则以私斗决之。 大概决斗也是贵族特有的一种形式吧，无论在东方还是西方。 礼有之：“父母存，不许友以死” 大意是指父母在的时候，不能为朋友赴死。如《史记》刺客列传里聂政说：“臣所以降志辱身居市井屠者，徒幸以养老母；老母在，政身未敢许人也”。 可见士族风尚学古。 道德自为人生不可缺少之要素，然亦只是人生中一端。 文章指出太过注重道德则会导致两个方面弊端：其一，人们争相以道德来分高下，如苏轼所说 “上以孝取人，则勇者割股，怯者庐墓；上以廉取人,则弊车羸马,恶衣菲食。” 这句来自苏轼的《议学校贡举状》，又读到其中有以下议论 1凡可以中上意，无所不至矣。德行之弊，一至于此乎！自文章而言之，则策论为有用，诗赋为无益；自政事言之，则诗赋、策论均为无用矣。虽知其无用，然自祖宗以来莫之废者，以为设法取士，不过如此也。岂独吾祖宗，自古尧舜亦然。《书》曰：“敷奏以言，明试以功。”自古尧舜以来，进人何尝不以言，试人何尝不以功乎？议者必欲以策论定贤愚、决能否，臣请有以质之。近世士大夫文章华靡者，莫如杨亿。使杨亿尚在，则忠清鲠亮之士也，岂得以华靡少之。通经学古者，莫如孙复、石介，使孙复、石介尚在，则迂阔矫诞之士也，又可施之于政事之间乎？自唐至今，以诗赋为名臣者，不可胜数，何负于天下，而必欲废之！近世士人纂类经史，缀缉时务，谓之策括。待问条目，搜抉略尽，临时剽窃，窜易首尾，以眩有司，有司莫能辨也。且其为文也，无规矩准绳，故学之易成；无声病对偶，故考之难精。以易学之士，付难考之吏，其弊有甚于诗赋者矣。 “古文运动”本在祛除读书人只注重华美的文体而忽略文章的内容的毛病。出发点是好的，但却也容易走向另一个极端，即上面所说的“无规矩准绳”和“无声病对偶”。表面上看这些文章讨论时政，似乎更切合实际，但“易学”、“难精”，没有统一的衡量标准也导致“难考”。正如今天所说的素质教育，较之过去统一的高考，实则有更大弊端。 另一方面，正如苏轼所说，那些文采华美的文章流传自今，有多少出自贤人的手笔？又有多少出自愚者的手笔？两相比较不难看出其正面远大于负面。王国维《人间词话》的论述，诗词更多的是反映作者胸中的大气象，如无大气象又怎能写出这样的文章？正如前文提到中国国运常常跟文化繁荣的程度相呼应，这应该就是内在的一点原因吧。 注：上面提到的唐代的”通榜“是一种不糊名的人才选拔机制。 太过注重道德的另一个弊端则是 若做事太看重道德，便流于重形式而忽略了内容的实际。 文中提到将军在尚未败北时已自杀殉国，只图自己留个好名。又想起所谓“天子守国门”和崇祯煤山自裁（虽然这并非完全归于道德），是不是有点内在联系呢？至少，这在当时直接导致的后果是 所以名士势力日大，而终不能铲除宦官的恶势力。因东汉人只看重私人和家庭的道德，故王室倾覆后，再不能重建一共戴的中央，而走入魏晋以下之衰运。 实际上在帝制的作用下，士人已渐渐由国家的创业者变成临时工，所以缺乏积极性也是难免的了。而随着帝制的不断推演，这种现象愈演愈烈，近代乃至当代的一些现象就不难理解了。 第十一章“统一政府之对外” 因此中国史上对外之胜负、强弱，几乎完全视国内政治为转移。 自秦朝统一到近代以前，这的确是个事实，无怪国人应对列强手足无措了。 第一节：“两汉之国力比较” 西汉的立国姿态，常是协调的、动的、进取的。 东汉的立国姿态，可以说常是偏枯的、静的、退守的。 这是由建都的位置引出的。早先听说过一个观点，即定都在西边的国家常常是强大的，如西汉、唐；定都在东边的常常是羸弱的，如东汉、宋、明。只觉得这里隐隐有一种内在原因，不甚清楚，而这里给出的讨论让人深思。 自古以来，中国东部土地肥沃、气候适宜，因而经济文化要比西部发达；西部生存环境相对恶劣，所以人民尚武而民风彪悍。定都在西部时，由于政治力量推动（这种力量当容易理解），东部繁华的经济文化向西部侵染，而西部磅礴气象也向东部扩散，实际上对双方都是有大好处的。文化离开大气象便成为靡靡之音，而彪悍的民风如没有文化的熏陶也会逐步走向失控和野蛮。这种东西方循环恰如人体的循环系统，隐隐维持和促进着社会经济文化发展。在这种循环过程中，人为因素便是其中的政治力量。定都在东部时，这种政治力量就不再推动大循环，而转为一种单一的、小规模的、不健康的循环。在缺乏政治力量推动的情况下，人民自身不会去弥补这种缺失。 文中列举 汉诸帝并有陵寝徙民的制度。 西汉实行将东部人民迁移到西部，这即政治力量的推动。如果没有则 人情安土重迁，宁就饥饿，犹群羊聚畜，须主者牧养处置。 东汉在这种维持稳定力量的缺失下，最终 东方食少而有黄巾，西边多事而又董卓，此诚两汉兴亡一大关键也。 联想到美国选州府，常常会在一个没听说过的小城市，其目的也是希望能凭借政治力量逐步平衡各地的差距，最终达到稳定国家的目的。虽然一段时间可能会造成一些效率降低，但长远来看正面作用更可观，不得不佩服这种高明的选择。 另一方面，西汉 大抵是一个杂色的局面。东汉则渐渐从杂色的转变成一色，… 又联想到一些企业招聘员工的时候常注重一个dynamics，即吸纳不同国家、文化背景下的人群。其基本原理在于不同人对事物的观点常常是不同的，这样的环境更加容易产生优秀的思想。又如美国高校有毕业生不能留在本校任教职。这种种做法都是为了避免一种“近亲繁殖”的问题。 东汉这种按照某种规则给不同人群划分品节的陋习也就随着历史流传给了后面的朝代。 太宗置官品令，谓房玄龄曰：“朕设此官员，以待贤士。工商杂色之流，假令术逾侪类，止可厚 给财物，必不可超授官秩。 可见唐朝也袭此流弊。 第二节：“西汉与匈奴” 观去病之将兵，较之项王未多逊。… 一段注释解释了一个以前困扰我的问题，即为何在《史记》中卫青霍去病合为一列传（111），而独为李将军一列传（109）。原因在于卫青和霍去病同为亲贵，所以与底层豪杰李广恰似两个相对的党派。 第三节：“东汉与西羌” 当时士大夫见朝事无可为，唯有拥兵以戮力边檄，尚足为功名一径… 这也导致了后来三国各地割据的局面。 第十二章“长期分裂之开始”第二节：“旧政权之没落” 散漫的农民在饥饿线上临时结合起来，其力量不够得推翻他。 且以中国疆域之展布，纵使大饥荒，亦必夹有丰收的地带，要一般农民一致奋起，事亦不易。于是无可团结的社会，乃借助于“宗教”与“迷信”。农民结合于宗教与迷信的传播之下，而一致奋起，成为东汉末年之黄巾。 国家本是精神的产物，把握时代力量的名士大族，他们不忠心要一个统一的国家，试问统一国家何从成立？ 历史上成功的农民起义，往往不真的是农民群体在起决定性作用，而是附庸在这个群体上真正起到决策作用的精英。正如作者所说，农民不具备团结一致的奋斗精神，往往会被一些近期的利益所诱惑，从而从内部分崩离析。看黄巾之乱、太平天国、李自成，再对比刘邦、朱元璋，就能发现其中的区别。精英阶级一旦无法通过合法的晋升阶梯入仕，那么积累的力量就会从别处发泄出来。 第四节：“新政权之黑暗” 自古受命及中兴之君，易尝不得贤人君子与之共治天下者乎？及其得贤也，曾不出阎巷，岂幸相遇哉？上之人不求之耳。今天下尚未定，此特求贤之急时也。“孟公绰为赵、魏老则优，不可以为膝、薛大夫”。若必廉士而后可用，则齐桓其何以霸世！今天下得无有被褐怀玉而钓于渭滨者乎？又得无盗嫂受金而未遇无知者乎？二三子其佐我明扬仄陋，唯才是举，吾得而用之。 夫有行之士未必能进取，进取之士未必能有行也。陈平岂笃行，苏秦岂守信邪？而陈平定汉业，苏秦济弱燕。由此观之，士有偏短，庸可废乎！有司明恩此义，则士无遗滞，官无废业矣。 昔伊挚、傅说出于贱人，管仲，桓公贼也，皆用之以兴。萧何、曹参，县吏也，韩信、陈平负汗辱之名，有见笑之耻，卒能成就王业，声著千载。吴起贪将，杀妻自信，散金求官，母死不归，然在魏，秦人不敢东向；在楚，则三晋不敢南谋。今天下得无有至德之人，放在民间，及果勇不顾，临敌力战：若文俗之吏，高才异质，或堪为将守；负汗辱之名，见笑之行；或不仁不孝，而有治国用兵之术。其各举所知，勿有所疑。 魏武三诏令，真前无古人后无来者，霸气侧漏。 “天下无有孤，不知几人称王，几人称帝？”，此不足为篡窃之正大理由。 阴谋不足以镇压反动，必然继之以惨毒的淫威。 人心是奇怪的东西，一个正大光明的理由能让它平静下来，反之，如果一开始的理由就是不对的，将来总有一天要还清当年欠下的债务。再强大的武力，也只是在一段时间内能镇压，不是长久之计。 正是这种不够光明正大的理由，奠定了魏晋这一历史上较黑暗混乱的时代。 第五节：“思想界之无出路” 但要提倡法治，起码的先决条件，在上应有一个较稳定的政权。【注：政权不稳定，法治精神无所倚依而生根。】政权之稳定，亦应依附于此政权者先有一番较正义，至少较不背乎人情的理想或事实。 人们往往只注意到法治的好处，却忽略这一基本的前提。在政权不稳定的状态下提倡法治，只会使政权更加动荡，导致更糟糕的结果。作者在文字上总有一个“较”字修饰，读起来不禁觉得实在精辟。 第十三章“统一政府之回光返照”第二节：“西晋王室之弱点” 一、没有光明的理想为之指导。二、贵族家庭之腐化。 西晋处在历史这样一个节点上，贵族尚没有意识到理想、教育的重要性，所以有乘“羊车”、食“肉糜”的奇葩出现，而当代的“新贵族”较以往已经进步很多了。 第四节：“怀慜被掳与人心之反映”作者从帝王、皇后、大臣、将军、士族几个方面的讨论，给人一个直观的印象：西晋是一个彻头彻尾无廉耻气节的政权。 第十五章“北方之长期纷乱”第三节：“五胡十六国大事简表” 盖浅化之民，性情暴戾，处粗野之生活中，尚堪放纵自适。一旦处繁杂之人事，当柔靡之奉养，转使野性无所发抒，冲荡溃决，如得狂疾。 可笑一些“精英”附庸在这样“粗野”的人身边，以为可以依仗成就一番事业，往往最终难逃悲催的下场，正是由于不懂这样的道理呀！ 第十六章“南方王朝之消沉”第二节：“南朝王室之恶化” 南朝的王室，在富贵家庭里成长起来，他们只稍微熏陶到了一些名士派放情恣志的风尚，而没有浸沉到名士们的家教与门风，又没有领略得名士们所研讨的玄言与远致。 只注意到外在表现而不理解内在机理，今天这样的情况不也大有人在么？一本本成功学书籍畅销，一篇篇心灵鸡汤文在朋友圈流传，一条条形而上的不知所云的分享大都是类似的情况吧。多少人不去多读几本书，多做些思考和研究，不在实践中总结反思，而是随手捡起几个普适的概念大肆宣扬，真是误人误己。 由名士为之则为雪夜访友，无知识，无修养，则变为达旦捕鼠。由名士为之则为排门看竹，无知识，无修养，则变为往寺庙偷狗吃。 名士的风尚虽只能独善其身，但总算雅致；皇室以此为精神寄托本就不对，何况画虎类犬，就只能贻笑大方了。后人看着当然觉得好笑，在当时却是整个社会一片黑暗没有出路了。 第十七章“北方政权之新生命”第三节：“魏孝文迁都及北魏之覆灭” 塞北荒寒，不配做新政治的中心。 地理决定论自有其内在道理。 惜乎孝文南迁五年即死。 历史从来不以人的意志为转移，而这正是所谓的气数已尽吧。 文治基础尚未稳固，而武臣出路却已断塞。 改革最忌讳建设尚未完成而已经将原有的关系彻底打破。这样的改革如果顺利则只能说是运气好，否则造成的反弹可能完全推翻改革本来已经带来的正面的效果。 一个国家，同时摆出两个绝不相同的社会，势必酿乱。 然而一个国家又非常容易酿成两个绝不相同的社会。比如前面讨论东西汉差异的时候提到的，定都的位置不同，在长安则由政治力量引导产生国内东西大循环；在洛阳则东部南北小循环，使得东西隔绝。如果政治力量无法处理好这种地区间的均衡，则必然酿成祸乱。 凡历史上有一番改进，往往有一度反动，不能因反动而归咎改进之本身；然亦须在改进中能善处反动方妙。魏孝文卒后，鲜卑并不能继续改进，并急速腐化，岂得以将来之反动，追难孝文！ 鲜卑本北方稍欠文化的部落，一旦处于温柔繁华的环境，腐化堕落的速度更是迅速，这与人成长的环境是紧密相关的。想起游戏《十字军之王2》里的设定：部落虽有较强武力，但在政治落后，容易招致内乱，而战斗士气随战争结束时间增长会迅速下降。这样的设定更加符合真实的情况，对比《三国志》系列喜欢把蛮族势力的武将个个设置成武力一般头脑简单的弱智要高明好多。 第四节：“北齐北周文治势力之演进” 齐律尤为隋、唐所本。 由唐至清，皆本隋律，隋律则本于齐。 从这种意义上讲，这似乎是上一个时代的终结与下一个时代的开始。高晓松所说中国古代的时代划分，好像也是以这里为分界的。 治民之本，莫若宰守。治民之体，先当治心。 这句话的大意是，“地方行政长官是治理人民的根本。要想治理人民，先要治理他们的内心”。这与阳明先生的心学似乎是呼应的。 于是以前的官吏，为门资所应得；而此后的官吏，则将为民众负责任。 这样的设定，在建设的同时不至于彻底废除历史的旧习，是一种政治上的妥协，于进步实有利。 僚吏俊彦，旦理公务，晚就讲习。 从学术影响到政治，回头再走上一条合理的路，努力造出一个合理的政府来。 “从学术影响政治”是钱老支持的主张，从前面的一些讨论中早有明示。衷心希望将来能真的有这么一天。 第十八章“变相的封建势力”第一节：“九品中正制与门阀” 三国丧乱之际…，一时逆转，而倒退为秦、汉初年之军功得官。 越是乱世，越崇尚军功，越是崇尚结果导向而不计后果。此消彼长，意味着与军功相对的，较斯文的方式在社会上无法晋升。想到当今各互联网公司提倡的狼性文化、军功导向，不禁觉得离文化真正兴盛还有很远的路要走。 于是有魏尚书陈群之“九品官人法”。 九品官人法于州设置大中正，于郡设置小中正，中正负责本地贤良的举荐。中正并非真正的官职，而是由中央官员兼职，因当时处乱世而人才集聚于中央。 与汉代查举相比，九品中正制有两个最大不同： 查举人才归地方而中正制归中央； 查举只涉及入仕而中正制是一套完整的官员入仕、考察机制。 处于乱世，中央往往急需各地人才，所以这样的设定无可厚非。待乱世已平，人才的流动仍然遵循从地方到中央就不合时宜了。这直接导致了地方人才凋零，而汇聚中央的人才又无处施展自己的能力。 所以当时已有人察觉这样的状态，而有 今天下复归一统，自当仍将查举权付之地方长官，不必再要一个中正。 九品中正制还注重一个“品”字。 “品”者履行，“状”者才能、绩效。 处乱世时，由于只重视军功，导致为官者品行良莠不齐。而九品中正制正为校正这种过失，而重新强调品的重要性。官员品行好则升迁，品行不好则降级，于是造成一些高品低能的状况。 一种制度往往因当时的社会状态产生。一旦社会状态发生变化，制度需要做相应调整，否则就会不合时宜。但制度形成容易，变化则难，因制度而产生的利益阶层总不会轻易放弃自己的利益。然而，有制度总比没有制度要好。回想起多年前，从小学时候我就有个不解的问题：比如我所居住的县城里，县长这样的官员是如何成长起来的？这个问题实际上我现在也不清楚，仿佛没有固定的方法和评价标准，既不看是否学识出众，也不看是否有良好的品行。想到此，再与上面的状态比较，不难发现现在的政治制度仍然在乱世走向治世的初级阶段而已。 第二节：“学校与考试制度之颓废” 东汉的累世经学，即为造成门阀之一因。但到门阀势力一旦长成，学校与考试制度即不见重要，难于存在。 联想到今天的两个现象，即所谓素质教育和出国热。这些现象的本质是什么呢？是希望打破以往国家制定的高考制度，而独立出另外的标准。这些标准跟高考制度最大的差别在哪呢？更加考察出身而已。农村的孩子没有机会学习钢琴、舞蹈，没有钱送到国外镀金。想想这些城里的家长，有相当部分实际上是高考制度的受益者。这与书中讨论的问题难道不是一个么？历史总是惊人的相似。 第五节：“北方的门第” 故南士借上以凌下，北族则附下以抗上。 真正的利害在于存在的合法性。南方士族与皇族是相互依存的，故南士的合法性在于皇族而不在民众；北方士族与异族皇族始终不是一路的，所以其合法性在于能组织民众。联想到军阀的合法性不在自身而在于能否为依附于军阀的下属们谋求利益。在真正的利害关系之前，其它因素如民族大义等，只能是少数人的追求，而从大势上讲是不足道的。 第十九章“变相的封建势力之下之社会形态（上）”第二节：“农民身份之转变” 李典之众自有武装，故称“部曲” 部曲在这里指私兵，即归附之前就已经拥有一定武装力量。 局势逐渐澄清，各地的强宗豪族，逐渐消并其势力于几个大势力之下，再建政府，这便是三国。 于东汉末年的乱世，由于中央集权的瓦解，地方只能自建势力以在乱世图存，而待大势力兴起则各择归附，于是有所谓“三国”。 两汉以来的农民，以公民资格自耕其地，而向政府纳租。现在是政府将无主荒田指派兵队耕种，无形中，农田的所有权，又从农民手里转移到政府去。 读各种历史，容易受到情感影响而迷失本质，往往纠结于正义和邪恶。实际上，真正应该关注的是内里本质的东西，只有拨开表面上的掩饰才能看到事情的真相。土地的所有权在谁手里，谁的社会地位才能有一定保障，否则无论以任何借口，都无法掩盖其暴乱的事实。 无论何种正义的理由，都不能剥夺私有财产。 第三节：“西晋之户调制度与官品占田制” 这一个制度的用意，并不是授予强宗豪族以私占的特权，乃是要把当时强宗豪族先已私占的户口及田亩括归公有，而许他们一个最高限度的私占权。 政治的艺术在于和平演变。在当时的政治环境下，从强宗豪族手里拿回户口和土地的确难以办到，但其中的政治智慧却不容忽视。在实力欠缺的情况下，勉强坚持一些不切实际却看似政治正确的主张是没有意义的，反而容易招到反噬的效果。所以不如以量化的方式先妥协，待实力发生变化后再逐步达到目的。历史上因为校真一两个口号或主义，没有推动历史进步，反而造成短暂倒退的例子比比皆是。而那些明知这样的后果，仍然煽动无知人群的背后的人，实际唯恐天下不乱而无法造就个人英雄，其心可诛。 第五节：“兵士的身份及待遇” 其先入士籍者得优廪，又可免役，其时则兵胜于民。渐次军旅之事，不为时重，则士伍唯以供役，又廪给日薄，其时则农胜于兵。 识时务者为俊杰。 要为军人谋出身，势必与贵族特权实力相冲突，如战国吴起在楚、商鞅在秦之事。 军人的地位，只与奴隶、罪犯相等，从军只是当苦役。 战国为用兵之际，所以自然有人站出来提高军人的社会地位，而在相对和平的时代，不会有这样的事情。此外，看既得利益集团的人在什么行业就能看出利益分配的大头所在。 军人的地位如此，如何可以为国宣劳，担负光复中原的重任？ 实则社会利益之分配必须与目标一致，否则要么无法达成目标，要么演变成无法控制的结果。 第二十章“变相的封建势力之下之社会形态（下）”第二节：“北魏均田制” 此制用意并不在求田亩之绝对均给，只求富贵者稍有一限度，贫者亦有一最低之水平。 …然此等皆不足为此制深病，治史者当就大体着眼也。 仍然体现着政治之循序渐进的思想。 尤要者则在绝其荫冒，使租收尽归公上。 “荫”则为逃避劳役而多户假作一户，“冒”则为丧乱中无主的土地被冒领。实际上，这两者都是针对豪强而言，普通人没有能力去荫冒。均田制使得“多户多得”，所以没有人再愿意附属在豪强上，从而独立出来。 正租入中央国库，义租纳郡县，备水旱灾。 那时已经有建立地方粮食储备以应对水旱灾害，实历史进步也。 不教民战，是谓弃之。临时抽丁，皆弃之也。 临时抓壮丁，又没有经过军事训练，实际上就是放弃他们呀！这样的情况下，军队又怎么能有战斗力呢？ 第三节：“西魏的府兵制” 府兵制长处，只在有挑选、有教训；而更重要的，在对兵士有善意，有较优的待遇。将此等兵队与临时的发奴为兵、谪役为兵，以及抽丁为兵相敌，自然可得胜利。 “仁者无敌”。 所以自行“均田”，而经济上贵族与庶民的不平等取消；自行”府兵“，而种族上胡人与汉人的隔阂取消。 能看到社会上的主要矛盾，而实施缓和的政治手段逐步消除这种矛盾，才是为政者需要不断思考的问题。 古之帝王所以建诸侯、立百官，非欲富贵其身而尊荣之，盖以天下至广，非一人所能独治，是以博访贤才，助己为治。若知其贤，则以礼命之。其人闻命之日，则惨然曰：“凡受人之事，任人之劳，何舍己而从人？”又自勉曰：“天生儁士，所以利时。彼人主欲与我共为治，安可苟辞？”于是降心受命。其居官也，不惶恤其私而忧其家。故妻子或有饥寒之弊而不顾。于是人主赐以俸禄、尊以轩冕而不以为患，贤臣受之亦不以为德。为君者诚能以此道授官，为臣者诚能以此情受位，则天下之大，可不言而治。后世衰微，以官职为私恩，爵禄为荣惠。君之命官，亲则授之，爱则任之。臣之受位，可以尊身而润屋者，则迂道而求之。至公之道没，而奸诈之萌生。天下不治，正为此矣。今圣上中兴，思去浇伪．在朝之士，当念战事之艰难。才堪者审己而当，不堪者收短而避。使天官不妄加，王爵不虚受，则淳素之风庶几可返。 这段话源于《周书·文帝纪下》，虽过于理想，没有真正给出实现的步骤，但仍然值得称赞。常常见到今天的创业者，仅有一点点成绩便招聘大量的员工，动则以手下的人数作为人生成功的衡量，实在好笑。 ##第二十一章“宗教思想之弥漫” 第一节：“古代宗教之演变” 古宗教以上帝、天子、民众为三位一体；儒家则以个人、大群与天为三位一体。墨家并不注重个人，只以大群与天合体。道家则以个人径自与天合体而不主有群；故于历史文化皆主倒演，即返到原始的无群状态。阴阳学家的缺点，第一在由儒家之偏重人道观又折返古代之偏重天道观；第二在由儒家之正面的、积极的观念里，又参杂进许多道家的反面的、消极的观念，因此遂有神仙思想之混入。 古代早期的宗教中，神创造一切也决定一切因果，所以人的生老病死、人生际遇、选择决定都是宿命决定的，普通人并不祈求神来给自己带来好运气；儒教认为天命不可期，而人的个人选择并非没有意义，并且广义的“孝”是社会的根基和组织形势；墨家发于底层社会，所以认为人人平等，而在一个群体中不应该突出个人；道家反对群体的组织形势，只重个人。阴阳家发于儒家，却把人性理解为天道，重新将人的选择归于宿命，相当于开了历史的倒车。 正是儒家对人性的重视使中华文明里多了许多人的成分。 所谓“鱼相望于江湖”，理想的社会，正如江湖然，使群鱼各得独自游行之乐，而无丝毫拘碍束缚。 这是道家理想中的社会形态，但这样哪里还有什么“社会”？社会本来就是人与人相互作用形成的一种群体，又怎么可能只是“相望”而已呢？ 古代一种严肃的、超个人的宗教观念，遂渐渐为一种个人的、私生活的乐利主义所混淆。 古代的宗教重群体，而近代宗教则重个人。 孔子指出人心中的一点之“仁”，来为此种共性画龙点睛。 为什么个人应该为群体的利益而付出？正是因为人性是相通的，而人性共通的部分则为“仁”。“老吾老、幼吾幼”、“己所不欲勿施于人”即个人推己及人的思想。如果没有这种理论支持，人是无法形成稳定的群体的。 第二节：“东汉以下之道教与方术” 东汉一方面是王纲之解纽，又一方面则是古人一种积极的全体观念之消失。相应于乱世而起者，乃个人之私期求，方术权力之迷信，于是后世之所谓道教，遂渐渐在下层社会流行。 政治腐败，则精英阶层无法通过社会晋升阶梯走上舞台，无所施展抱负，不能入世则只能出世，于是群体观念崩塌。 此“道教”非彼“道家”，所谓“方、术、道三名同义”，这里的道教是指个人通过特定方式与鬼神沟通从而获得有利于自己的好处；但从观念上，道教又是道家的延续，都强调只重个人而忽视群体。 名士世族在不安宁的大世界中，过着他们私人安宁的小世界生活，他们需要一种学理上的解释与慰藉。 正如如今之中东国家，人民无从改变自己混乱的生活，只能退回宗教，而且只能走向宗教里最为极端的道路。 第三节：“魏晋南北朝时代之佛教”","categories":[{"name":"Book Reading","slug":"Book-Reading","permalink":"liqul.github.io/blog/categories/Book-Reading/"}],"tags":[{"name":"国史大纲","slug":"国史大纲","permalink":"liqul.github.io/blog/tags/国史大纲/"}]},{"title":"PHP项目的Git pre-commit hook","slug":"git-precommit-hook","date":"2016-01-26T06:20:00.000Z","updated":"2018-03-06T10:47:02.000Z","comments":true,"path":"git-precommit-hook/","link":"","permalink":"liqul.github.io/blog/git-precommit-hook/","excerpt":"","text":"将此脚本放在.git/hook/pre-commit就行。实际上git支持多种hooks，根据需要可以在各个阶段自动完成一些任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bash# Author: Remigijus Jarmalavičius &lt;remigijus@jarmalavicius.lt&gt; # Author: Vytautas Povilaitis &lt;php-checker@vytux.lt&gt;## XDebug check added by William Clemens &lt;http://github.com/wesclemens&gt;ROOT_DIR=&quot;$(pwd)/&quot;LIST=$(git diff-index --cached --name-only --diff-filter=ACMR HEAD)ERRORS_BUFFER=&quot;&quot;for file in $LISTdo EXTENSION=$(echo &quot;$file&quot; | grep &quot;.php$&quot;) if [ &quot;$EXTENSION&quot; != &quot;&quot; ]; then ERRORS=$(php -l $ROOT_DIR$file 2&gt;&amp;1 | grep &quot;Parse error&quot;) if [ &quot;$ERRORS&quot; != &quot;&quot; ]; then if [ &quot;$ERRORS_BUFFER&quot; != &quot;&quot; ]; then ERRORS_BUFFER=&quot;$ERRORS_BUFFER\\n$ERRORS&quot; else ERRORS_BUFFER=&quot;$ERRORS&quot; fi echo &quot;Syntax errors found in file: $file &quot; fi # Check for xdebug statments ERRORS=$(grep -nH xdebug_ $ROOT_DIR$file | \\ sed -e &apos;s/^/Found XDebug Statment : /&apos;) if [ &quot;$ERRORS&quot; != &quot;&quot; ]; then if [ &quot;$ERRORS_BUFFER&quot; != &quot;&quot; ]; then ERRORS_BUFFER=&quot;$ERRORS_BUFFER\\n$ERRORS&quot; else ERRORS_BUFFER=&quot;$ERRORS&quot; fi fi fidoneif [ &quot;$ERRORS_BUFFER&quot; != &quot;&quot; ]; then echo echo &quot;Found PHP parse errors: &quot; echo -e $ERRORS_BUFFER echo echo &quot;PHP parse errors found. Fix errors and commit again.&quot; exit 1else echo &quot;No PHP parse errors found. Committed successfully.&quot;fi","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"git","slug":"git","permalink":"liqul.github.io/blog/tags/git/"}]},{"title":"Useful Links","slug":"useful-links","date":"2016-01-26T02:55:00.000Z","updated":"2018-03-06T10:49:25.000Z","comments":true,"path":"useful-links/","link":"","permalink":"liqul.github.io/blog/useful-links/","excerpt":"","text":"下面按照分类列举一些非常有用的信息。 PHP相关 PHP curl的使用。 PHP代码静态分析工具讨论。 算法相关 任务分配算法Munkres’s Algorithm。 Jekyll相关 CentOS上升级Ruby。 Git相关 CentOS上升级Git。 tlassian给出的非常好的Git入门文档。 分布式系统 有关Log基本原理的经典好文，理解Kafka必读。 计算机网络 Raj Jain的视频资源","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"links","slug":"links","permalink":"liqul.github.io/blog/tags/links/"}]},{"title":"有用的Git相关汇总","slug":"git-recipe","date":"2016-01-22T02:55:00.000Z","updated":"2018-03-06T10:47:10.000Z","comments":true,"path":"git-recipe/","link":"","permalink":"liqul.github.io/blog/git-recipe/","excerpt":"","text":"平时收集的一些有用的git命令（持续更新）： git reset –hard: 去掉所有改动，比如某些文件被改动了，用这招可以将它们都干掉，主要用于处理刚刚接手的杂乱版本，后面跟上commit号就会跳到对应的版本。 git diff –diff-filter=D –name-only -z &brvbar; xargs -0 git rm 批量删除所有被delete的文件 git branch –merged master &brvbar; grep -v master &brvbar; xargs git branch -d 删除所有已经与master合并的分支，用于清理中间的一个本地git库 git clean -fd: 干掉所有untracked file and folder，上面这招不会去掉新添加的文件或文件夹，靠这招更加彻底清除。 git branch -D: 干掉一个分支，主要用于去掉那些已经没有意义的分支，如fix某个bug的分支。 git checkout – ：去掉被add了的文件，用.表示去掉所有被add的文件。 git checkout -b : 从当前分支分出一个新分支，主要用于创建一个feature或bug分支。 git diff &lt;commit id1/reflog id&gt; &lt;commit id2/reflog id&gt; : 查看某个文件两个不同版本间的差别。 git log ：查看某个文件的历史记录，如果没则表示整个branch的历史记录，加–color表示生成一个树状图，-p表示查看修改详情。 git fetch会同步git库和本地库的版本信息，而不会真正下载代码。 git checkout version_number /path/to/file 将某个文件切换到指定的版本。 git stash用于将当前修改缓存下来，方便切到其它分支工作；完成工作后切回来用git stash pop。 git 回滚远程代码库 步骤一：本地先回滚到正确的版本。用git reset –hard version_number version_number可以是commit号或reflog编号。 步骤二：将本地版本强制推送到远端，覆盖远程版本。用git push origin master -f 步骤三：将线上服务器同步到git代码库。直接用git pull origin master是不行的，因为这时候往往线上服务器比代码库更新。所以需要执行下面的命令来强制同步： git fetch –all git reset –hard origin/master 这样就同步好了。基本原理可以参考http://stackoverflow.com/questions/1125968/force-git-to-overwrite-local-files-on-pull。这样做比直接删代码风险要小一些。 git log -p –name-only 其中的–name-only是个非常好用的参数，可以用来只显示改动的文件，可以与其它很多命令搭配起来用。 git init –bare用于在server端新建一个repo，随后在本地代码文件夹下git init再git add remote对应的地址就可以将本地与server关联上了。 git checkout -b –orphan branchname 从当前分支产生一个无历史的新分支，可以用来分享代码。 git config –global push.default current 设置全局push默认的分支为当前分支，执行后可以通过git push来推送当前分支，而不需要显式写出分知名。 git reflog 查看本地的git命令执行历史。 git fetch 同步本地和远程的分支信息，尤其可以消除“Your branch is ahead of ‘origin/master’ by 1 commit.”这样的提醒。 git mv old_name new_name 用于重命名一个文件。免去rm+add的重复劳动。 git branch -a 查看远程分支。 git push origin –delete branch_name 根据分支名删除远程分支。 git push origin –delete tag tag_name 根据tag删除远程分支。 git checkout mybranch; git rebase origin 把自己的分支以patch的形式加到origin分支上，与pull或merge产生的效果类似，但“看起来“历史不同，参考http://gitbook.liuhui998.com/4_2.html。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"git","slug":"git","permalink":"liqul.github.io/blog/tags/git/"}]},{"title":"MySQL安装部署","slug":"mysql-deployment","date":"2015-10-14T23:52:21.000Z","updated":"2018-03-06T10:48:47.000Z","comments":true,"path":"mysql-deployment/","link":"","permalink":"liqul.github.io/blog/mysql-deployment/","excerpt":"","text":"参考http://dev.mysql.com/doc/refman/5.6/en/installing-source-distribution.html。 安装前配置 groupadd mysql useradd -r -g mysql mysql yum install cmake 安装tar zxvf mysql-VERSION.tar.gzcd mysql-VERSIONcmake .makemake install 安装后配置 cd /usr/local/mysql chown -R mysql . chgrp -R mysql . # df -ha 用于查看各个分区的使用率，在处理之前确保有足够存储空间 bin/mysql_install_db --datadir=/var/lib/mysql --user=mysql chown -R root . chown -R mysql data bin/mysqld_safe --user=mysql &amp; mysqladmin -u root -h host_name password newpassword # 启动服务 /usr/local/mysql/bin/mysqld_safe --user=mysql &; # my.cnf在/etc目录下 设置用户#创建一个新用户，host可以用%替换，表示从任意ip地址登陆CREATE USER ‘username‘@’host’ IDENTIFIED BY ‘password’; #查看用户信息SELECT user, host, password FROM user WHERE user=’username’; #分配权限，primitive包括select, insert, update, delete, create, drop, index, alter, grant, reference, reload, shutdown, process, file共14个权限GRANT primitive1, primitive2, …, primitiven ON db.table TO username@host IDENTIFIED BY password #删除用户DROP USER username; #取消用户授权REVOKE primitive ON db.table FROM username@host; 备份下面的命令里都省略了-u和-p，在实际操作时须补全。关于bin log的部分，文档《binlog基本认识》有非常详尽的介绍。开启binlog编辑my.cnf文件，在[mysqld]区块中加入log-bin=mysql-bin，重启服务即生效执行show variables like ‘log_%’;来确认已经打开bin log 常用的bin log操作show master logs; # 查看所有bin log日志列表show master status; # 查看最新一个bin log日志编号名称，以及最后一个操作事件结束点flush logs; # 刷新日志，产生一个新的bin log日志文件reset master; # 清空所有bin log日志 查看bin logmysqlbinlog mysql-bin.000013 #打开一个bin log文件show binlog events [IN log_name] [FROM pos] [LIMIT [offset,] row_count]例如指定查询 mysql-bin.000021 这个文件，从pos点:8224开始查起，偏移2行，查询10条 show binlog events in ‘mysql-bin.000021’ from 8224 limit 2,10\\G; 利用bin log恢复数据mysqlbinlog mysql-bin.0000xx | mysql 备份整个数据库，备份前需要获取全局读锁，因此必须在traffic较低的时候进行mysqldump –single-transaction –all-databases &gt; backup_yy_mm_dd_hh_mm_ss.sqlmysql &lt; backup_yy_mm_dd_hh_mm_ss.sql 主从同步 完整的说明在这里：http://dev.mysql.com/doc/refman/5.6/en/replication-howto.html。 另一篇文章是个不错的简化版，并且更加实用“Setting up MySQL replication without the downtime”。 ## 设置主服务器 # 确保master已经通过log-bin来开启bin log # 确保每个host对应的server-id是不同的 [mysqld] log-bin=mysql-bin server-id=1 innodb_flush_log_at_trx_commit=1 sync_binlog=1 # 为slave创建专门的用户名 create user 'repl'@'%.mydomain.com' identified by 'slavepass'; grant replication slave on *.* to 'repl'@'%.mydomain.com'; ## 设置从服务器 # 确保slave的server-id与其它服务器不同 [mysqld] server-id=2 ## 获取master的同步位置 # 执行flush操作，开启一个client session，执行下面语句后保持不退出 flush tables with read lock; # 在另一个client session执行，记录File、Position对应的值 show master status; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 73 | test | manual,mysql | +------------------+----------+--------------+------------------+ # 回到锁住的session执行 unlock tables; ## 创建master的镜像 mysqldump --all-database --master-data > dbdump.db # 如果需要选择性同步，需要在slave上配置--replicate-ignore-db=db_name来完成，详细使用方法参考官方文档 ## 在slave导入镜像 #带--skip-slave-start选项启动slave上的mysql #导入dump文件 mysql < dbdump.db #在slave执行change master to指令 CHANGE MASTER TO MASTER_HOST='master_host_name', MASTER_USER='replication_user_name', MASTER_PASSWORD='replication_password', MASTER_LOG_FILE='recorded_log_file_name', MASTER_LOG_POS=recorded_log_position; start slave; 配置实例 注意：配置MySQL的目的不是有什么秘诀，关键在于根据系统本身的配置高低和处理的业务类型来调整各种资源配置 这里有根据不同业务量级给出的实例，https://github.com/onddo/mysql_tuning-cookbook。 可以从这里出发，按照步骤来配置https://tools.percona.com/wizard 执行一下这个工具来获得优化建议https://github.com/major/MySQLTuner-perl","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"liqul.github.io/blog/tags/mysql/"}]},{"title":"Nginx+PHP安装部署","slug":"php-deployment","date":"2015-10-13T23:52:21.000Z","updated":"2018-03-06T10:48:51.000Z","comments":true,"path":"php-deployment/","link":"","permalink":"liqul.github.io/blog/php-deployment/","excerpt":"","text":"这里记录安装部署Nginx+PHP相关的一些东西。 环境+依赖12345678910111213141516171819202122232425262728# 基本编译环境yum -y install gcc automake autoconf libtool make gcc-c++ glibc# PHP依赖yum -y install libmcrypt-devel mhash-devel libxslt-devel libjpeg \\libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 \\libxml2-devel zlib zlib-devel glibc glibc-devel glib2 glib2-devel \\bzip2 bzip2-devel ncurses ncurses-devel curl curl-devel e2fsprogs \\e2fsprogs-devel krb5 krb5-devel libidn libidn-devel openssl openssl-devel# 安装pcre:cd /usr/local/srcwget ftp://ftp.csx.cam.ac.uk/pub/software/programming/pcre/pcre-xx.xx.tar.gz tar -zxvf pcre-xx.xx.tar.gzcd pcre-xx.xx./configuremakemake install# 安装zlib:cd /usr/local/srcwget http://zlib.net/zlib-x.x.x.tar.gztar -zxvf zlib-x.x.x.tar.gzcd zlib-x.x.x./configuremakemake install# 安装ssl:cd /usr/local/srcwget http://www.openssl.org/source/openssl-x.x.x.tar.gztar -zxvf openssl-x.x.x.tar.gz 安装PHP安装这里可能遇到各种缺少依赖的错误，遇到什么就安装什么就行，可以参考文章“20+ common PHP compilation errors and fix”。这里的配置项比较多，也很难对每一项都非常了解，所以其实可以设置少一些选项，然后等真的需要的时候再重新编译。这其实非常容易，重新安装一遍后重启php-fpm就可以了 12345678910111213141516171819202122232425262728293031323334353637383940wget http://cn2.php.net/distributions/php-xx.xx.xx.tar.gztar zvxf php-xx.xx.xx.tar.gzcd php-xx.xx.xx.tar.gz./configure --prefix=/usr/local/php \\--enable-fpm \\--with-fpm-user=www-data \\--with-fpm-group=www-data \\--with-mcrypt \\--enable-mbstring \\ --disable-pdo \\--with-curl \\--disable-debug \\--disable-rpath \\--enable-inline-optimization \\--with-bz2 \\--with-zlib \\--enable-sockets \\--enable-sysvsem \\--enable-sysvshm \\--enable-sysvmsg \\--enable-pcntl \\--enable-mbregex \\ --with-mhash \\--enable-zip \\--with-pcre-regex \\--with-mysql \\--with-mysqli \\--with-gd \\--with-jpeg-dir \\--with-openssl \\--enable-pdo \\--with-pdo-mysql \\--with-mysql-sock=/var/lib/mysql/mysql.sock \\--with-config-file-path=/usr/local/php/etcmake all install # 把php.ini放到指定的位置cp php.ini-development（根据需要换成其它） /usr/local/php/etc/php.ini# 配置php.ini中的timezone参数为Asia/Shanghai 配置1234567891011121314cd /usr/local/phpcp etc/php-fpm.conf.defautl etc/php-fpm.confvi etc/php-fpm.conf# 修改user = www-datagroup = www-data# 如果www-data用户不存在则添加www-data用户：groupadd www-data &amp;&amp; useradd -g www-data www-data# 添加php到环境变量vi /etc/profile# 在底部增加一行export PATH=/usr/local/php/bin:$PATHsource /etc/profile# 检查php cli配置文件php --ini 若输出路径与--with-config-file-path对应的路径不同，需要建一个软链 启动/重启123#启动：/usr/local/php/sbin/php-fpm#关闭：kill -INT pid#重启：kill -USR2 pid php.ini的问题偶然发现一个问题，phpinfo()里输出的配置文件路径与php –ini的路径不同。而实际上在编译PHP的时候明确设定了–with-config-file-path参数，设置与phpinfo()输出相同，与php –ini不同。这与文档中描述的有点不一致。 搜索了一把，发现phpinfo()实际对应php/bin/php-cgi，而运行php –ini的时候对应的是php/bin/php，因而两者调用的实际上是不同的可执行文件。只能理解，在我使用的版本里，cli对应的php.ini与cgi的不同。解决方法最简单的则是在php –ini里显示的配置文件路径上放一个php.ini软链，这就好了。 Composer的应用Composer对于框架的各个组件来说，就好像Container对于各种类一样。如果用一句话来概括Composer的作用，那应该是： Composer is a tool for dependency management in PHP. 从实际应用的角度，Composer的作用体现在两个方面： 安装和维护项目依赖的第三方库 支持项目内部autoload 用Composer安装第三方库在Composer的支持下，安装和使用第三方库非常方便。大概步骤包括（1）更改composer.json；（2）执行composer install下载和安装库；（3）在项目的入口中引用vendor目录下的autoload.php。一个最简单的composer.json看起来是这样的： { “require”: { “monolog/monolog”: “1.0.*” }}它定义了项目依赖的第三方库。 用Composer支持autoload虽然autoload.php位于vendor目录下，但它实际上也支持项目自身的autoload。Composer支持4种autoload方式，分别是psr-0、psr-4、classmap和files。其中psr-0/psr-4属于按规则autoload，而classmap和files则属于按配置autoload。所谓“按规则”是指：只要文件命名、文件路径、类命名符合一定规则，autoloader就能根据类的命名空间和类名加载对应的文件。先举个psr-4的例子： $obj = new App\\MyLib\\TestComposer();假设composer.json像下面这样： { “autoload”: { “psr-4”: { “App\\“: “src/“, } }}那么对应的文件将是src/MyLib/TestComposer.php里的TestComposer类。如果上面的规则是psr-0，那么对应的文件则是src/App/MyLib/TestComposer.php。可以看到两者的差别其实很小，psr-4里是把左边（App\\）替换成右边（src/）。psr-0则是把左边连接到右边后面。因此，psr-4的一个主要目的是简化项目的目录结构。 另外两种方式classmap和files比较容易理解，如下面的例子： { “autoload”: { “classmap”: [“src/MyLib/“, “TestComposer.php”], “files”: [“src/MyLib/functions.php”] }}classmap针对文件里定义的class，可以配置整个目录或某个文件。files则用于加载非class的内容，例如函数实现。 最后，增加了新的文件或修改了composer.json还需要执行 composer dump-autoload来重新生成autoload逻辑。对于psr-0/psr-4的还可以加-o来优化autoload速度。 安装Nginx安装1234567891011cd /usr/local/srcwget http://nginx.org/download/nginx-x.x.x.tar.gztar -zxvf nginx-x.x.x.tar.gzcd nginx-x.x.x./configure --sbin-path=/usr/local/nginx/nginx \\--conf-path=/usr/local/nginx/nginx.conf \\--pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module \\--with-pcre=/usr/local/src/pcre-xx.xx --with-zlib=/usr/local/src/zlib-x.x.x \\--with-openssl=/usr/local/src/openssl-x.x.xmakemake install 启动1sudo /usr/local/nginx/nginx 配置实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667user www-data;worker_processes 1;pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; &quot;$request_time&quot;&apos;; #[important] need to specify the index index index.php index.html index.htm; access_log logs/access.log main; error_log logs/error.log; sendfile on; tcp_nopush on; keepalive_timeout 65; #若有多个server，建议分为多个文件，每个包含一个server模块，用include包含 server &#123; listen 80; server_name example.com; #access_log logs/host.access.log main; #[important] need to make sure each subpath in the root have +x permission root /home/work/laravel/public; #[important] need to do the rewrite here location / &#123; try_files $uri $uri/ /index.php?$query_string; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #[important] need to have the * below location ~* \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # location ~ /\\.ht &#123; deny all; &#125; &#125;&#125; 维护12345# 把下面的命令放在脚本里用于备份配置文件cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.$(date &quot;+%b_%d_%Y_%H.%M.%S&quot;)#重新加载conf：#测试：sudo /usr/local/nginx/nginx -t#重载：sudo /usr/local/nginx/nginx -s reload","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"php","slug":"php","permalink":"liqul.github.io/blog/tags/php/"},{"name":"nginx","slug":"nginx","permalink":"liqul.github.io/blog/tags/nginx/"}]},{"title":"基于惯性传感器的姿态恢复","slug":"imu","date":"2015-09-22T23:52:21.000Z","updated":"2018-03-06T10:47:16.000Z","comments":true,"path":"imu/","link":"","permalink":"liqul.github.io/blog/imu/","excerpt":"","text":"一个物体在空间中的存在可以从两个维度描述：一方面是在什么位置，这可以由其所在坐标$${x,y,z}$$来描述；另一方面是处于什么姿态（orientation)，例如它是倾斜的还是正的。要知道一个物体在空间的位置是困难的，需要借助外在测量系统，例如GPS利用24颗卫星定位一个物体在地球上的经纬度和海拔。即便有GPS，要知道物体在室内的位置还是不行，需要相应的室内定位技术。扯远了，室内定位以后再说。要知道一个物体的姿态，相对容易一些，这可以借助惯性传感器（inertial sensor）。 惯性传感器通常指代下面三个具体的传感器： 加速度计（accelerometer）：测量三轴加速度，单位$$m/s^2$$； 陀螺仪 （gyroscope）：测量绕三轴旋转的角速度，单位是$$degree/s$$； 磁力计（magnetometer）：测量三轴磁力的大小，单位是$$tesla$$； 惯性传感器的应用非常广泛，几乎遍及所有的智能硬件设备（如手机、手环等）。它能用于姿势识别，如apple watch自动识别用户是否正在看表；zepp捕捉网球、棒球姿势辅助训练；手环识别走路、跑步、骑车等等。这些看似神奇的应用都基于惯性传感器，以后有机会再深入讨论这些算法。 怎么才能测量一个物体的姿态呢？首先我们必须假定该物体与惯性传感器之间是刚体连接的，这样通过读取惯性传感器的值就能推测物体的姿态。加速度计可以用来测量物体的倾斜。如果物体处于静止状态，因为我们知道重力的方向总是向下的，这是非常容易做到的。如果物体静止，测量到的加速度方向就是重力的方向。此外，陀螺仪也可以用来计算物体的姿态。假定已知物体的初始姿态，那么我们将物体沿三轴旋转的角速度积分，就知道物体的姿态了。 但实际情况比这复杂。物体可能处于运动状态，而陀螺仪读数有严重的漂移，使得姿态估计并不容易。现有的主流思路是结合加速度计和陀螺仪，根据两者的读数分别估算物体姿态，然后依据某种方法将两方面融合起来。其中的算法细节，没有比这篇文章讲得更清楚的了，感兴趣的话可以好好读一遍（A practical approach to Kalman filter and how to implement it），中文翻译）。 Demo采用Arduino和SparkFun 9Dof Sensor Stick。连接和代码借鉴这篇文章，讲得比较详细，照着做一遍就行了，代码参考这里。在上面代码的accelerometer_norm和gyro_comp需要用到两个数组，需要在校准中获得。 传感器得到的原始读数无法直接使用。例如加速度计在水平静止时的读数可能是$$ x=0, y=0, z=-211$$，这里的211其实是g的大小，而我们需要把这个值与g对应起来。对应上面的代码，accelerometer_norm[2]就应该是211。同理，需要找到x轴、y轴与g的对应关系。将传感器的x轴、y轴与水平面垂直静止放置（即分别使x轴、y轴朝上），得到的读数分别对应accelerometer_norm[0]和accelerometer_norm[1]。陀螺仪的校准与加速度计不同。陀螺仪的问题在于即使处于静止状态，各个轴角速度一般都不为0，这是明显错误的。所以简单的做法是把传感器静止放置，记录陀螺仪各个轴角速度测量值，例如$$ x=5, y=30, z=-10$$，那么对应的gyro_comp[3] = {-5, -30, 10}。需要注意的是，这里的校准是非常粗糙的，也并不能完全消除误差，而实际的误差模型非常复杂，如果需要得到更加精确的测量结果还需要更加复杂的校准过程。 姿态恢复采用开源项目。这个项目本来是针对xio自产的传感器设备，为了接入自己的设备需要做一些修改。只需要修改program.cs。 建议安装Arduino IDE则不需要安装额外的驱动了。 改变传感器的姿态可以从Visual Studio执行的项目中看到下面的画面：","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"IMU","slug":"IMU","permalink":"liqul.github.io/blog/tags/IMU/"},{"name":"accelerometer","slug":"accelerometer","permalink":"liqul.github.io/blog/tags/accelerometer/"},{"name":"gyroscope","slug":"gyroscope","permalink":"liqul.github.io/blog/tags/gyroscope/"},{"name":"compass","slug":"compass","permalink":"liqul.github.io/blog/tags/compass/"}]},{"title":"RabbitMQ报错[ERROR: node with name 'rabbit' already running on 'localhost']","slug":"rabbitmq-error1","date":"2015-09-07T10:52:21.000Z","updated":"2018-03-06T10:49:02.000Z","comments":true,"path":"rabbitmq-error1/","link":"","permalink":"liqul.github.io/blog/rabbitmq-error1/","excerpt":"","text":"这个报错是在执行./rabbitmq-server -detached的时候报出来的，除了这句以外没有任何信息。这个报错的本意是rabbit@localhost已经在运行了，不允许再启动一个实例。于是尝试干掉已有的进程： ps aux | grep rabbit执行后发现没有任何进程。尝试Google类似的问题，找了很久终于发现一个可能的问题——/etc/hosts文件可能被改出问题了。 这要回到RabbitMQ的一个基本认识上，它是通过hostname来唯一确定本机的，甚至包括队列存储都与hostname直接对应，如果hostname被改了，意味着实例本身都变化了。于是执行 hostname -s发现居然没问题，还是localhost。所以，的确感觉非常奇怪。 进一步检查/etc/hosts文件，发现无意间把localhost指向了局域网内的另一台机器： 192.168.1.56 localhost于是终于找到原因了，RabbitMQ启动时检查localhost，发现上面已经启动了一个RabbitMQ实例，于是报了最前面的错误。只需要去掉hosts文件里的配置就好了。 上面说的是主要原因，另一个意外操作导致这个错误的原因是erlang的守护进程被其它用户启动了，这可以通过： ps aux | grep epmd检查。需要干掉这个进程才能顺利把RabbitMQ启动起来。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"liqul.github.io/blog/tags/rabbitmq/"}]},{"title":"Dependency Injection","slug":"dependency-injection","date":"2015-08-17T23:52:21.000Z","updated":"2018-03-06T10:46:54.000Z","comments":true,"path":"dependency-injection/","link":"","permalink":"liqul.github.io/blog/dependency-injection/","excerpt":"","text":"面向对象的一条准则就是将类与类之间的依赖关系解耦，这并不是什么新概念。Dependency Injection（DI）的目的也是一样，引用《implementing laravel》的一句原文： Dependency Injection is the act of adding (injecting) any dependencies into a class, rather than instantiating them somewhere within the class code itself. 举一个例子来区分这两种情况： class NoDI { function __construct() { $this->inner_class = new InnerClass(); } } class DI{ function __construct(SomeInterface $implementation) { $this-&gt;inner_class = $implementation; }}上面前一种情况下，如果inner_class的实现发生变化，例如需要重写一个NewInnerClass()，那么需要在NoDI中修改这行代码。而后一种情况下，由于inner_class是从外部传入的，所以DI类本身不需要发生任何变化。 这个道理并不新颖，所以乍一眼看来这没有什么有意思的东西。如果无论什么情况都需要为类额外写一个interface，反倒是一种负担。另外，如果类之间的依赖关系比较复杂，使用起来也不方便，例如：类A依赖类B，类B又依赖类C，那么如果要获取一个A的对象，需要先得到B的对象，进一步递推到C的对象。 为了减少这种重复的实例化过程，Laravel提供了Container。实际上一个Laravel的APP对象本身就继承自Container。还是以A、B、C之间的依赖关系为例： interface B_Interface {}interface C_Interface {} class A { function __construct(B_Interface $b){}} class B implements B_Interface { function __construct(C_Interface $c){}} class C implements C_Interface { function __construct(){}}在没有Container的情况下，会像下面一样来实例化A对象： $c = new C();$b = new B($c);$a = new A($b);试想我们每次获取A对象都需要写这样的3行代码。在面向对象的开发中，类之间的依赖关系是非常复杂的，这使得这样实例化变得不可行。这或许是许多人更加倾向于面向过程来开发（即使他们的代码里也充满了类定义）。 有了Container以后这样的情况得到了很大的改善。如下面所示： APP::bind(‘B_Interface’, ‘B’); APP::bind(‘C_Interface’, function(){ return new C();}); $a = APP::make(‘A’);在Laravel中，上面的代码会这样执行： 尝试实例化类A，发现其依赖B_Interface 尝试找到实现B_Interface的类，发现已注册的B_Interface指向B类 尝试实例化类B，发现其依赖C_Interface 尝试找到实现C_Interface的类，发现已注册的C_Interface执行一个closure，返回一个C类对象 按照上面相反的顺序依次获得C、B、A对象Laravel自动完成上面这样的递归式实例化过程。在PHP中，只要利用反射就能做到这一点。也许有人会说，上面这样的代码岂止3行，比前面的实现更加麻烦了。但是，在真实的场景下，两个bind语句是由服务提供方来实现的，而且只要做一次，在全局都能使用。所以，其实只需要最后一句话就够了。 上面的例子里，生成实现B_Interface的类的时候用的是： APP::bind(‘B_Interface’, ‘B’);但对于C_Interface，用的是closure。这样做是由于B依赖于C_Interface，如果也用closure的话就不方便写了。 Container在Laravel处于绝对核心地位。可以说所有类实例化都是经过Container来完成的，上面举的例子其实只是一种用法而已，在使用Laravel的过程中，由于Container的存在还有非常多技巧。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"php","slug":"php","permalink":"liqul.github.io/blog/tags/php/"}]},{"title":"RabbitMQ负载均衡","slug":"rabbitmq-load-balancing","date":"2015-08-15T10:52:21.000Z","updated":"2018-03-06T10:49:08.000Z","comments":true,"path":"rabbitmq-load-balancing/","link":"","permalink":"liqul.github.io/blog/rabbitmq-load-balancing/","excerpt":"","text":"读RabbitMQ文档的时候奇怪为什么它不支持负载均衡。后来看过别人写的Haproxy+RabbitMQ实现负载均衡，觉得似乎这样做就完美了，直到读到这篇文章《Load Balancing a RabbitMQ Cluster》。 这篇文章里介绍了RabbitMQ在队列备份时的一些细节：假设一个cluster里有两个实例，记作rabbitA和rabbitB。如果某个队列在rabbitA上创建，随后在rabbitB上镜像备份，那么rabbitA上的队列称为该队列的主队列（master queue），其它备份均为从队列。接下来，无论client访问rabbitA或rabbitB，最终消费的队列都是主队列。换句话说，即使在连接时主动连接rabbitB，RabbitMQ的cluster会自动把连接转向rabbitA。当且仅当rabbitA服务down掉以后，在剩余的从队列中再选举一个作为继任的主队列。 如果这种机制是真的，那么负载均衡就不能简单随机化连接就能做到了。需要满足下面的条件： 队列本身的建立需要随机化，即将队列分布于各个服务器； client访问需要知道每个队列的主队列保存在哪个服务器； 如果某个服务器down了，需要知道哪个从队列被选择成为继任的主队列。于是，Load Balancing a RabbitMQ Cluster的作者给出了下图的结构。 这还是颇有点复杂的。首先，在建立一个新队列的时候，Randomiser会随机选择一个服务器，这样能够保证队列均匀分散在各个服务器（这里暂且不考虑负载）。建立队列后需要在Meta data里记录这个队列对应的服务器；另外，Monitor Service是关键，它用于处理某个服务器down掉的情况。一旦发生down机，它需要为之前主队列在该服务器的队列重新建立起与服务器的映射关系。 这里会遇到一个问题，即怎么判断某个队列的主队列呢？一个方法是通过rabbitmqctl，如下面的例子： ./rabbitmqctl -p production list_queues pid slave_pids registration-email-queue &lt;rabbit@mq01.2.1076.0&gt; [&lt;rabbit@mq00.1.285.0&gt;] registration-sms-queue &lt;rabbit@mq01.2.1067.0&gt; [&lt;rabbit@mq00.1.281.0&gt;] 可以看到pid和slave_pids分别对应主队列所在的服务器和从服务器（可能有多个）。利用这个命令就可以了解每个队列所在的主服务器了。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"liqul.github.io/blog/tags/rabbitmq/"}]},{"title":"RabbitMQ内存管理","slug":"rabbitmq-memory-management","date":"2015-08-15T10:52:21.000Z","updated":"2018-03-06T10:49:13.000Z","comments":true,"path":"rabbitmq-memory-management/","link":"","permalink":"liqul.github.io/blog/rabbitmq-memory-management/","excerpt":"","text":"之前一直有个疑问：如果一个exchange关联n个queue，那么一份消息会不会需要拷贝n份呢？这个问题十分关键。如果真的会有n个拷贝，那么queue的数量就必须严格限制了。庆幸的是RabbitMQ官方文档的确完整，于是看摘抄如下（完整文章在此）： A bit of backgroundFirst of all we need to understand how Erlang manages memory. Erlang is a bit different from most garbage-collected languages in that it does not have a global heap. Instead, each process has a separate heap that’s private to it. In RabbitMQ terms process might be queues, channels, connections and so on. This means that the entire system does not have to come to a halt every time garbage collection needs to happen; instead each process collects garbage on its own schedule. That’s fine, but as a message passes through RabbitMQ, it will pass through several different processes. We’d like to avoid doing too much copying as this happens. Therefore Erlang gives us a different memory management scheme for binaries, which are used for a number of things inside RabbitMQ, of which the most interesting is message bodies. Binaries are shared between processes and reference-counted (with references being held by processes and garbage-collected along with everything else). How this applies to RabbitMQThis means that memory used by message bodies is shared among processes in RabbitMQ. And this sharing also happens between queues too: if an exchange routes a message to many queues, the message body is only stored in memory once. 由此可见，RabbitMQ在内存处理上还是考虑周到的，每个消息无论在内存还是disk都只会保留一个拷贝，队列里只保存对消息的引用。因此，在设计系统时，队列数量可以作为一个相对比较廉价的开销。","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"liqul.github.io/blog/tags/rabbitmq/"}]},{"title":"RabbitMQ安装部署","slug":"rabbitmq-deployment","date":"2015-07-25T10:52:21.000Z","updated":"2018-03-06T10:48:56.000Z","comments":true,"path":"rabbitmq-deployment/","link":"","permalink":"liqul.github.io/blog/rabbitmq-deployment/","excerpt":"","text":"安装ErlangRabbitMQ是由Erlang编写的，所以需要先安装Erlang。RabbitMQ官方提供了一个足够RabbitMQ使用的Erlang的rpm，可以这样安装： wget https://www.rabbitmq.com/releases/erlang/erlang-17.4-1.el6.x86_64.rpm rpm -i erlang-17.4-1.el6.x86_64.rpm 安装RabbitMQRabbitMQ的安装非常简单，只需要下载tar包即可。 wget https://github.com/rabbitmq/rabbitmq-server/releases/download/rabbitmq_v3_5_3/rabbitmq-server-generic-unix-3.5.3.tar.gz tar zxvf rabbitmq-server-generic-unix-3.5.3.tar.gz 启动/关闭RabbitMQ服务 sbin/rabbitmq-server -detached sbin/rabbitctl stop #关闭服务 端口配置如果需要建立RabbitMQ集群，需要打开下面的端口：4369 (epmd), 25672 (Erlang distribution)5672 (AMQP 0-9-1 without TLS)一些插件还需要打开额外的端口，暂时不需要。例如rabbitmq-management需要打开15672端口（推荐打开）。 iptables -I INPUT 1 -p tcp –dport 5672 -j ACCEPT iptables -I INPUT 1 -p tcp –dport 4369 -j ACCEPT iptables -I INPUT 1 -p tcp –dport 25672 -j ACCEPT iptables -I INPUT 1 -p tcp –dport 15672 -j ACCEPT /etc/init.d/iptables save /etc/init.d/iptables restart 建立集群官方文档在此https://www.rabbitmq.com/clustering.html 需要在多台机器上部署RabbitMQ，并保证上面提到的端口没有被防火墙盾掉。注意，不需要每台机器去配置vhost、exchange、queue等等，待建立集群后在任意一节点上创建就好了。 将一个节点的/var/lib/rabbitmq/.erlang.cookie（如果该目录无权限则在$HOME下）拷贝到其它节点对应的位置，注意权限要保持一致400（即启动rabbitmq的用户下400）。另外，在每个节点上要保证可以通过短hostname（hostname -s来查看）来找到对应的ip地址，所以需要修通/etc/hosts文件，将所有节点的hostname和ip添加到该文件里。 以任意一个节点为主节点（例如rabbit1），在非主节点节点上执行下面操作： rabbit2$ sbin/rabbitmqctl stop_app Stopping node rabbit@rabbit2 ...done. rabbit2$ sbin/rabbitmqctl join_cluster rabbit@rabbit1 Clustering node rabbit@rabbit2 with [rabbit@rabbit1] ...done. rabbit2$ sbin/rabbitmqctl start_app Starting node rabbit@rabbit2 ...done. 这样rabbit2上的节点就与rabbit1上的节点组成一个cluster。在其它非主节点上依次执行上面的操作即可。 查看集群状态：sbin/rabbitmqctl cluster_status 停止某个节点：在对应节点上执行sbin/rabbitmqctl stop 某个节点退出：在对应节点上执行sbin/rabbitmqctl stop_app；sbin/rabbitmqctl reset；sbin/rabbitmqctl start_app hostname的问题：RabbitMQ直接读取hostname来写数据，如果hostname变了RabbitMQ会重新建立一个新的数据库。 单个节点异常后如何恢复：前提必须保证 /var/lib/rabbitmq/.erlang.cookie一致 rabbitmqctl stop_app；rabbitmqctl update_cluster_nodes rabbit@hostname；rabbitmqctl start_app 用户配置RabbitMQ默认的用户名密码为guest:guest，这个用户只能在localhost环境下使用，不能远程使用。RabbitMQ大部分配置都可以用sbin/rabbitmqctl完成，可以参考https://www.rabbitmq.com/man/rabbitmqctl.1.man.html。 增加用户：sbin/rabbitmqctl add_user user-name this-is-the-password 删除用户：sbin/rabbitmqctl delete_user user-name 更改密码：sbin/rabbitmqctl change_password user-name new-password 列举所有用户：sbin/rabbitmqctl list_users 为用户打tag：sbin/rabbitmqctl set_user_tags production monitoring virtual hostRabbitMQ设计了virtual host这个概念，类似于文件系统的不同路径。一个应用是将同一组服务器给线上、线下使用，这时候可以建立两个不同的virtual host，它们相互不影响。 增加virtual host：sbin/rabbitmqctl add_vhost vhost-path 删除virtual host：sbin/rabbitmqctl delete_vhost vhost-path 列举所有virtual host：sbin/rabbitmqctl list_vhosts 设置用户权限 sbin/rabbitmqctl set_permissions [-p vhostpath] user-name conf-regex write-regex read-regex 这个命令可以指定virtual host路径，跟前面提到的对应。需要注意的是后三个参数均为正则表达式，conf控制配置权限，write控制写权限，read控制读权限。例如： rabbitmqctl set_permissions -p /myvhost tonyg \"^tonyg-.*\" \".*\" \".*\" 这个命令配置用户tonyg只能配置以tongyg-开头的资源，但可以读写所有资源。 清空权限：sbin/rabbitmqctl clear_permissions [-p vhostpath] user-name 列举所有用户权限：sbin/rabbitmqctl list_permissions [-p vhostpath] 列举特定用户权限：sbin/rabbitmqctl list_user_permissions user-name 设置服务规则一个RabbitMQ cluster中所有节点都共享访问user、vhost、queue、exchange等等，其中queue默认只在创建的node上保存，虽然其他节点可以访问这个queue，但如果创建节点down掉就没法访问了。一个解决方案是让队列在多个节点上备份。关于High availability的配置参考https://www.rabbitmq.com/ha.html一个常用的policy是 sbin/rabbitmqctl [-p vhostpath] set_policy ha-all \".*\" '{\"ha-mode\":\"all\"}' 这将所有queue都设置为全节点备份，在小规模的cluster中是可行的。 **文件位置** RabbitMQ默认把所有相关文件都保存在安装目录下，一些常用的目录包括： * sbin目录保存控制服务的可执行脚本； * etc目录保存配置文件； * var目录保存服务日志文件； **PHP client库部署** RabbitMQ有多个PHP client实现，其官方tutorial对应的库是https://github.com/videlalvaro/php-amqplib。php-amqplib是一个完全用PHP实现的库。安装方法见github说明文档。 **维护** RabbitMQ自带rabbitmq_management 插件可以用来监控队列状态。https://www.rabbitmq.com/management.html 打开插件：sbin/rabbitmq-plugins enable rabbitmq_management 关闭插件：sbin/rabbitmq-plugins disable rabbitmq_management","categories":[{"name":"Tech","slug":"Tech","permalink":"liqul.github.io/blog/categories/Tech/"}],"tags":[{"name":"rabbitmq","slug":"rabbitmq","permalink":"liqul.github.io/blog/tags/rabbitmq/"}]}]}