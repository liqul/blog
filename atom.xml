<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Liqun&#39;s Homepage</title>
  
  <subtitle>A place for knowledge</subtitle>
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="liqul.github.io/blog/"/>
  <updated>2018-04-10T13:59:45.230Z</updated>
  <id>liqul.github.io/blog/</id>
  
  <author>
    <name>Liqun Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>阳明先生的哲学</title>
    <link href="liqul.github.io/blog/2018/04/10/2016-12-03-yangming-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2018/04/10/2016-12-03-yangming-NSConflict-liqul/</id>
    <published>2018-04-10T13:59:45.230Z</published>
    <updated>2018-04-10T13:59:45.230Z</updated>
    
    <content type="html"><![CDATA[<p>王守仁，幼名云，字伯安，浙江余姚人。初次接触阳明先生是通过《明朝那些事》，作者当年明月称其为“一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人”。阳明先生对后世的影响应首推其哲学，当时称之为“心学”，可惜《明朝那些事》里讲得更多的是他的事迹，虽然提到“知行合一”但没有解释清楚其内在含义。所以虽然读了《明朝那些事》，阳明先生的思想在我看来还是雾里看花。</p><p>阳明先生虽然在哲学界享有盛名，但其知名度并不高。这是个很奇怪的现象，其原因可能有二：其一，阳明先生似乎并不喜欢著书，最为著名的《传习录》也由其弟子收集整理，类似于《论语》。其实大部头的书籍并不符合他的性格，他崇尚简洁，其倡导的观念归纳起来寥寥数句而已，何须长篇累牍。其二，阳明先生所推崇的思想在当代看来是赤裸裸的唯心主义。在这“唯物主义”的意识形态下，这纯粹是封建社会糟粕，不禁止就不错了，怎会大张旗鼓地宣传？当然，这或许也与某些人十分崇拜他有关吧。</p><p>我对阳明先生的肤浅理解其实仅仅来源于《传习录》。个人拙见，其思想的根源可以归纳为“人心即天地”，《传习录》有：</p><blockquote><p>‘心犹镜也。圣人心如明镜。常人心如昏镜。近世格物之说，如以镜照物，照上用功。不知镜尚昏在，何能照？先生之格物，如磨镜而使之明。磨上用功。明了后亦未尝废照’ </p></blockquote><p>此类语句在《传习录》中还有很多。</p><p>所以，人们真正该用功的地方乃是磨练自己的内心。“知行合一”或“天泉桥话别”为阳明先生解释如何磨练人心的具体方法。其中“知行合一”被许多人认为是阳明先生的思想核心，我认为其原因是：这句话说起来朗朗上口容易引起“共鸣”，没有真正理解其意义的读者往往以为类似“行胜于言”或“言行合一”，其实差之毫厘谬以千里。</p><p>明白了阳明先生的哲学，就清楚人的成长没有捷径。人成长的过程即人心接受历练的过程，不经历诸多繁杂事务决不能“动心忍性增益其所不能”。同时，正由于人心即天地，“磨镜而使之明”就成为了唯一和终极的人生目标，看清这一点，才会明白什么成长过程中遇到的障碍不过“磨镜”的石头而已，应敞开胸怀地迎接，而不是畏首畏尾地逃避。</p><p>2016-05-01：理解了“知行合一”，就明白为什么说评价一个人不要看他说过什么，而要看他做过什么。他说出来的话不一定代表他最真实的想法，而做的事情必然是最符合他的内心的。</p>]]></content>
    
    <summary type="html">
    
      一个高尚的人，一个纯粹的人，一个有道德的人，一个脱离了低级趣味的人，一个有益于人民的人
    
    </summary>
    
      <category term="Thinking" scheme="liqul.github.io/blog/categories/Thinking/"/>
    
    
      <category term="Learning" scheme="liqul.github.io/blog/tags/Learning/"/>
    
  </entry>
  
  <entry>
    <title>Install Octave on OSX EI Capitan 10.11.3</title>
    <link href="liqul.github.io/blog/2018/04/10/2016-12-03-install-octave-on-osx-ei-capitan-10113-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2018/04/10/2016-12-03-install-octave-on-osx-ei-capitan-10113-NSConflict-liqul/</id>
    <published>2018-04-10T13:59:45.224Z</published>
    <updated>2018-04-10T13:59:45.225Z</updated>
    
    <content type="html"><![CDATA[<p>Installing Octave on the latest OSX can be error prone. One may download the Octave-Forge bundle from sourceforge <a href="https://sourceforge.net/projects/octave/files/Octave%20MacOSX%20Binary/2013-12-30%20binary%20installer%20of%20Octave%203.8.0%20for%20OSX%2010.9.1%20%28beta%29/" target="_blank" rel="noopener">here</a> which however is large and also a bit out of date. </p><!--break--><p>Alternatively, if you use homebrew to do this, please be ware of<br>a few traps. Here are the steps I adopted to successfully install Octave.<br>If you do not have Xcode or Homebrew installed yet, please refer to Google<br>to get them installed properly.</p><h3 id="1-Preliminaries"><a href="#1-Preliminaries" class="headerlink" title="1. Preliminaries"></a>1. Preliminaries</h3><ul><li>Install XQuartz from <a href="http://www.xquartz.org/" target="_blank" rel="noopener">here</a>.</li><li>Install gcc with (this is for gfortran).</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install gcc</span><br></pre></td></tr></table></figure><p>It may take quite some time to install gcc from source. If you cannot wait, do xcode-select –install before install gcc. This will have mac install a pre-compiled version which is very fast.</p><h3 id="2-Import-the-scientific-computing-packages-with"><a href="#2-Import-the-scientific-computing-packages-with" class="headerlink" title="2. Import the scientific computing packages with"></a>2. Import the scientific computing packages with</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew update &amp;&amp; brew upgrade</span><br><span class="line">$ brew tap homebrew/science</span><br></pre></td></tr></table></figure><p>If you see any warnings, run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew doctor</span><br></pre></td></tr></table></figure><p>and follow any suggestions to fix the problem. And then re-import the packages as follows</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew untap homebrew/science</span><br><span class="line">$ brew tap homebrew/science</span><br></pre></td></tr></table></figure><h3 id="3-Install-Octave"><a href="#3-Install-Octave" class="headerlink" title="3. Install Octave"></a>3. Install Octave</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install octave --without-docs</span><br></pre></td></tr></table></figure><p>The option <em>–without-docs</em> above is to suppress errors due to missing Tex installation. </p><h3 id="4-Install-gnuplot"><a href="#4-Install-gnuplot" class="headerlink" title="4. Install gnuplot"></a>4. Install gnuplot</h3><p>As gnuplot will automatically be installed with octave, but without support for X11. So we need to reinstall it properly.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew uninstall gnuplot</span><br><span class="line">$ brew install gnuplot --with-x</span><br></pre></td></tr></table></figure><p>To me, I still got the following warnings after the steps above.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">warning: ft_render: unable to load appropriate font</span><br><span class="line">warning: could not match any font: *-normal-normal-10</span><br></pre></td></tr></table></figure><p>It can be fixed by following this stack overflow <a href="http://stackoverflow.com/questions/35249881/octave-fontconfig-error" target="_blank" rel="noopener">post</a>. Simply put it here where you should add this line into your ~/.bash_profile.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export FONTCONFIG_PATH=/opt/X11/lib/X11/fontconfig</span><br></pre></td></tr></table></figure><p>And run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source ~/.bash_profile</span><br></pre></td></tr></table></figure><p>to reload the config within your terminal. Alternatively, you could restart your teminal.</p><h3 id="5-Configurations"><a href="#5-Configurations" class="headerlink" title="5. Configurations"></a>5. Configurations</h3><p>Put the following configurations into <em>~/.octaverc</em>. If there’s no such file, just create one yourself.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">setenv (&quot;GNUTERM&quot;, &quot;X11&quot;)</span><br><span class="line"></span><br><span class="line"># optional if you are in favor of a more elegant prompt.</span><br><span class="line">PS1(&apos;❯❯ &apos;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Installing Octave on the latest OSX can be error prone.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="octave" scheme="liqul.github.io/blog/tags/octave/"/>
    
  </entry>
  
  <entry>
    <title>《国史大纲》读书笔记</title>
    <link href="liqul.github.io/blog/2018/04/10/2016-12-03-book-guoshidagang-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2018/04/10/2016-12-03-book-guoshidagang-NSConflict-liqul/</id>
    <published>2018-04-10T13:59:45.221Z</published>
    <updated>2018-04-10T13:59:45.221Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>一、当信任何一国之国民，尤其是自称知识在水平线以上之国民，对其本国已往历史，应该略有所知。<br> (否则最多只算一有知识的人，不能算一有知识的国民。) </p></blockquote><blockquote><p>二、所谓对其本国已往历史略有所知者，尤必附随一种对其本国已往历史之温情与敬意。<br> (否则只算知道了一些外国史，不得云对本国史有知识。) </p></blockquote><blockquote><p>三、所谓对其本国已往历史有一种温情与敬意者，至少不会对其本国以往历史抱一种偏激的虚无主义，<br>  (即视本国已往历史为无一点有价值，亦无一处足以使彼满意。)<br> 亦至少不会感到现在我们是站在已往历史最高之顶点，<br> (此乃一种浅薄狂妄的进化观。)<br> 而将我们当身种种罪恶与弱点，一切诿卸于古人。<br> (此乃一种似是而非之文化自谴。) </p></blockquote><blockquote><p>四、当信每一国家必待其国民具备上列诸条件者比数渐多，其国家乃再有向前发展之希望。<br> (否则其所改进，等于一个被征服国或次殖民地之改进，对其国家自身不发生关系。换言之，此种改进，无异是一种变相的文化征服，乃其文化自身之萎缩与消灭，并非其文化自身之转变与发皇。) </p></blockquote><!--break--><p>前三章包括序言没有写笔记，一来当时没做，二来其中大量远古事件只有梗概。但并非不重要，也无谓追求完美。</p><h2 id="第四章”霸政时期”"><a href="#第四章”霸政时期”" class="headerlink" title="第四章”霸政时期”"></a>第四章”霸政时期”</h2><h3 id="第三节：“齐桓晋文之霸业”"><a href="#第三节：“齐桓晋文之霸业”" class="headerlink" title="第三节：“齐桓晋文之霸业”"></a>第三节：“齐桓晋文之霸业”</h3><blockquote><p>总观当时霸业，有二大要义：一则为诸夏耕稼民族之城市联盟，以抵抗北方游牧部落之侵略，因此得保持城市文化，使不致沦亡于游牧之蛮族。二则诸夏和平联合以抵御南方的楚国帝国主义之武力兼并，因此得保持封建文化，使不致即进为郡县的国家。</p></blockquote><blockquote><p>其大势为文化先进诸国逐次结合，而为文化后进诸国逐次征服。同时文化后进诸国，虽逐次征服先进诸国，而亦逐次为先进诸国所同化。其文化落伍诸部族，则逐次消灭，或逐次驱斥。</p></blockquote><p>原来历史书上所说的“春秋五霸”，实际上主要是由于外力作用下，诸国之间形成的，替代没落的周天子的抱团方式。随着所谓戎狄蛮夷的逐渐同化和被驱逐，这种外在的压力最终不复存在，国与国之间就逐渐开始兼并了。</p><p>文化是个很有意思的东西，往往先进的文化会被落后的文化征服，似乎越是有文化，就越是骄奢淫逸…而最终实际上要么征服者被先进文化同化，要么由于处理不好这个问题短短数年就被推翻，历史上这样的例子太多太多了。秦国能统一，大概只是因为他地处边缘，最后被中原文化同化，所以在前辈们已经趴在沙滩上动不了的时候上去补了个刀。</p><h2 id="第五章“军国斗争之新局面”"><a href="#第五章“军国斗争之新局面”" class="headerlink" title="第五章“军国斗争之新局面”"></a>第五章“军国斗争之新局面”</h2><h3 id="第二节：“从宗法封建到新军国之种种变迁”"><a href="#第二节：“从宗法封建到新军国之种种变迁”" class="headerlink" title="第二节：“从宗法封建到新军国之种种变迁”"></a>第二节：“从宗法封建到新军国之种种变迁”</h3><p>书里总结了几个变化：</p><ol><li>郡县制替代了贵族世袭的采地，食禄制度兴起和养贤</li><li>战争由贵族之间的“游戏”变成残酷杀戮</li><li>军民渐趋分治，商业和货币兴起，军人和商人兴起而替换了传统的贵族</li><li>废井田开阡陌和税收制度改革</li></ol><p>这些变化存在内在的逻辑关联。周代礼崩乐坏，贵族之间为争夺有限的资源而产生激烈竞争，从而类似中世纪骑士们”过家家“似得战车战转变为步卒肉搏战。为了战胜对方，不再依赖养尊处优的贵族阶级，而必须产生专业的杀戮工具———军队，而为了鼓励底层士卒又产生了军功系统。军人有了晋升阶梯，成为新贵族，而他们需要得到土地奖励，导致了废除与传统贵族分封制对应的井田制，进一步产生了新土地制度下的税收体系。贵族的没落，商人从贵族的附属流入民间，进入自由商业时代，从而引发了货币的兴起。</p><p>看这一系列的描述，总感觉能看到欧洲文艺复兴时期的味道，两者都是封建社会崩溃的过程。但是，为何欧洲随后横扫全球而当年“中国”只是实现了秦朝的大一统？两者类似螺旋上升的不同层次。从战国到文艺复兴，两者相隔千年，生产力在逐渐进步，因此类似战国时期各国对内的扩展和对外的兼并，欧洲各国有更强的实力去扩展和兼并，因此不仅在欧洲，它们有能力去兼并更广大的世界。</p><p>另外，以前从历史课本里知道的秦朝大一统后的政策，实际上只是战国的延续，并非秦朝创立的新制度。</p><h2 id="第六章“民间自由学术之兴起”"><a href="#第六章“民间自由学术之兴起”" class="headerlink" title="第六章“民间自由学术之兴起”"></a>第六章“民间自由学术之兴起”</h2><h3 id="第二节：“儒墨两家之兴起”"><a href="#第二节：“儒墨两家之兴起”" class="headerlink" title="第二节：“儒墨两家之兴起”"></a>第二节：“儒墨两家之兴起”</h3><blockquote><p>“兼爱”与“仁”不同。仁非不爱人，特有亲疏等差，故说“孝悌为仁之本”。人决无不能爱其父母而能爱别人者。“兼爱”异于“别爱”，乃一种无分别之爱，亦可说是一种大同之爱，抹杀个人，只就大群着眼。</p></blockquote><blockquote><p>但是儒、墨两派，有他们共同的精神，他们全是站在全人类的立场，来批评和反对当时的贵族生活。儒家精神比较温和，可说是反对贵族立场的右派；墨家较激烈，可说是左派。以下战国学派，全逃不出儒、墨两家之范围。</p></blockquote><p>正因为孔子、墨子身处不同环境，所以产生的思想也就有差别。孔子认为孝是爱的本源，通过推己及人来把这种爱由自己父母推及其他人。这应该是家族观念的一种延续。而墨子出身寒苦，则没有深刻的家族意识，有的是抱团取暖的底层人民，所以更强调一种类似共产主义的东西。</p><p>这里出现了“右派”和“左派”，对应保守和激进。在改革的时候，保守派更加容易容于当时环境，虽然存在一些不彻底的地方，但温和的演变带来的破坏力也就比较小。激进常常伴随着不懂得妥协，一旦成功，其破坏力通常将不好的和好的一并带走。</p><h3 id="第五节：“贵族养贤”"><a href="#第五节：“贵族养贤”" class="headerlink" title="第五节：“贵族养贤”"></a>第五节：“贵族养贤”</h3><blockquote><p>唯四公子门下，真士少，伪士多。只见游士气焰之高涨，而不见他们的真贡献。</p></blockquote><blockquote><p>此非当时之无士，四公子争以养士为名高，动称“门下食客三千人”，何来有如许士？伪滥杂进，则真士不致。</p></blockquote><p>物以类聚、人以群分，这个道理无论在那时还是今天都是成立的。一旦群体里滋生了争名夺利的氛围，有识之士也就只能敬而远之或沉默不语了。</p><h3 id="第六节：“平民学者间之反动思想”"><a href="#第六节：“平民学者间之反动思想”" class="headerlink" title="第六节：“平民学者间之反动思想”"></a>第六节：“平民学者间之反动思想”</h3><blockquote><p>老子的理论，其要者，反尚智，反好动，反游士食客，皆针对当时的现象。注：此种现象，皆春秋时代所无。</p></blockquote><p>由此可知老子主要生活在战国时期。</p><h2 id="第七章“大一统政府之创建”"><a href="#第七章“大一统政府之创建”" class="headerlink" title="第七章“大一统政府之创建”"></a>第七章“大一统政府之创建”</h2><h3 id="第二节：“国家民族之传成”"><a href="#第二节：“国家民族之传成”" class="headerlink" title="第二节：“国家民族之传成”"></a>第二节：“国家民族之传成”</h3><blockquote><p>秦以下，虽封建遗形尚未全绝，然终不能再兴。</p></blockquote><blockquote><p>自此景、武下逮东汉，封建名存实亡，寸土一民，皆统于中央，诸封王唯食邑而已。</p></blockquote><blockquote><p>唯明初封诸王，欲以封建、郡县相杂，然一、再传即废。</p></blockquote><blockquote><p>以唐太宗之英武，唐初文、武诸功臣之出众，诚使君臣割地，各自专制一方，…，。所以不能尔者，由国人对于政治意义之认识，久已不许复有贵族世袭封建制度之存在。</p></blockquote><p>观念最难形成，也最难改变。《乌合之众》里说，一个民族有其固有的基因；《晚晴七十年》里说一种制度代替另一种制度要二三百年时间。辛亥革命把民主思想带入人心后，复辟就变得不可能了。可笑明初居然想复古，结果落下一朝的各种残疾，可笑。</p><blockquote><p>战国之秦乃如春秋之楚，不得即此谓秦果夷瞿秋。</p></blockquote><p>中华民族本是各民族融合而成。若怀着无比狭义的民族主义，哪里还有所谓“中华民族”？</p><h3 id="第三节：“第一次统一征服之出现及其覆灭”"><a href="#第三节：“第一次统一征服之出现及其覆灭”" class="headerlink" title="第三节：“第一次统一征服之出现及其覆灭”"></a>第三节：“第一次统一征服之出现及其覆灭”</h3><blockquote><p>始皇曰：“天下共苦战阀不休，以有侯王。天下初定，又复立国，是树兵也”。</p></blockquote><blockquote><p>秦君臣此番建树，于中国史上政体之促进有大功绩。后人空以专制讥秦，疏欠平允。</p></blockquote><p>“罗辑思维”介绍《秦迷》一书，讲到秦始皇为统一六国的一系列政治手段，不知道是真是假。但如果上面所言不虚，则在那时候始皇果然十分先进。</p><blockquote><p>秦代政治的失败，最主要的在其役使民力之逾量。</p></blockquote><p>创业中都难免犯各种错误，何况第一次形成一个统一的国家。后世不应该以现在的思想去评议古人的决定。</p><h3 id="第四节：“平民政府之产生”"><a href="#第四节：“平民政府之产生”" class="headerlink" title="第四节：“平民政府之产生”"></a>第四节：“平民政府之产生”</h3><blockquote><p>项羽、田横之徒皆贵族，而皆不能成事，此可以观世变。</p></blockquote><p>以前没有从这个方面想过。</p><blockquote><p>当时平民政府的第二个反动思想则为“无为而治”。</p></blockquote><blockquote><p>即如萧何定律，而夷三族、妖言令、携书律等皆存在。至孝惠、高后、文帝时逐渐废除。唯精神上汉则恭俭，秦则骄奢，此其异。</p></blockquote><p>汉初政治并不比秦国来得先进，相似的政治制度，在不同的掌权人手里，也会出现不同的结果。但这种人治的成分必须减弱才是系统能长期良性运行的保证。可惜后代还是有不明白这个道理的浑人当道。</p><blockquote><p>平民政府有其必须完成之两大任务，首先要完成统一，其次为完成文治。</p></blockquote><p>平民政府只能通过暴力革命夺取政权，然后自然对军人要论功行赏，过渡到文治需要相当漫长的时间。</p><h2 id="第八章“统一政府文治之演进”"><a href="#第八章“统一政府文治之演进”" class="headerlink" title="第八章“统一政府文治之演进”"></a>第八章“统一政府文治之演进”</h2><h3 id="第二节：“西汉初年之政府”"><a href="#第二节：“西汉初年之政府”" class="headerlink" title="第二节：“西汉初年之政府”"></a>第二节：“西汉初年之政府”</h3><blockquote><p>如是则当时的政治组成，第一层是宗室，第二层是武人，第三层是富人，第四层是杂途。注：…。文学、儒术亦杂途之一。</p></blockquote><p>这个政治结构或许后代会有所演进，但基本面还是差不多的。这也就算是平民学术兴起带来的结果吧。</p><h3 id="第三节：“西汉初年的士人与学术”"><a href="#第三节：“西汉初年的士人与学术”" class="headerlink" title="第三节：“西汉初年的士人与学术”"></a>第三节：“西汉初年的士人与学术”</h3><blockquote><p>秦代焚书，最主要者为六国史记，其次为诗、书古文，而百家言非其所重。</p></blockquote><blockquote><p>焚书本起于议政冲突，博士淳于越称说诗、书，引据古典，主复封建，李斯极斥之，遂牵连而请焚书。</p></blockquote><p>焚书缘起于朝堂上的政治冲突，但领土统一后消灭人民对故国的怀念，最好的方法似乎也是消灭故国的历史。但任何激进的做法其效果都难以理想，尤其是针对人们心理上的问题。秦刚刚灭六国，六国虽然被灭但人还在，除非政权稳固到反对的声音都安静下来，而这需要非常长的时间，否则只能招致报复和反叛。</p><h3 id="第四节：“中央政府文治思想之开始”"><a href="#第四节：“中央政府文治思想之开始”" class="headerlink" title="第四节：“中央政府文治思想之开始”"></a>第四节：“中央政府文治思想之开始”</h3><blockquote><p>注：农民政府之好处在真朴，坏处在无礼貌；可爱初在皇帝、宰相如家人，其弊处则皇帝待宰相如奴仆。</p></blockquote><p>淳朴通常意味着价值观混乱、多重标准。</p><blockquote><p>汉初政治，往往有较秦为后退者，注：此因平民政府缺少学术意味之故。</p></blockquote><p>回想中国历代，每每政治清明国力强盛的时候，其学术活动也是极其活跃的，或体现为诗、辞、歌、赋；每每政治昏暗的时候，其学术活动也陷入低潮，内容晦涩黯淡。虽然这只是一个侧面，但仔细想想也不无道理。</p><h3 id="第八节：“王莽受禅与变法”"><a href="#第八节：“王莽受禅与变法”" class="headerlink" title="第八节：“王莽受禅与变法”"></a>第八节：“王莽受禅与变法”</h3><blockquote><p>王莽的政治，完全是一种书生的政治。</p></blockquote><blockquote><p>王莽的失败，变法让贤的政治理论，从此消失，渐变为帝王万世一统的思想。政治只求保王室之安全，亦绝少注意到一般平民的生活。这不是王莽个人的失败，是中国史演进过程中的一个大失败。</p></blockquote><p>《国史大纲》的论述里谈到，王莽篡位不是一件单纯的个人行为，否则不至于能坚持长达18年的时间。这实际上承袭了儒家一直以来的轮换让贤思想，是一种政治结构的实践。与之对比民国初年张勋复辟正是由于是一种反先进思想的闹剧，所以不能真正在历史上留下什么。从现在来看，似乎觉得那必然是失败的，或许只是因为接受了“万世一统”的观念太深了。这真的是必然吗？当然不是，只是在合适的时机做一件合适的事情会得到好的效果；反过来，哪怕出发点和事情本身是好的，时机不对，得到的效果可能是相反的，而且在人们心理上造成的负面影响可能很久都无法消失。Sigh，这次失败实际上意味着后世再不会有人尝试这种“脑残”的想法了。</p><h2 id="第九章“统一政府之堕落”"><a href="#第九章“统一政府之堕落”" class="headerlink" title="第九章“统一政府之堕落”"></a>第九章“统一政府之堕落”</h2><h3 id="第四节：“外戚参加王室的由来”"><a href="#第四节：“外戚参加王室的由来”" class="headerlink" title="第四节：“外戚参加王室的由来”"></a>第四节：“外戚参加王室的由来”</h3><blockquote><p>西汉初年，宗室、功臣、外戚，为朝廷之三大系。</p></blockquote><p>随着王朝的演进，由功臣而来的荫功集体首先变得越来越弱。政府渐渐独立于王室，而王室则需要外戚来牵制政府的权利。只是宗室逐渐堕落，而导致外戚实力增大而已。不过，只要外戚不至于改朝换代，则没有大碍，因为外戚是不世袭的。</p><h3 id="第五节：“宦官参加王室之由来”"><a href="#第五节：“宦官参加王室之由来”" class="headerlink" title="第五节：“宦官参加王室之由来”"></a>第五节：“宦官参加王室之由来”</h3><p>所谓九卿，是指</p><ul><li>太常：掌宗庙礼仪</li><li>光禄勋：掌宫殿门户文官</li><li>卫尉：掌宫殿门户武官</li><li>太仆：掌车马</li><li>廷尉：掌刑辟</li><li>大鸿胪：掌蛮夷归化</li><li>宗正：掌宗属</li><li>大司农：掌农货</li><li>少府：掌山货</li></ul><p>不得不说，虽然名字听起来都很文雅，实际上都是些帝王身边的贴身侍从或“家务官”。只是帝王身边的侍从，后来都演变成其它更实际职能的官员，至古至今都一样的道理。</p><blockquote><p>宦本宦学、仕宦，非恶称也。</p></blockquote><p>宦本是做官与做学问，“本无恶称”，奈何历史上留下骂名的比较多，再加上身体残疾，所以成了一种恶称。比起外戚，宦官的可传递性更弱，所以会被选择成为王室集团的一员，与外朝争夺权力。其实，从宦官当权的本质上来讲，也只是帝王堕落而导致的。</p><h2 id="第十章“士族之新地位”"><a href="#第十章“士族之新地位”" class="headerlink" title="第十章“士族之新地位”"></a>第十章“士族之新地位”</h2><h3 id="第一节：“士族政治势力之逐步膨胀”"><a href="#第一节：“士族政治势力之逐步膨胀”" class="headerlink" title="第一节：“士族政治势力之逐步膨胀”"></a>第一节：“士族政治势力之逐步膨胀”</h3><blockquote><p>博士弟子额之日益增添。</p></blockquote><p>须知要扩大一组织的影响力，并不是要层层设卡减少参加的人数，美其名曰提高质量。当然也不能鱼龙混杂，否则会造成前面提到过的“真士不至”的问题。两者之间要求得一个平衡。</p><blockquote><p>全国各郡县常得平均参加中央政局，对大一统政府之维系，尤为有效。</p></blockquote><p><em>参与感</em>总是凝聚组织结构的最重要手段。</p><h3 id="第三节：“太学清议”"><a href="#第三节：“太学清议”" class="headerlink" title="第三节：“太学清议”"></a>第三节：“太学清议”</h3><blockquote><p>士人在政治、社会上势力之表现，最先则为一种“清议”。</p></blockquote><blockquote><p>东汉自光武、明、章，虽云崇奖儒业，…，清议为转移，直至东汉末叶，此风弗衰。</p></blockquote><p>这段文字读起来非常不容易懂，尤其是其中一些句子如：</p><blockquote><p>国家喜文法廉吏，以为足以止佞，然文吏习为欺谩，廉吏清在一己，无益百姓流亡，盗贼为害也。</p></blockquote><blockquote><p>承王莽后，加严猛为政，因以成俗，是以郡国所举，多办职俗吏，不应宽博之选。</p></blockquote><blockquote><p>王充论衡亦极辩世俗常高“文吏”，贱下“儒生”之非。</p></blockquote><p>这里面提到的“文法廉吏”、“文吏”、“廉吏”、“俗吏”之类的实际上指代一类人，他们有一定学问，被地方长官选为辅助的官吏，处理一些复杂的政务；而对应的“儒生”则对应那些有学问但不是官吏的人。文吏由于长期处理事务，有一定的执政经验，所以在社会上的地位比儒生要高；相对的儒生没有行政经验，所以被认为不能承担责任，遇到事情战战兢兢不知道如何处理。</p><p>文吏可以由儒生转变过来，而长期处理杂务会导致忠诚正直方面欠缺。一些儒生因为看到文吏的腐败行为而拒绝加入其行列。这些儒生看到其它儒生为了当官而放弃自身在正直方面的追求而责骂他们，这导致了社会上更加轻谩儒生。针对这种现象，《论衡》里的<a href="http://so.gushiwen.org/guwen/bfanyi_4792.aspx" target="_blank" rel="noopener">“程材篇”</a>有很多讨论。</p><p>正因为东汉初年这些议论，导致了</p><blockquote><p>稍后郡国查举，渐移趋向。言事者谓郡国查举不以功次，养虚名者累进，故守职者益懈，而吏事陵迟。</p></blockquote><p>既然推荐制度不考虑完成的功绩，而重点看个人的品行和社会上的名望，所以渐渐人们开始只注重名声，因此从政者为保持高尚的名节，很多实际事务便无法处理。从政者注意民间舆论是一种进步，然而过于影响政治推移就不好了。</p><p>矫枉过正，这也是一种必然的结局。</p><h3 id="第四节：“党锢之狱”"><a href="#第四节：“党锢之狱”" class="headerlink" title="第四节：“党锢之狱”"></a>第四节：“党锢之狱”</h3><blockquote><p>因此宦官之势，乃非外朝士人之力所能摧陷廓清，名士不得不内结外戚，而外戚到底是一种腐败的因袭体，名士遂终与之两败。</p></blockquote><p>国家虽有法度，而有些群体不在约束范围内，或其行为不为外人知晓而实际上无法执行，则法度是不会起作用的。前者可以通过立法来弥补，或者则需打破一些根深蒂固的东西，实际上比立法更难。</p><p>依靠一种污浊的势力去打击另一种污浊的势力，此消彼长，最终只能变成一种“循环发生”的现象了，而国家在这个过程中</p><blockquote><p>“人之云亡，邦国殄瘁”。</p></blockquote><p>即身系国家安危的贤人都死了，离亡国就不远了。</p><blockquote><p>黑暗腐败的汉王室，终于倾覆，依附于王室的外戚与宦官，亦同归于尽。</p></blockquote><p>外戚与宦官实际上寄生于王室，皮之不存毛将焉附？</p><h3 id="第五节：“门第之造成”"><a href="#第五节：“门第之造成”" class="headerlink" title="第五节：“门第之造成”"></a>第五节：“门第之造成”</h3><blockquote><p>“累世经学”与“累世公卿”，便造成士族传袭的势力，积久遂成门第。</p></blockquote><p>做学问在当时是少数家族专有的权益，而经学又是仕途的入门手段，所以传袭成门第。另一方面，上升途径若只掌握在少数群体手中，则长期以往，那些无法上升群体的矛盾将逐步激化。</p><blockquote><p>自东汉统一政府的倾覆，遂变成变相之封建。长期的分崩离析，而中国史开始走上衰运。</p></blockquote><p>钱老的观点一直是：中国政治体制需在平静中不断演进，而长久的战争只会造成倒退。回顾古代、近代一些运动，无不印证这个观点。钱老亲历一段岁月，作为那个时代的知识分子，身系中华民族文化是否能传承下去，书写《国史大纲》来从中国历史的发展来解答和对抗一系列的“新思想”，不可不称作一位斗士。这个线索不妨听听梁文道在其<em>一千零一夜</em>里对这部书的讨论。</p><h3 id="第六节：“东汉士族之风尚”"><a href="#第六节：“东汉士族之风尚”" class="headerlink" title="第六节：“东汉士族之风尚”"></a>第六节：“东汉士族之风尚”</h3><blockquote><p>古者刑不上大夫，故贵族阶级相互有隙，不得直于法庭，则以私斗决之。</p></blockquote><p>大概决斗也是贵族特有的一种形式吧，无论在东方还是西方。</p><blockquote><p>礼有之：“父母存，不许友以死”</p></blockquote><p>大意是指父母在的时候，不能为朋友赴死。如《史记》刺客列传里聂政说：“臣所以降志辱身居市井屠者，徒幸以养老母；老母在，政身未敢许人也”。</p><p>可见士族风尚学古。</p><blockquote><p>道德自为人生不可缺少之要素，然亦只是人生中一端。</p></blockquote><p>文章指出太过注重道德则会导致两个方面弊端：其一，人们争相以道德来分高下，如苏轼所说</p><blockquote><p>“上以孝取人，则勇者割股，怯者庐墓；上以廉取人,则弊车羸马,恶衣菲食。”</p></blockquote><p>这句来自苏轼的《<a href="http://www.laomu.cn/wxzp/xiaoshuo/china/Ts8/html04/mydoc04052.htm" target="_blank" rel="noopener">议学校贡举状</a>》，又读到其中有以下议论</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">凡可以中上意，无所不至矣。德行之弊，一至于此乎！自文章而言之，则策论为有用，诗赋为无益；自政事言之，则诗赋、策论均为无用矣。虽知其无用，然自祖宗以来莫之废者，以为设法取士，不过如此也。岂独吾祖宗，自古尧舜亦然。《书》曰：“敷奏以言，明试以功。”自古尧舜以来，进人何尝不以言，试人何尝不以功乎？议者必欲以策论定贤愚、决能否，臣请有以质之。近世士大夫文章华靡者，莫如杨亿。使杨亿尚在，则忠清鲠亮之士也，岂得以华靡少之。通经学古者，莫如孙复、石介，使孙复、石介尚在，则迂阔矫诞之士也，又可施之于政事之间乎？自唐至今，以诗赋为名臣者，不可胜数，何负于天下，而必欲废之！近世士人纂类经史，缀缉时务，谓之策括。待问条目，搜抉略尽，临时剽窃，窜易首尾，以眩有司，有司莫能辨也。且其为文也，无规矩准绳，故学之易成；无声病对偶，故考之难精。以易学之士，付难考之吏，其弊有甚于诗赋者矣。</span><br></pre></td></tr></table></figure><p>“古文运动”本在祛除读书人只注重华美的文体而忽略文章的内容的毛病。出发点是好的，但却也容易走向另一个极端，即上面所说的“无规矩准绳”和“无声病对偶”。表面上看这些文章讨论时政，似乎更切合实际，但“易学”、“难精”，没有统一的衡量标准也导致“难考”。正如今天所说的素质教育，较之过去统一的高考，实则有更大弊端。</p><p>另一方面，正如苏轼所说，那些文采华美的文章流传自今，有多少出自贤人的手笔？又有多少出自愚者的手笔？两相比较不难看出其正面远大于负面。王国维《人间词话》的论述，诗词更多的是反映作者胸中的大气象，如无大气象又怎能写出这样的文章？正如前文提到中国国运常常跟文化繁荣的程度相呼应，这应该就是内在的一点原因吧。</p><p><em>注：上面提到的唐代的”通榜“是一种不糊名的人才选拔机制。</em></p><p>太过注重道德的另一个弊端则是</p><blockquote><p>若做事太看重道德，便流于重形式而忽略了内容的实际。</p></blockquote><p>文中提到将军在尚未败北时已自杀殉国，只图自己留个好名。又想起所谓“天子守国门”和崇祯煤山自裁（虽然这并非完全归于道德），是不是有点内在联系呢？至少，这在当时直接导致的后果是</p><blockquote><p>所以名士势力日大，而终不能铲除宦官的恶势力。因东汉人只看重私人和家庭的道德，故王室倾覆后，再不能重建一共戴的中央，而走入魏晋以下之衰运。</p></blockquote><p>实际上在帝制的作用下，士人已渐渐由国家的创业者变成临时工，所以缺乏积极性也是难免的了。而随着帝制的不断推演，这种现象愈演愈烈，近代乃至当代的一些现象就不难理解了。</p><h2 id="第十一章“统一政府之对外”"><a href="#第十一章“统一政府之对外”" class="headerlink" title="第十一章“统一政府之对外”"></a>第十一章“统一政府之对外”</h2><blockquote><p>因此中国史上对外之胜负、强弱，几乎完全视国内政治为转移。</p></blockquote><p>自秦朝统一到近代以前，这的确是个事实，无怪国人应对列强手足无措了。</p><h3 id="第一节：“两汉之国力比较”"><a href="#第一节：“两汉之国力比较”" class="headerlink" title="第一节：“两汉之国力比较”"></a>第一节：“两汉之国力比较”</h3><blockquote><p>西汉的立国姿态，常是协调的、动的、进取的。</p></blockquote><blockquote><p>东汉的立国姿态，可以说常是偏枯的、静的、退守的。</p></blockquote><p>这是由建都的位置引出的。早先听说过一个观点，即定都在西边的国家常常是强大的，如西汉、唐；定都在东边的常常是羸弱的，如东汉、宋、明。只觉得这里隐隐有一种内在原因，不甚清楚，而这里给出的讨论让人深思。</p><p>自古以来，中国东部土地肥沃、气候适宜，因而经济文化要比西部发达；西部生存环境相对恶劣，所以人民尚武而民风彪悍。定都在西部时，由于政治力量推动（这种力量当容易理解），东部繁华的经济文化向西部侵染，而西部磅礴气象也向东部扩散，实际上对双方都是有大好处的。文化离开大气象便成为靡靡之音，而彪悍的民风如没有文化的熏陶也会逐步走向失控和野蛮。这种东西方循环恰如人体的循环系统，隐隐维持和促进着社会经济文化发展。在这种循环过程中，人为因素便是其中的政治力量。定都在东部时，这种政治力量就不再推动大循环，而转为一种单一的、小规模的、不健康的循环。在缺乏政治力量推动的情况下，人民自身不会去弥补这种缺失。</p><p>文中列举</p><blockquote><p>汉诸帝并有陵寝徙民的制度。</p></blockquote><p>西汉实行将东部人民迁移到西部，这即政治力量的推动。如果没有则</p><blockquote><p>人情安土重迁，宁就饥饿，犹群羊聚畜，须主者牧养处置。</p></blockquote><p>东汉在这种维持稳定力量的缺失下，最终</p><blockquote><p>东方食少而有黄巾，西边多事而又董卓，此诚两汉兴亡一大关键也。</p></blockquote><p>联想到美国选州府，常常会在一个没听说过的小城市，其目的也是希望能凭借政治力量逐步平衡各地的差距，最终达到稳定国家的目的。虽然一段时间可能会造成一些效率降低，但长远来看正面作用更可观，不得不佩服这种高明的选择。</p><p>另一方面，西汉</p><blockquote><p>大抵是一个杂色的局面。东汉则渐渐从杂色的转变成一色，…</p></blockquote><p>又联想到一些企业招聘员工的时候常注重一个dynamics，即吸纳不同国家、文化背景下的人群。其基本原理在于不同人对事物的观点常常是不同的，这样的环境更加容易产生优秀的思想。又如美国高校有毕业生不能留在本校任教职。这种种做法都是为了避免一种“近亲繁殖”的问题。</p><p>东汉这种按照某种规则给不同人群划分品节的陋习也就随着历史流传给了后面的朝代。</p><blockquote><p>太宗置官品令，谓房玄龄曰：“朕设此官员，以待贤士。工商杂色之流，假令术逾侪类，止可厚 给财物，必不可超授官秩。</p></blockquote><p>可见唐朝也袭此流弊。</p><h3 id="第二节：“西汉与匈奴”"><a href="#第二节：“西汉与匈奴”" class="headerlink" title="第二节：“西汉与匈奴”"></a>第二节：“西汉与匈奴”</h3><blockquote><p>观去病之将兵，较之项王未多逊。…</p></blockquote><p>一段注释解释了一个以前困扰我的问题，即为何在《史记》中卫青霍去病合为一列传（111），而独为李将军一列传（109）。原因在于卫青和霍去病同为亲贵，所以与底层豪杰李广恰似两个相对的党派。</p><h3 id="第三节：“东汉与西羌”"><a href="#第三节：“东汉与西羌”" class="headerlink" title="第三节：“东汉与西羌”"></a>第三节：“东汉与西羌”</h3><blockquote><p>当时士大夫见朝事无可为，唯有拥兵以戮力边檄，尚足为功名一径…</p></blockquote><p>这也导致了后来三国各地割据的局面。</p><h2 id="第十二章“长期分裂之开始”"><a href="#第十二章“长期分裂之开始”" class="headerlink" title="第十二章“长期分裂之开始”"></a>第十二章“长期分裂之开始”</h2><h3 id="第二节：“旧政权之没落”"><a href="#第二节：“旧政权之没落”" class="headerlink" title="第二节：“旧政权之没落”"></a>第二节：“旧政权之没落”</h3><blockquote><p>散漫的农民在饥饿线上临时结合起来，其力量不够得推翻他。</p></blockquote><blockquote><p>且以中国疆域之展布，纵使大饥荒，亦必夹有丰收的地带，要一般农民一致奋起，事亦不易。于是无可团结的社会，乃借助于“宗教”与“迷信”。农民结合于宗教与迷信的传播之下，而一致奋起，成为东汉末年之黄巾。</p></blockquote><blockquote><p>国家本是精神的产物，把握时代力量的名士大族，他们不忠心要一个统一的国家，试问统一国家何从成立？</p></blockquote><p>历史上成功的农民起义，往往不真的是农民群体在起决定性作用，而是附庸在这个群体上真正起到决策作用的精英。正如作者所说，农民不具备团结一致的奋斗精神，往往会被一些近期的利益所诱惑，从而从内部分崩离析。看黄巾之乱、太平天国、李自成，再对比刘邦、朱元璋，就能发现其中的区别。精英阶级一旦无法通过合法的晋升阶梯入仕，那么积累的力量就会从别处发泄出来。</p><h3 id="第四节：“新政权之黑暗”"><a href="#第四节：“新政权之黑暗”" class="headerlink" title="第四节：“新政权之黑暗”"></a>第四节：“新政权之黑暗”</h3><blockquote><p>自古受命及中兴之君，易尝不得贤人君子与之共治天下者乎？及其得贤也，曾不出阎巷，岂幸相遇哉？上之人不求之耳。今天下尚未定，此特求贤之急时也。“孟公绰为赵、魏老则优，不可以为膝、薛大夫”。若必廉士而后可用，则齐桓其何以霸世！今天下得无有被褐怀玉而钓于渭滨者乎？又得无盗嫂受金而未遇无知者乎？二三子其佐我明扬仄陋，唯才是举，吾得而用之。</p></blockquote><blockquote><p>夫有行之士未必能进取，进取之士未必能有行也。陈平岂笃行，苏秦岂守信邪？而陈平定汉业，苏秦济弱燕。由此观之，士有偏短，庸可废乎！有司明恩此义，则士无遗滞，官无废业矣。</p></blockquote><blockquote><p>昔伊挚、傅说出于贱人，管仲，桓公贼也，皆用之以兴。萧何、曹参，县吏也，韩信、陈平负汗辱之名，有见笑之耻，卒能成就王业，声著千载。吴起贪将，杀妻自信，散金求官，母死不归，然在魏，秦人不敢东向；在楚，则三晋不敢南谋。今天下得无有至德之人，放在民间，及果勇不顾，临敌力战：若文俗之吏，高才异质，或堪为将守；负汗辱之名，见笑之行；或不仁不孝，而有治国用兵之术。其各举所知，勿有所疑。</p></blockquote><p>魏武三诏令，真前无古人后无来者，霸气侧漏。</p><blockquote><p>“天下无有孤，不知几人称王，几人称帝？”，此不足为篡窃之正大理由。</p></blockquote><blockquote><p>阴谋不足以镇压反动，必然继之以惨毒的淫威。</p></blockquote><p>人心是奇怪的东西，一个正大光明的理由能让它平静下来，反之，如果一开始的理由就是不对的，将来总有一天要还清当年欠下的债务。再强大的武力，也只是在一段时间内能镇压，不是长久之计。</p><p>正是这种不够光明正大的理由，奠定了魏晋这一历史上较黑暗混乱的时代。</p><h3 id="第五节：“思想界之无出路”"><a href="#第五节：“思想界之无出路”" class="headerlink" title="第五节：“思想界之无出路”"></a>第五节：“思想界之无出路”</h3><blockquote><p>但要提倡法治，起码的先决条件，在上应有一个较稳定的政权。【注：政权不稳定，法治精神无所倚依而生根。】政权之稳定，亦应依附于此政权者先有一番较正义，至少较不背乎人情的理想或事实。</p></blockquote><p>人们往往只注意到法治的好处，却忽略这一基本的前提。在政权不稳定的状态下提倡法治，只会使政权更加动荡，导致更糟糕的结果。作者在文字上总有一个“较”字修饰，读起来不禁觉得实在精辟。</p><h2 id="第十三章“统一政府之回光返照”"><a href="#第十三章“统一政府之回光返照”" class="headerlink" title="第十三章“统一政府之回光返照”"></a>第十三章“统一政府之回光返照”</h2><h3 id="第二节：“西晋王室之弱点”"><a href="#第二节：“西晋王室之弱点”" class="headerlink" title="第二节：“西晋王室之弱点”"></a>第二节：“西晋王室之弱点”</h3><blockquote><p>一、没有光明的理想为之指导。<br>二、贵族家庭之腐化。</p></blockquote><p>西晋处在历史这样一个节点上，贵族尚没有意识到理想、教育的重要性，所以有乘“羊车”、食“肉糜”的奇葩出现，而当代的“新贵族”较以往已经进步很多了。</p><h3 id="第四节：“怀慜被掳与人心之反映”"><a href="#第四节：“怀慜被掳与人心之反映”" class="headerlink" title="第四节：“怀慜被掳与人心之反映”"></a>第四节：“怀慜被掳与人心之反映”</h3><p>作者从帝王、皇后、大臣、将军、士族几个方面的讨论，给人一个直观的印象：西晋是一个彻头彻尾无廉耻气节的政权。</p><h2 id="第十五章“北方之长期纷乱”"><a href="#第十五章“北方之长期纷乱”" class="headerlink" title="第十五章“北方之长期纷乱”"></a>第十五章“北方之长期纷乱”</h2><h3 id="第三节：“五胡十六国大事简表”"><a href="#第三节：“五胡十六国大事简表”" class="headerlink" title="第三节：“五胡十六国大事简表”"></a>第三节：“五胡十六国大事简表”</h3><blockquote><p>盖浅化之民，性情暴戾，处粗野之生活中，尚堪放纵自适。一旦处繁杂之人事，当柔靡之奉养，转使野性无所发抒，冲荡溃决，如得狂疾。</p></blockquote><p>可笑一些“精英”附庸在这样“粗野”的人身边，以为可以依仗成就一番事业，往往最终难逃悲催的下场，正是由于不懂这样的道理呀！</p><h2 id="第十六章“南方王朝之消沉”"><a href="#第十六章“南方王朝之消沉”" class="headerlink" title="第十六章“南方王朝之消沉”"></a>第十六章“南方王朝之消沉”</h2><h3 id="第二节：“南朝王室之恶化”"><a href="#第二节：“南朝王室之恶化”" class="headerlink" title="第二节：“南朝王室之恶化”"></a>第二节：“南朝王室之恶化”</h3><blockquote><p>南朝的王室，在富贵家庭里成长起来，他们只稍微熏陶到了一些名士派放情恣志的风尚，而没有浸沉到名士们的家教与门风，又没有领略得名士们所研讨的玄言与远致。</p></blockquote><p>只注意到外在表现而不理解内在机理，今天这样的情况不也大有人在么？一本本成功学书籍畅销，一篇篇心灵鸡汤文在朋友圈流传，一条条形而上的不知所云的分享大都是类似的情况吧。多少人不去多读几本书，多做些思考和研究，不在实践中总结反思，而是随手捡起几个普适的概念大肆宣扬，真是误人误己。</p><blockquote><p>由名士为之则为雪夜访友，无知识，无修养，则变为达旦捕鼠。由名士为之则为排门看竹，无知识，无修养，则变为往寺庙偷狗吃。</p></blockquote><p>名士的风尚虽只能独善其身，但总算雅致；皇室以此为精神寄托本就不对，何况画虎类犬，就只能贻笑大方了。后人看着当然觉得好笑，在当时却是整个社会一片黑暗没有出路了。</p><h2 id="第十七章“北方政权之新生命”"><a href="#第十七章“北方政权之新生命”" class="headerlink" title="第十七章“北方政权之新生命”"></a>第十七章“北方政权之新生命”</h2><h3 id="第三节：“魏孝文迁都及北魏之覆灭”"><a href="#第三节：“魏孝文迁都及北魏之覆灭”" class="headerlink" title="第三节：“魏孝文迁都及北魏之覆灭”"></a>第三节：“魏孝文迁都及北魏之覆灭”</h3><blockquote><p>塞北荒寒，不配做新政治的中心。</p></blockquote><p>地理决定论自有其内在道理。</p><blockquote><p>惜乎孝文南迁五年即死。</p></blockquote><p>历史从来不以人的意志为转移，而这正是所谓的气数已尽吧。</p><blockquote><p>文治基础尚未稳固，而武臣出路却已断塞。</p></blockquote><p>改革最忌讳建设尚未完成而已经将原有的关系彻底打破。这样的改革如果顺利则只能说是运气好，否则造成的反弹可能完全推翻改革本来已经带来的正面的效果。</p><blockquote><p>一个国家，同时摆出两个绝不相同的社会，势必酿乱。</p></blockquote><p>然而一个国家又非常容易酿成两个绝不相同的社会。比如前面讨论东西汉差异的时候提到的，定都的位置不同，在长安则由政治力量引导产生国内东西大循环；在洛阳则东部南北小循环，使得东西隔绝。如果政治力量无法处理好这种地区间的均衡，则必然酿成祸乱。</p><blockquote><p>凡历史上有一番改进，往往有一度反动，不能因反动而归咎改进之本身；然亦须在改进中能善处反动方妙。魏孝文卒后，鲜卑并不能继续改进，并急速腐化，岂得以将来之反动，追难孝文！</p></blockquote><p>鲜卑本北方稍欠文化的部落，一旦处于温柔繁华的环境，腐化堕落的速度更是迅速，这与人成长的环境是紧密相关的。想起游戏《十字军之王2》里的设定：部落虽有较强武力，但在政治落后，容易招致内乱，而战斗士气随战争结束时间增长会迅速下降。这样的设定更加符合真实的情况，对比《三国志》系列喜欢把蛮族势力的武将个个设置成武力一般头脑简单的弱智要高明好多。</p><h3 id="第四节：“北齐北周文治势力之演进”"><a href="#第四节：“北齐北周文治势力之演进”" class="headerlink" title="第四节：“北齐北周文治势力之演进”"></a>第四节：“北齐北周文治势力之演进”</h3><blockquote><p>齐律尤为隋、唐所本。</p></blockquote><blockquote><p>由唐至清，皆本隋律，隋律则本于齐。</p></blockquote><p>从这种意义上讲，这似乎是上一个时代的终结与下一个时代的开始。高晓松所说中国古代的时代划分，好像也是以这里为分界的。</p><blockquote><p>治民之本，莫若宰守。治民之体，先当治心。</p></blockquote><p>这句话的大意是，“地方行政长官是治理人民的根本。要想治理人民，先要治理他们的内心”。这与阳明先生的心学似乎是呼应的。</p><blockquote><p>于是以前的官吏，为门资所应得；而此后的官吏，则将为民众负责任。</p></blockquote><p>这样的设定，在建设的同时不至于彻底废除历史的旧习，是一种政治上的妥协，于进步实有利。</p><blockquote><p>僚吏俊彦，旦理公务，晚就讲习。</p></blockquote><blockquote><p>从学术影响到政治，回头再走上一条合理的路，努力造出一个合理的政府来。</p></blockquote><p>“从学术影响政治”是钱老支持的主张，从前面的一些讨论中早有明示。衷心希望将来能真的有这么一天。</p><h2 id="第十八章“变相的封建势力”"><a href="#第十八章“变相的封建势力”" class="headerlink" title="第十八章“变相的封建势力”"></a>第十八章“变相的封建势力”</h2><h3 id="第一节：“九品中正制与门阀”"><a href="#第一节：“九品中正制与门阀”" class="headerlink" title="第一节：“九品中正制与门阀”"></a>第一节：“九品中正制与门阀”</h3><blockquote><p>三国丧乱之际…，一时逆转，而倒退为秦、汉初年之军功得官。</p></blockquote><p>越是乱世，越崇尚军功，越是崇尚结果导向而不计后果。此消彼长，意味着与军功相对的，较斯文的方式在社会上无法晋升。想到当今各互联网公司提倡的狼性文化、军功导向，不禁觉得离文化真正兴盛还有很远的路要走。</p><blockquote><p>于是有魏尚书陈群之“九品官人法”。</p></blockquote><p>九品官人法于州设置大中正，于郡设置小中正，中正负责本地贤良的举荐。中正并非真正的官职，而是由中央官员兼职，因当时处乱世而人才集聚于中央。</p><p>与汉代查举相比，九品中正制有两个最大不同：</p><ol><li>查举人才归地方而中正制归中央；</li><li>查举只涉及入仕而中正制是一套完整的官员入仕、考察机制。</li></ol><p>处于乱世，中央往往急需各地人才，所以这样的设定无可厚非。待乱世已平，人才的流动仍然遵循从地方到中央就不合时宜了。这直接导致了地方人才凋零，而汇聚中央的人才又无处施展自己的能力。</p><p>所以当时已有人察觉这样的状态，而有</p><blockquote><p>今天下复归一统，自当仍将查举权付之地方长官，不必再要一个中正。</p></blockquote><p>九品中正制还注重一个“品”字。</p><blockquote><p>“品”者履行，“状”者才能、绩效。</p></blockquote><p>处乱世时，由于只重视军功，导致为官者品行良莠不齐。而九品中正制正为校正这种过失，而重新强调品的重要性。官员品行好则升迁，品行不好则降级，于是造成一些高品低能的状况。</p><p>一种制度往往因当时的社会状态产生。一旦社会状态发生变化，制度需要做相应调整，否则就会不合时宜。但制度形成容易，变化则难，因制度而产生的利益阶层总不会轻易放弃自己的利益。然而，有制度总比没有制度要好。回想起多年前，从小学时候我就有个不解的问题：比如我所居住的县城里，县长这样的官员是如何成长起来的？这个问题实际上我现在也不清楚，仿佛没有固定的方法和评价标准，既不看是否学识出众，也不看是否有良好的品行。想到此，再与上面的状态比较，不难发现现在的政治制度仍然在乱世走向治世的初级阶段而已。</p><h3 id="第二节：“学校与考试制度之颓废”"><a href="#第二节：“学校与考试制度之颓废”" class="headerlink" title="第二节：“学校与考试制度之颓废”"></a>第二节：“学校与考试制度之颓废”</h3><blockquote><p>东汉的累世经学，即为造成门阀之一因。但到门阀势力一旦长成，学校与考试制度即不见重要，难于存在。</p></blockquote><p>联想到今天的两个现象，即所谓素质教育和出国热。这些现象的本质是什么呢？是希望打破以往国家制定的高考制度，而独立出另外的标准。这些标准跟高考制度最大的差别在哪呢？更加考察出身而已。农村的孩子没有机会学习钢琴、舞蹈，没有钱送到国外镀金。想想这些城里的家长，有相当部分实际上是高考制度的受益者。这与书中讨论的问题难道不是一个么？历史总是惊人的相似。</p><h3 id="第五节：“北方的门第”"><a href="#第五节：“北方的门第”" class="headerlink" title="第五节：“北方的门第”"></a>第五节：“北方的门第”</h3><blockquote><p>故南士借上以凌下，北族则附下以抗上。</p></blockquote><p>真正的利害在于存在的合法性。南方士族与皇族是相互依存的，故南士的合法性在于皇族而不在民众；北方士族与异族皇族始终不是一路的，所以其合法性在于能组织民众。联想到军阀的合法性不在自身而在于能否为依附于军阀的下属们谋求利益。在真正的利害关系之前，其它因素如民族大义等，只能是少数人的追求，而从大势上讲是不足道的。</p><h2 id="第十九章“变相的封建势力之下之社会形态（上）”"><a href="#第十九章“变相的封建势力之下之社会形态（上）”" class="headerlink" title="第十九章“变相的封建势力之下之社会形态（上）”"></a>第十九章“变相的封建势力之下之社会形态（上）”</h2><h3 id="第二节：“农民身份之转变”"><a href="#第二节：“农民身份之转变”" class="headerlink" title="第二节：“农民身份之转变”"></a>第二节：“农民身份之转变”</h3><blockquote><p>李典之众自有武装，故称“部曲”</p></blockquote><p>部曲在这里指私兵，即归附之前就已经拥有一定武装力量。</p><blockquote><p>局势逐渐澄清，各地的强宗豪族，逐渐消并其势力于几个大势力之下，再建政府，这便是三国。</p></blockquote><p>于东汉末年的乱世，由于中央集权的瓦解，地方只能自建势力以在乱世图存，而待大势力兴起则各择归附，于是有所谓“三国”。</p><blockquote><p>两汉以来的农民，以公民资格自耕其地，而向政府纳租。现在是政府将无主荒田指派兵队耕种，无形中，农田的所有权，又从农民手里转移到政府去。</p></blockquote><p>读各种历史，容易受到情感影响而迷失本质，往往纠结于正义和邪恶。实际上，真正应该关注的是内里本质的东西，只有拨开表面上的掩饰才能看到事情的真相。土地的所有权在谁手里，谁的社会地位才能有一定保障，否则无论以任何借口，都无法掩盖其暴乱的事实。</p><p>无论何种正义的理由，都不能剥夺私有财产。</p><h3 id="第三节：“西晋之户调制度与官品占田制”"><a href="#第三节：“西晋之户调制度与官品占田制”" class="headerlink" title="第三节：“西晋之户调制度与官品占田制”"></a>第三节：“西晋之户调制度与官品占田制”</h3><blockquote><p>这一个制度的用意，并不是授予强宗豪族以私占的特权，乃是要把当时强宗豪族先已私占的户口及田亩括归公有，而许他们一个最高限度的私占权。</p></blockquote><p>政治的艺术在于和平演变。在当时的政治环境下，从强宗豪族手里拿回户口和土地的确难以办到，但其中的政治智慧却不容忽视。在实力欠缺的情况下，勉强坚持一些不切实际却看似政治正确的主张是没有意义的，反而容易招到反噬的效果。所以不如以量化的方式先妥协，待实力发生变化后再逐步达到目的。历史上因为校真一两个口号或主义，没有推动历史进步，反而造成短暂倒退的例子比比皆是。而那些明知这样的后果，仍然煽动无知人群的背后的人，实际唯恐天下不乱而无法造就个人英雄，其心可诛。</p><h3 id="第五节：“兵士的身份及待遇”"><a href="#第五节：“兵士的身份及待遇”" class="headerlink" title="第五节：“兵士的身份及待遇”"></a>第五节：“兵士的身份及待遇”</h3><blockquote><p>其先入士籍者得优廪，又可免役，其时则兵胜于民。渐次军旅之事，不为时重，则士伍唯以供役，又廪给日薄，其时则农胜于兵。</p></blockquote><p>识时务者为俊杰。</p><blockquote><p>要为军人谋出身，势必与贵族特权实力相冲突，如战国吴起在楚、商鞅在秦之事。</p></blockquote><blockquote><p>军人的地位，只与奴隶、罪犯相等，从军只是当苦役。</p></blockquote><p>战国为用兵之际，所以自然有人站出来提高军人的社会地位，而在相对和平的时代，不会有这样的事情。此外，看既得利益集团的人在什么行业就能看出利益分配的大头所在。</p><blockquote><p>军人的地位如此，如何可以为国宣劳，担负光复中原的重任？</p></blockquote><p>实则社会利益之分配必须与目标一致，否则要么无法达成目标，要么演变成无法控制的结果。</p><h2 id="第二十章“变相的封建势力之下之社会形态（下）”"><a href="#第二十章“变相的封建势力之下之社会形态（下）”" class="headerlink" title="第二十章“变相的封建势力之下之社会形态（下）”"></a>第二十章“变相的封建势力之下之社会形态（下）”</h2><h3 id="第二节：“北魏均田制”"><a href="#第二节：“北魏均田制”" class="headerlink" title="第二节：“北魏均田制”"></a>第二节：“北魏均田制”</h3><blockquote><p>此制用意并不在求田亩之绝对均给，只求富贵者稍有一限度，贫者亦有一最低之水平。</p></blockquote><blockquote><p>…然此等皆不足为此制深病，治史者当就大体着眼也。</p></blockquote><p>仍然体现着政治之循序渐进的思想。</p><blockquote><p>尤要者则在绝其荫冒，使租收尽归公上。</p></blockquote><p>“荫”则为逃避劳役而多户假作一户，“冒”则为丧乱中无主的土地被冒领。实际上，这两者都是针对豪强而言，普通人没有能力去荫冒。均田制使得“多户多得”，所以没有人再愿意附属在豪强上，从而独立出来。</p><blockquote><p>正租入中央国库，义租纳郡县，备水旱灾。</p></blockquote><p>那时已经有建立地方粮食储备以应对水旱灾害，实历史进步也。</p><blockquote><p>不教民战，是谓弃之。临时抽丁，皆弃之也。</p></blockquote><p>临时抓壮丁，又没有经过军事训练，实际上就是放弃他们呀！这样的情况下，军队又怎么能有战斗力呢？</p><h3 id="第三节：“西魏的府兵制”"><a href="#第三节：“西魏的府兵制”" class="headerlink" title="第三节：“西魏的府兵制”"></a>第三节：“西魏的府兵制”</h3><blockquote><p>府兵制长处，只在有挑选、有教训；而更重要的，在对兵士有善意，有较优的待遇。将此等兵队与临时的发奴为兵、谪役为兵，以及抽丁为兵相敌，自然可得胜利。</p></blockquote><p>“仁者无敌”。</p><blockquote><p>所以自行“均田”，而经济上贵族与庶民的不平等取消；自行”府兵“，而种族上胡人与汉人的隔阂取消。</p></blockquote><p>能看到社会上的主要矛盾，而实施缓和的政治手段逐步消除这种矛盾，才是为政者需要不断思考的问题。</p><blockquote><p>古之帝王所以建诸侯、立百官，非欲富贵其身而尊荣之，盖以天下至广，非一人所能独治，是以博访贤才，助己为治。若知其贤，则以礼命之。其人闻命之日，则惨然曰：“凡受人之事，任人之劳，何舍己而从人？”又自勉曰：“天生儁士，所以利时。彼人主欲与我共为治，安可苟辞？”于是降心受命。其居官也，不惶恤其私而忧其家。故妻子或有饥寒之弊而不顾。于是人主赐以俸禄、尊以轩冕而不以为患，贤臣受之亦不以为德。为君者诚能以此道授官，为臣者诚能以此情受位，则天下之大，可不言而治。后世衰微，以官职为私恩，爵禄为荣惠。君之命官，亲则授之，爱则任之。臣之受位，可以尊身而润屋者，则迂道而求之。至公之道没，而奸诈之萌生。天下不治，正为此矣。<br>今圣上中兴，思去浇伪．在朝之士，当念战事之艰难。才堪者审己而当，不堪者收短而避。使天官不妄加，王爵不虚受，则淳素之风庶几可返。</p></blockquote><p>这段话源于《周书·文帝纪下》，虽过于理想，没有真正给出实现的步骤，但仍然值得称赞。常常见到今天的创业者，仅有一点点成绩便招聘大量的员工，动则以手下的人数作为人生成功的衡量，实在好笑。</p>]]></content>
    
    <summary type="html">
    
      当信任何一国之国民，尤其是自称知识在水平线以上之国民，对其本国已往历史，应该略有所知
    
    </summary>
    
      <category term="Book Reading" scheme="liqul.github.io/blog/categories/Book-Reading/"/>
    
    
      <category term="国史大纲" scheme="liqul.github.io/blog/tags/%E5%9B%BD%E5%8F%B2%E5%A4%A7%E7%BA%B2/"/>
    
  </entry>
  
  <entry>
    <title>认知螺旋</title>
    <link href="liqul.github.io/blog/2018/04/10/2016-12-03-cycling-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2018/04/10/2016-12-03-cycling-NSConflict-liqul/</id>
    <published>2018-04-10T13:59:45.221Z</published>
    <updated>2018-04-10T13:59:45.222Z</updated>
    
    <content type="html"><![CDATA[<p>在认知事物的过程中，一个人的外在表现通常是循环往复的。最常见的情况是，起初对某个领域知之甚少，感觉到自己的无知；随着不断学习，又会在某一阶段认为自己已经掌握了大部分知识；而更进一步的深入，又会觉得在许多地方理解不到位，无知感又随之而来。这种现象，可以用螺旋式台阶来比喻，正如下图的上面部分。</p><p><img src="/blog/assets/luoxuan.png" alt="螺旋上升"></p><p>这会产生一个有趣的问题，虽然认识在逐步加深，也就好比沿着台阶不断上升，但将这些台阶投影到一个平面（如图下半部分）会发现不同高度的台阶会投影到同一个区域里，而这意味着什么呢？从内在看，这意味着某些观念或感觉的往复，正如开头的例子里无知感会反复出现；而从外在来看，则是人对外的表现形式有一定的类似。理解了这一点有两方面的好处：首先，在认知事物的过程中，不要因为出现往复感而焦虑，因为这是深入的必经之路，如果没有这种感觉反倒是有问题的；另外，更重要的是我们能够更加理性地观察和分析他人所处的状态。</p><p>如果不理解这种螺旋上升和投影的关系，当我们观察到某人的外在表现时，很容易把他们定位在错误的认知程度上。比如在学术上，大师往往能把非常复杂的问题用浅显的语言描述清楚，而一些只知道浅显原理的人也能说出类似的段子，如果不能理性观察，就容易把“浮于表面”错误看成“深入浅出”。再比如在当前的互联网公司里存在许多杂家，他们表面上什么都懂，却无一精通，其理解事物的深度有限，从而对整体的把握也是畸形的，这跟许多从业多年的人比起来，乍一眼看上去是类似的，需要多了解一点才能真正分辨出来。</p><p>看《国史大纲》里对战国时期的一系列的描述，包括废井田开阡陌、军人和商人替代贵族等等，总感觉能看到欧洲文艺复兴时期的味道，两者都是封建社会崩溃的过程。但是，为何欧洲随后横扫全球而当年“中国”只是实现了秦朝的大一统？两者类似螺旋上升的不同层次。从战国到文艺复兴，两者相隔千年，生产力在逐渐进步，因此放在战国时期，各国只能对内扩张对外兼并；放在欧洲，则各国有更强的实力去对外扩展和兼并欧洲以外更广大的世界。</p><h2 id="2017-04-15更新"><a href="#2017-04-15更新" class="headerlink" title="2017-04-15更新"></a>2017-04-15更新</h2><p>近年来中国的互联网行业大都沿袭野蛮生长的模式，其中以华为、阿里为成功的典型。虽然大家习惯认为BAT是一个梯队，但BT和A并非同一时代的企业，与那俩相比A要晚的多。野蛮生长的最大特点是少量精英+大量的廉价劳动力（即“铁打的营盘流水的兵”），最大的特点在工资低、无偿加班、强调狼性文化和价值观。原本奉行精英文化的公司（比如外企、百度），掉头学习这种野蛮模式以自救。</p><p>从时间轴上看，这只是生产力发展到一定阶段，为了迎合这种生产力而诞生的一种生产关系。随着各种技术的成熟，不再需要精英阶层从事一线生产活动，这些精英转而成为领导层去带领一些廉价劳动力完成过去看似无法完成的生产活动。然而，社会仍旧向前发展，将会进入下一阶段，而这个阶段又与上上个阶段表现出的现象类似。</p><p>简而言之，社会发展的方向是不断加深协作的深度，这将无法依赖现在的野蛮生长模式完成进化。在短期的将来，更合理的模式将是中等规模的精英间的协作。在人力资源受限的情况下，必然依赖全球的人才聚集，突破这种限制，一些高质量的开源项目正是在这种背景下完成的。适应这种状态的公司的模式不再大，依赖少量的精英人才分布式自由组合。为什么是少量呢？因为生产力的发展，已经不再需要那么多人了；为什么是分布式自由组合？因为这样的组织效率是最高的。</p><p>截止今天，仍然能看到市面上各种沿袭野蛮生长模式的公司，其中很多已经死掉了，或者在苟延残喘依赖风投的钱续命。另一些看似成功的公司，比如滴滴、膜拜、ofo等等等等，它们的命运让人拭目以待。那些近年来靠野蛮生长走向巅峰的公司，它们必将进入一轮刮骨疗毒般的清理门户。在将来的几年里，这些都值得期待~</p>]]></content>
    
    <summary type="html">
    
      在认知事物的过程中，一个人的外在表现通常是循环往复的。最常见的情况是，起初对某个领域知之甚少，感觉到自己的无知；随着不断学习，又会在某一阶段认为自己已经掌握了大部分知识；而更进一步的深入，又会觉得在许多地方理解不到位，无知感又随之而来。这种现象，可以用螺旋式台阶来比喻
    
    </summary>
    
      <category term="Thinking" scheme="liqul.github.io/blog/categories/Thinking/"/>
    
    
      <category term="认知" scheme="liqul.github.io/blog/tags/%E8%AE%A4%E7%9F%A5/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="liqul.github.io/blog/2018/04/10/hello-world/"/>
    <id>liqul.github.io/blog/2018/04/10/hello-world/</id>
    <published>2018-04-10T13:55:29.926Z</published>
    <updated>2018-04-10T13:55:29.927Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Eventual Consistency vs. Strong Consistency</title>
    <link href="liqul.github.io/blog/2018/03/16/consistency_model/"/>
    <id>liqul.github.io/blog/2018/03/16/consistency_model/</id>
    <published>2018-03-16T07:05:39.000Z</published>
    <updated>2018-04-10T14:19:32.645Z</updated>
    
    <content type="html"><![CDATA[<p>Here is a very good explanation about eventual consistency and strong consistency. I’d like to put the two figures on that page below:</p><p><img src="/blog/assets/eventual-consistency.png"><br>Fig. 1 figure for eventual consistency</p><p>In this example above, Node A is the master, which replicate X to its followers Node B and C. Suppose the time when X is successfully writen to Node A is t_1, and the time when X  is replicated to Node B is t_2. Any time between t_1 and t_2, if a client reads from Node A, it gets the latest value of X. But if the client reads from Node B, it gets an old version of X. In other words, the result of a read depends on which Node the client reads from, and therefore, the storage service presents an inconsistent global view for the client. </p><p>In contrast, if the storage service provides a strong consistency semantic, the client should always read the same result. This figure below illustrates an example of strong consistency. </p><p><img src="/blog/assets/strong-consistency.png"><br>Fig. 2 figure for strong consistency</p><p>The single difference between Fig. 1 and Fig. 2 is that before X has been successfully replicated to Node B and C, a read request of X to Node B and C should be blocked. How about reading from Node A before all replications done? It should be blocked as well, and therefore, there is a missing  ‘lock’ symbol in Fig. 2. The full picture should has the following steps:</p><ol><li>A client issues a write request of X to Node A;</li><li>Node A locks X globally to prevent any read or write to X;</li><li>Node A store X locally, and then replicate X to Node B and C;</li><li>Node B and C store X locally and send Node A a response;</li><li>After receiving from Node B and C, Node A release the lock of X and respond to the client;</li></ol><p>These steps are only used to understand the basic idea of strong consistency, which is not necessary a best practice. If you want to know more details, research some real systems such as Spanner or Kudu.</p><p>While sounds more understandable for developers, strong consistency trades Availability for Consistency. In the instance shown in Fig. 2, a client may need to wait for a while before it reads the value of X. If the networking fails apart (for example, Node C is partitioned from Node A and B), any write requests to Node A will fail if each value is forced to have 3 replications. In addition, if the global lock service fails, the storage service will also be unavailable. In general, a storage service with strong consistency has much higher requirements to the infrastructure in order to function well, and therefore, is more difficult to scale compared to one with eventual consistency.</p><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html#ConsistencyModel" target="_blank" rel="noopener">AWS S3’s consistency model</a>.</p>]]></content>
    
    <summary type="html">
    
      Understanding the differences between these two consistency models.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="distributed" scheme="liqul.github.io/blog/tags/distributed/"/>
    
      <category term="consistency" scheme="liqul.github.io/blog/tags/consistency/"/>
    
  </entry>
  
  <entry>
    <title>Reading &quot;State Management in Apache Flink&quot;</title>
    <link href="liqul.github.io/blog/2018/02/05/flink/"/>
    <id>liqul.github.io/blog/2018/02/05/flink/</id>
    <published>2018-02-05T12:47:09.000Z</published>
    <updated>2018-04-10T13:59:45.242Z</updated>
    
    <content type="html"><![CDATA[<p>Updated on 2018-02-05 </p><p>I recently read an excellent <a href="https://streaml.io/blog/exactly-once/" target="_blank" rel="noopener">blog</a> about exactly-once streaming processing. It details typical solutions for exactly-once processing used by various open source projects. No matter if the solution is based on streaming or mini-batch, exactly-once processing incurs a inevitably latency. For example in Flink, the state at each operation can only be read at each checkpoint, in order not to read something that might be rollbacked during a crash. </p><p>===</p><p>I recently read the VLDB’17 paper “State Management in Apache Flink”. In one sentence,</p><blockquote><p>The Apache Flink system is an open-source project that provides a full software stack for programming, compiling and running distributed continuous data processing pipelines.</p></blockquote><p>For me, Flink sounds yet another computation framework alternative to Spark and Mapreduce with a workflow management tool. However,</p><blockquote><p>In contrast to batch-centric job management which prioritizes reconfiguration and coordination, Flink employs a schedule-once, long-running allocation of tasks. </p></blockquote><p>How exactly does a streaming-centric framework differ from a batch-centric framework? Conceptually, there is no fundamental difference between the two. Any batch processing framework can work “like” a streaming processing framework by reducing the size of each batch to 1. However, in practice, they are indeed different. A batch-centric framework usually involve a working procedure such as </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch 1 start</span><br><span class="line">do some job</span><br><span class="line">batch 1 end</span><br><span class="line">update some state</span><br><span class="line"></span><br><span class="line">batch 2 start</span><br><span class="line">do some job</span><br><span class="line">batch 2 end</span><br><span class="line">update some state</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Note that the job is started and ended within each batch. In contrast, for a streaming-centric framework, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">start a job</span><br><span class="line"></span><br><span class="line">receiving a new data</span><br><span class="line">process the data</span><br><span class="line">update some state</span><br><span class="line">pass the data to the next job</span><br><span class="line"></span><br><span class="line">receiving a new data</span><br><span class="line">process the data</span><br><span class="line">update some state</span><br><span class="line">pass the data to the next job</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">end the job</span><br></pre></td></tr></table></figure><p>This comparison is clear. A job in the streaming-centric framework usually work continuously without being started/stopped multiple times as in a batch-centric framework. Starting and stopping a job usually incur some cost. Therefore, a batch-centric framework usually performs less efficiently compared to a streaming-centric one. Additionally, if the application is mission critical (e.g., malicious event detection), processing data in batch usually means high latency. However, if the task is batch-by-batch in nature, a batch-centric framework usually performs as efficiently as a streaming-centric one. </p><p>Another problem is about snapshotting. Snapshotting is a key capability for a processing pipeline. A snapshot is consist of both the state and data. The global state of the pipeline is composed of the sub-state of each operator. Each state is either a <em>Keyed state</em> or a <em>Operator state</em>. The former represents all type of states indexed by the key from data (e.g., count by key); the latter is more an operator-aware state (e.g., the offset of data). Snapshotting the data is tricky where Flink assumes that </p><blockquote><p>Input data streams are durably logged and indexed externally allowing dataflow sources to re-consume their input, upon recovery, from a specific logical time (offset) by restoring their state. This functionality is typically provided by file systems and message queues such as Apache Kafka</p></blockquote><p>Each operator snapshots the current state once processing a mark in the dataflow. With the marks and the snapshotted states of each operator, we can always restore the system state from the last snapshot. One should note that the keyed state is associated with an operator, and therefore, the data with the same key should be physically processed at the same node. Otherwise, there should be a scalability issue. Consequently, there should be a shuffle before such operators, or the data is already prepared to ensure data with the same key is processed at a single node.</p><p>In conclusion, Flink is great as streaming-centric frameworks have some fundamental advantages over batch-centric frameworks. However, since batch-centric frameworks such as Mapreduce and Spark are already widely applied, there should be really strong motivations to migrate existing systems to this new framework. Moreover, the implementation quality and contributor community are two very important facts for the adoption of a new born framework, while Spark has been a really popular project. Maybe, a higher level project such as the <a href="https://beam.apache.org/" target="_blank" rel="noopener">Apache Beam</a> is a good direction. Beam hides the low-level execution engine by unifying the interface. Any application written in Beam is then compiled to run on an execution engine such as Spark or Flink. </p>]]></content>
    
    <summary type="html">
    
      Understanding the concepts in Apache Flink.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="flink" scheme="liqul.github.io/blog/tags/flink/"/>
    
  </entry>
  
  <entry>
    <title>Notes on MR memory issues</title>
    <link href="liqul.github.io/blog/2018/02/05/2017-07-03-experience_with_mr_memory_parameters-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2018/02/05/2017-07-03-experience_with_mr_memory_parameters-NSConflict-liqul/</id>
    <published>2018-02-05T03:29:09.000Z</published>
    <updated>2018-04-10T13:59:45.239Z</updated>
    
    <content type="html"><![CDATA[<p>Updated on 2018-02-05</p><p>I recently encountered several OOMs from mapper tasks reading parquet files. The yarn container is killed due to running out of physical memory. Since I already set the JVM memory to 0.8 of the container size, I’m pretty sure that this is due to off-heap memory allocation issues. I found the two jira issues <a href="https://issues.apache.org/jira/browse/SPARK-4073" target="_blank" rel="noopener">here</a> and <a href="https://issues.apache.org/jira/browse/PARQUET-118" target="_blank" rel="noopener">here</a>, pointing me to the snappy codec used by parquet for decompression. There aren’t so much I can do except allocating more memory beside the JVM.  </p><p>===</p><p>I recently experienced two OOM problems running a mapreduce application. The MR application reads from a group of parquet files, shuffles the input rows, and writes into parquet files, too. </p><p>The first OOM is thrown by the mapper with error logs look like following</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2017-06-22 09:59:10.978 STDIO [ERROR] [WORKER] [129] Container [pid=14638,containerID=container_e26_1495868456939_0784_01_000066] is running beyond physical memory limits. Current usage: 1.0 GB of 1 GB physical memory used; 1.5 GB of 2.1 GB virtual memory used. Killing container.</span><br><span class="line">Dump of the process-tree for container_e26_1495868456939_0784_01_000066 :</span><br><span class="line">    |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE</span><br><span class="line">    |- 14638 14632 14638 14638 (bash) 0 0 17096704 774 /bin/bash -c /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN  -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242 1&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stdout 2&gt;/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/stderr  </span><br><span class="line">    |- 14655 14638 14638 14638 (java) 4654 290 1616650240 272880 /usr/lib/jvm/java-7-oracle-cloudera/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx1024m -Djava.io.tmpdir=/disk1/yarn/nm/usercache/hdfs/appcache/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/disk2/yarn/container-logs/application_1495868456939_0784/container_e26_1495868456939_0784_01_000066 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Dhadoop.root.logfile=syslog org.apache.hadoop.mapred.YarnChild 192.168.130.123 46432 attempt_1495868456939_0784_m_000020_1 28587302322242</span><br></pre></td></tr></table></figure><p>After some investigation, I realized this is due to a misconfiguration of the mapper container memory limit (mapreduce.map.memory.mb) and the mapper JVM memory limit (mapreduce.map.java.opts). Basically, the latter should be smaller than the former, because the mapper container consumes some memory itself. After setting mapreduce.map.java.opts = mapreduce.map.memory.mb * 0.8, the OOM problem is gone. I note that this also applies for the reducer, which has two corresponding parameters (mapreduce.reduce.java.opts and mapreduce.reduce.memory.mb). This <a href="https://discuss.pivotal.io/hc/en-us/articles/201462036-MapReduce-YARN-Memory-Parameters" target="_blank" rel="noopener">article</a> explains nicely.</p><p>The second OOM issue is much harder to address, which comes with the shuffle phase. I saw error logs like following</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">2017-06-21 20:22:42.294 STDIO [ERROR] [WORKER] [100] Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#1</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)</span><br><span class="line">    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    at javax.security.auth.Subject.doAs(Subject.java:415)</span><br><span class="line">    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)</span><br><span class="line">    at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56)</span><br><span class="line">    at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:514)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)</span><br><span class="line">    at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)</span><br></pre></td></tr></table></figure><p>This is not an old problem which could be found in <a href="https://issues.apache.org/jira/browse/MAPREDUCE-6447" target="_blank" rel="noopener">here</a> and <a href="https://issues.apache.org/jira/browse/MAPREDUCE-6108" target="_blank" rel="noopener">here</a>. Most of the solutions suggest tuning the three parameters:</p><ul><li>mapreduce.reduce.shuffle.input.buffer.percent (default 0.7): how much memory shuffle can use to store data pulled from mappers for in-memory sort.</li><li>mapreduce.reduce.shuffle.memory.limit.percent (default 0.25): how much memory each shuffle thread uses for pulling data from mappers into memory. </li><li>mapreduce.reduce.shuffle.parallelcopies (default 10): the number of shuffle thread can run in parallel</li></ul><p>Some solutions claims that we should have </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent * mapreduce.reduce.shuffle.parallelcopies &lt; 1</span><br></pre></td></tr></table></figure><p>which is actually not correct. MergeManager allocates memory to shuffle threads which is used for copying mapper output into memory. Each time a shuffle thread applies for a copy action, the MergeManager determines if the application is granted by checking (1) if the appliedMemory size is more than the max memory each shuffle thread can have. This is controlled by mapreduce.reduce.shuffle.input.buffer.percent * mapreduce.reduce.shuffle.memory.limit.percent. Suppose the reducer JVM has 3.5G heap size, each shuffle can apply no more than 3500*0.7*0.25=612M with default settings. (2) if the usedMemory is more than memoryLimit. The used memory accounts for memory used by shuffles and in-memory merge. The memory limit is calculated by 3.5*0.7 = 2.45G with 3.5G JVM heap size. Now, if the usedMemory is 2.44G and appliedMemory is 612M, the real memory used by shuffle could be more than 3G !!! </p><p>This is not a bug, since there is a detailed comments in MergeManagerImpl.reserve. The comments explain why the actually used memory could be one shuffle larger than the limit. From the other side, this could cause OOM. Due to this issue, there’s no 100% safe way to fix the OOM by tuning the parameters. We can only mitigate this problem by reducing mapreduce.reduce.shuffle.input.buffer.percent and/or mapreduce.reduce.shuffle.memory.limit.percent. One should carefully calculate these parameters according to the real workload. Especially, the memory each shuffle can use limit the max size of output from each mapper. For example, if the mapper produces a 300M intermediate file, the shuffle should be able to allocate memory more than 300M. Otherwise, all sort will be done on disk. </p><p>One more thing is about the parquet format. It is a highly compressed format, and therefore the decompressed mapper output is much larger than the input split size. I think this is why OOM happens more frequently for parquet files than other file formats. </p>]]></content>
    
    <summary type="html">
    
      I recently experienced two OOM problems running a mapreduce application. The MR application reads from a group of parquet files, shuffles the input rows, and writes into parquet files, too. 
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="Mapreduce" scheme="liqul.github.io/blog/tags/Mapreduce/"/>
    
      <category term="OOM" scheme="liqul.github.io/blog/tags/OOM/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the SSD</title>
    <link href="liqul.github.io/blog/2017/12/07/ssd/"/>
    <id>liqul.github.io/blog/2017/12/07/ssd/</id>
    <published>2017-12-07T12:47:09.000Z</published>
    <updated>2018-04-10T13:59:45.245Z</updated>
    
    <content type="html"><![CDATA[<p>Reading the chapter 13.5 “Arranging data on disk” in the book “DATABASE SYSTEM: IMPLEMENTATION” makes me think of a question: How data should be arranged on a SSD (Solid-State Drive)? This is indeed an old question, so after doing some research with Google, I find some very good explanations. </p><p><a href="http://site.aleratec.com/blog/2011/09/22/overview-pages-blocks-ftls-solidstate-drive-ssd/" target="_blank" rel="noopener">An Overview of Pages, Blocks and FTLs in a Solid-State Drive (SSD)</a></p><p><a href="https://www.extremetech.com/extreme/210492-extremetech-explains-how-do-ssds-work" target="_blank" rel="noopener">How Do SSDs Work?</a></p><p>The two articles above describes how SSD works differently from a HDD. Some key points to take away are:</p><ul><li>The minimum read/write unit for a SSD is a <em>page</em>. A <em>block</em> is made up of a set of pages.</li><li>A dirty page (with data) can <em>not</em> be overwritten before being erased.</li><li>The minimum erase unit for a SSD is a <em>block</em>.</li><li>Each block has a finite program/erase cycles (P/E cycles).</li></ul><p>Within a SSD, data can only be erased by block. <em>Garbage collection</em> need to run to reclaim logically deleted pages (e.g., due to update). Therefore, data in blocks with deleted pages are packed and rewrite to another empty block. A piece of data might be rewritten over and over again, which is called the <em>write amplification</em> problem. This also leads to the fact that data is moving constantly which is quite different from data stored within a HDD.</p><p><a href="https://blog.2ndquadrant.com/tables-and-indexes-vs-hdd-and-ssd/" target="_blank" rel="noopener">Tables and indexes vs. HDD and SSD</a></p><p>This article above discussed about the strategy of storing table data and indexes on HDD vs. SSD. The results are clearly shown by those charts. Also, the discussion in the comments is worthwhile for reading. </p><p><a href="http://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/" target="_blank" rel="noopener">Coding for SSDs</a></p><p>Finally, I found a very interesting serial of blogs “Coding for SSDs”. The author built a key-value store optimized for SSDs. There are quite a lot of insights in these blogs. </p><p>In conclusion, SSDs outperform HDDs from almost every aspects today, except the price per bit. However, I envision that in the near future, the price could be made low enough to replace most HDDs. SSDs are almost drop-in replacement for HDDs. However, to get the best performance from SSDs, developers do need to take care about the data access characteristics of SSDs. </p>]]></content>
    
    <summary type="html">
    
      Understanding the characteristics of SSD.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="ssd" scheme="liqul.github.io/blog/tags/ssd/"/>
    
  </entry>
  
  <entry>
    <title>Things about replication in Elasticsearch</title>
    <link href="liqul.github.io/blog/2017/11/10/things-about-replication-in-elasticsearch/"/>
    <id>liqul.github.io/blog/2017/11/10/things-about-replication-in-elasticsearch/</id>
    <published>2017-11-10T03:33:09.000Z</published>
    <updated>2018-04-10T13:59:45.246Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Updated-on-2017-11-10"><a href="#Updated-on-2017-11-10" class="headerlink" title="Updated on 2017-11-10"></a>Updated on 2017-11-10</h3><p>Elasticsearch is evolving fast in the past few years. There have been quite some discussions on data loss during node crashes, which can be found <a href="https://github.com/elastic/elasticsearch/issues/10933" target="_blank" rel="noopener">here</a> and <a href="https://github.com/elastic/elasticsearch/issues/14252" target="_blank" rel="noopener">here</a>. Most of the issues have been fixed as described <a href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html" target="_blank" rel="noopener">here</a>. However, since Elasticsearch carried out a major upgrade to version 5+, some serious issues still remain for low versions, e.g., the stale replica problem described <a href="https://github.com/elastic/elasticsearch/issues/14671" target="_blank" rel="noopener">here</a>. </p><p>I already discussed in the original article about the two node issue. I recently carried out an experiment with 3 nodes which is actually the recommended minimum size for an Elasticsearch cluster. With 3 nodes, the quorum size is 2 and the minimum master nodes is 2 (discovery.zen.minimum_master_nodes). Therefore, there is always an overlap where some nodes have the latest state. Let me explain this with an example. The nodes are A, B, and C. We go through the following test steps:</p><ol><li>Create a new index with 2 replicas, i.e., 3 copies in total;</li><li>Shut down A;</li><li>Index 1 document on index B and C successfully;</li><li>Shut down B and C;</li><li>Turn on A;</li></ol><p>What about the state for the index? The replica on A will not be allocated as the primary shard since there is only one alive node less than the minimum master nodes 2. Now, we turn on B. As B has the latest state, B propagate the latest state to A. </p><p>Most of open sourced distributed system rely on a mature consensus approach such as Raft or Zookeeper. However, Elasticsearch decided to invent its own. This actually leads to most of those serious issues. </p><p>========</p><p>Replication is a key feature for Elasticsearch from two aspects: (1) When some machines fail, the alive ones can still serve requests; (2) Replication boosts the read speed since read can retrieve data from multiple nodes simultaneously. </p><p>Elasticsearch follows a primary-secondary fashion for replication. When a write request (e.g., create, index, update, delete) arrives, it is first forward to the primary replica. The primary replica finishes the request, and then, concurrently forward the requests to all alive secondary replicas. There are a few details about this process. </p><p>First, there is a concept of write consistency level in Elasticsearch, with available options one, quorum, and all. This concept is a bit different from what we normally find for other systems such as Kafka. It barely forces the primary replica to check if there are enough alive replicas available receiving a write request. For instance, suppose we have a cluster of 3 nodes with replica number 2, i.e., each shard is with one primary replica and 2 secondary replicas. If we set the write consistency level to quorum, when the primary replica receives a index request, it checks if there are at least 2 replicas available (i.e., &gt;=replicas/2+1). If the check passes, the primary replica will start the action of index, after which it forward the request to all replicas. One should note that the consistency level is only for the check. This means there is a chance when a replica fails after the check, and right before the action.</p><p>Second, we need to answer the question: when shall the primary replica respond to the client? It turns out that there are two modes, sync and async as discussed <a href="https://discuss.elastic.co/t/es-default-async-or-sync/19654" target="_blank" rel="noopener">here</a>. The sync mode means the primary replica only responds the client if “all” secondary replicas have finished the request. Note that the “all” here, which has nothing to do with the selected write consistency level. Under the async mode, the primary replica responds to the client right after itself finishing the request. The request is then forward to other replicas in an async way. This accelerate the response timing for the client, which however may lead to overload for the Elasticsearch cluster. Mean while, since the request propagates eventually to the replicas, there will be no read-write consistency guarantee even inside the same session if the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-preference.html" target="_blank" rel="noopener">read preference</a> is not set to primary. </p><p>In normal case, there is only one primary replica for each shard. Once the primary replica fails, a secondary replica is elected to serve as primary. In some special situations, the primary replica may lose connection to other replicas, leading to multiple primary replicas in the system, which is called the split brain problem as discussed <a href="https://qbox.io/blog/split-brain-problem-elasticsearch" target="_blank" rel="noopener">here</a>. The cue to this problem is by setting the discovery.zen.minimum_master_nodes to &gt;= half of nodes + 1. For example, if you have 3 nodes, the minimum_master_nodes should be set to 2. By setting the minimum_master_nodes we ensure that the service is only available if there are more than minimum_master_nodes living nodes within one master domain. In other words, there can not be two masters in the system. </p><p>Finally, I want to discuss the problem of stale shard which I read recently from <a href="https://www.elastic.co/blog/tracking-in-sync-shard-copies" target="_blank" rel="noopener">here</a>. Let’s start by use a concrete example. Say if we have two nodes and each shard has two replicas (one primary and the other secondary). We first index 10 documents with the secondary shard node turned off. Then, we turn off the primary shard node, and bring up the secondary shard node. The question here is whether the secondary shard will be promoted to primary? If it is, how about the 10 documents we indexed before? According to this <a href="https://www.elastic.co/blog/tracking-in-sync-shard-copies" target="_blank" rel="noopener">blog</a>, with Elasticsearch v5+, the primary shard will not only do the index, but also inform the master about in-sync shards. In this case, the answer to our questions are no. Because the secondary shard is not in in-sync state after being brought up. I didn’t experiment it myself about this since I don’t have a Elasticsearch v5+ environment. I only tested this with Elasticsearch 2.4.5 where I found different answer. After secondary shard node was brought up, the secondary shard was indeed promoted to primary, and the 10 documents were lost if I then brought up the previous primary shard node. This is indeed a problem if such special situation happens, which however should be quite rare in practice especially if you have more than 2 nodes, and with quorum write consistency level. </p>]]></content>
    
    <summary type="html">
    
      Notes on what I learn about replication in Elasticsearch.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="replication" scheme="liqul.github.io/blog/tags/replication/"/>
    
      <category term="consistency" scheme="liqul.github.io/blog/tags/consistency/"/>
    
      <category term="elasticsearch" scheme="liqul.github.io/blog/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>读《洪业：清朝开国史》有感</title>
    <link href="liqul.github.io/blog/2017/10/28/ming_lessons/"/>
    <id>liqul.github.io/blog/2017/10/28/ming_lessons/</id>
    <published>2017-10-28T12:47:09.000Z</published>
    <updated>2018-04-10T13:59:45.242Z</updated>
    
    <content type="html"><![CDATA[<p>读<a href="https://www.amazon.cn/%E6%B4%AA%E4%B8%9A-%E6%B8%85%E6%9C%9D%E5%BC%80%E5%9B%BD%E5%8F%B2-%E9%AD%8F%E6%96%90%E5%BE%B7/dp/B06XFTC5TJ/ref=sr_1_1?ie=UTF8&amp;qid=1509194106&amp;sr=8-1&amp;keywords=%E6%B4%AA%E4%B8%9A" target="_blank" rel="noopener">《洪业：清朝开国史》</a>关于崇祯的一些感受。</p><h2 id="魏忠贤问题"><a href="#魏忠贤问题" class="headerlink" title="魏忠贤问题"></a>魏忠贤问题</h2><ul><li>上策：保持互相制衡，两方敲打，改革弊政</li><li>中策：无所作为</li><li>下策：杀魏忠贤导致文臣势力过大</li></ul><h2 id="皇太极的问题"><a href="#皇太极的问题" class="headerlink" title="皇太极的问题"></a>皇太极的问题</h2><ul><li>上策：联络岱善，内部瓦解后金统治阶级</li><li>中策：同意与皇太极议和，攘外必先安内，剿灭李自成</li><li>下策：同时面对两股敌人</li></ul><h2 id="用人的问题"><a href="#用人的问题" class="headerlink" title="用人的问题"></a>用人的问题</h2><ul><li>上策：黑猫白猫，不在意细节，以能力取人</li><li>中策：坚持用人时间长一点，不随意更替</li><li>下策：动辄得咎，反复无情，换人如流水</li></ul><h2 id="自杀的问题"><a href="#自杀的问题" class="headerlink" title="自杀的问题"></a>自杀的问题</h2><ul><li>上策：未知</li><li>中策：南下或北上。南下学宋高宗，虽末世无法与南宋比肩，但仍有一战的实力；北上联系后金，一同剿灭李自成，有崇祯在后金不那么容易南侵，何况还有吴三桂</li><li>下策：自杀身死，连太子也没有放过</li></ul>]]></content>
    
    <summary type="html">
    
      读《洪业：清朝开国史》有感。
    
    </summary>
    
      <category term="Book Reading" scheme="liqul.github.io/blog/categories/Book-Reading/"/>
    
    
      <category term="洪业" scheme="liqul.github.io/blog/tags/%E6%B4%AA%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>Quorum in Amazon Aurora</title>
    <link href="liqul.github.io/blog/2017/10/27/quorum_in_amazon_aurora/"/>
    <id>liqul.github.io/blog/2017/10/27/quorum_in_amazon_aurora/</id>
    <published>2017-10-27T08:12:09.000Z</published>
    <updated>2018-04-10T13:59:45.244Z</updated>
    
    <content type="html"><![CDATA[<p>I recently read a serial of posts about the quorum mechanism in Amazon Aurora, which is a distributed relational database. These posts are: </p><ul><li><a href="/blog/assets/Amazon Aurora under the hood_ quorums and correlated failure _ AWS Database Blog.pdf">post1</a>: quorums and correlated failure.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Quorum Reads and Mutating State _ AWS Database Blog.pdf">post2</a>: quorum reads and mutating state.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Reducing Costs Using Quorum Sets _ AWS Database Blog.pdf">post3</a>: reducing costs using quorum sets.</li><li><a href="/blog/assets/Amazon Aurora Under the Hood_ Quorum Membership _ AWS Database Blog.pdf">post4</a>: quorum membership.</li></ul><p>Besides, there is actually a Sigmod’17 paper about Amazon Aurora which could be found <a href="http://www.allthingsdistributed.com/files/p1041-verbitski.pdf" target="_blank" rel="noopener">here</a>. I only briefly went through that paper which spends most of words talking about the basic architecture. </p><p>I like this serial of posts which is a very good tutorial if you want to learn practical usage of quorum. By definition, a quorum model is </p><blockquote><p>Formally, a quorum system that employs V copies must obey two rules. First, the read set, Vr, and the write set, Vw, must overlap on at least one copy.</p></blockquote><blockquote><p>Second, you need to ensure that the quorum used for a write overlaps with prior write quorums, which is easily done by ensuring that Vw &gt; V/2. </p></blockquote><p>At the heart of this model is that each read/write to the cluster of nodes overlaps at least one node with each other. </p><p>While it is cool to enjoy the replication benefit with the quorum model, there comes cost for both read and write. For read, a client may need to consult multiple nodes (i.e., the read set) in order to ensure reading the latest state. For write, the multiple copies need to be materialized in order to maintain the quorum model. The author introduced the basic ideas of solving these two problems in post2 and post3. Especially, for the read penalty, the master maintains a cache of the status of all successful replicas, including their latency estimations. Therefore, a client need only to find information from the master in order to read the latest information. </p><p>Membership management is discussed in post4 where they use the approach of overlapping quorums to solve the node failure problem. One nice feature is that this approach is robust given new failures happening right during the handling process. </p><p>Finally, I’d like to end up with the following sentence from the posts:</p><blockquote><p>State is often considered a dirty word in distributed systems—it is hard to manage and coordinate consistent state as you scale nodes and encounters faults. Of course, the entire purpose of database systems is to manage state, providing atomicity, consistency, isolation, and durability (ACID). </p></blockquote>]]></content>
    
    <summary type="html">
    
      Notes on the quorum mechanism in Amazon Aurora.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="quorum" scheme="liqul.github.io/blog/tags/quorum/"/>
    
      <category term="amazon aurora" scheme="liqul.github.io/blog/tags/amazon-aurora/"/>
    
  </entry>
  
  <entry>
    <title>Reading the New Apache HBase MOB Compaction Policy</title>
    <link href="liqul.github.io/blog/2017/08/29/new_apache_hbase_mob_compaction_policy/"/>
    <id>liqul.github.io/blog/2017/08/29/new_apache_hbase_mob_compaction_policy/</id>
    <published>2017-08-29T02:19:09.000Z</published>
    <updated>2018-04-10T13:59:45.243Z</updated>
    
    <content type="html"><![CDATA[<p>In case you want to understand more on MOB (Moderate Object Storage), you may refer to this <a href="https://issues.apache.org/jira/browse/HBASE-11339" target="_blank" rel="noopener">issue</a>. Basically, hbase was first introduced with capability of storing mainly small objects (&lt;100k). Moderate objects stand for files from 100k to 10m. </p><p>Recently, there is a <a href="https://blog.cloudera.com/blog/2017/06/introducing-apache-hbase-medium-object-storage-mob-compaction-partition-policies/?elqTrackId=2a7ed08f6935464e84b51ad5a8f15cb2&amp;elq=896612af50b741d7b8bf576ac30276e4&amp;elqaid=4662&amp;elqat=1&amp;elqCampaignId=2850" target="_blank" rel="noopener">blog</a> introducing the new compaction policy for MOB files. The problem with the initial approach is multiple compaction. For instance, the goal is to compact the objects created in one calendar day into one big file. The compaction process starts after the first hour. The objects created in the first hour are compacted into a temporal file. Then, the objects created in the second hour, and the temporal file created for the first hour are compacted into a new temporal file…</p><p>In this way, finally, all objects created in one day is compacted into one file. However, the objects in the first hour is compacted quite a few of times, wasting IO. The new method is based on partition. For instance, we may compact the objects in each hour of day, which is the first stage. Then, the temporal files in each hour are compacted into the final file, which is the second stage. This saves a lot of IO in comparison with the initial approach. Actually, this improvement is quite straightforward. </p><p>What I found really insightful is about the compaction partitioned by the created time. Note that the creation time of each object is never changed during its life time. Therefore, suppose a set of objects is compacted into a big file which say contains objects between 2017-08-23 ~ 2017-08-24. After a while, some objects in that set may be deleted (with tombstone in hbase), or replaced with newer versioned metadata. However can we remove these objects physically? The answer is easy. We search for all objects created between 2017-08-23 ~ 2017-08-24, which should result in a subset of the original set of objects. We then extract the remain objects into a new big file, and delete the old big file. There are two other essential points to achieve the clear process described above: (1) the metadata should be 1:1 mapping with the objects. In other words, there should be no more than 1 metadata pointing to the same object. (2) the creation time and the pointer to file should be always updated atomically. </p>]]></content>
    
    <summary type="html">
    
      Notes on the new apache hbase mob compaction policy.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="hbase" scheme="liqul.github.io/blog/tags/hbase/"/>
    
      <category term="compaction" scheme="liqul.github.io/blog/tags/compaction/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Chain Replication</title>
    <link href="liqul.github.io/blog/2017/07/28/2017-07-28-notes-chain-replication/"/>
    <id>liqul.github.io/blog/2017/07/28/2017-07-28-notes-chain-replication/</id>
    <published>2017-07-28T10:05:39.000Z</published>
    <updated>2018-04-10T13:59:45.241Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/blog/assets/chain_replication.svg" alt="Chain Replication"></p><p>I learned the idea of chain replication from <a href="https://github.com/hibari/hibari" target="_blank" rel="noopener">hibari</a>,</p><blockquote><p>Hibari is a production-ready, distributed, ordered key-value, big data store. Hibari uses chain replication for strong consistency, high-availability, and durability. Hibari has excellent performance especially for read and large value operations.</p></blockquote><p>The term “strong consistency” indeed caught my attention as I already know a few key-value storage services with only eventually consistency, e.g., openstack swift. I read its doc to find out the key tech sitting in the core is called “chain replication”. I did some investigation about this concept which actually back to very early days in 2004 in a OSDI <a href="http://www.cs.cornell.edu/home/rvr/papers/OSDI04.pdf" target="_blank" rel="noopener">paper</a>. </p><p>The idea is actually very easy to understand. The service maintains a set of chains. Each chain is a sequence of servers, where one server is called the <em>head</em>, and one is called the <em>tail</em>; all servers in between are <em>middle</em> servers. The figure in the very beginning shows such an example with two middle servers. Each write request is directed to the head server, and the update is pipelined from the head server to the tail server though the chain. Read requests are directed to only tail servers. What a client can read from the chain is definitely replicated across all servers belonging to the chain, and therefore, strong consistency is guaranteed. </p><p>Though the idea sounds straightforward, there are few practical issues. First of all, the traffic load at tail servers is higher than other servers, since they handle both write and read traffics. A load balancing aware chain organization algorithm is needed to balance the load across all servers. For instance, one server may be middle server of one chain and meanwhile tail server of another chain (see Fig. 3 in the <a href="http://www.snookles.com/scott/publications/erlang2010-slf.pdf" target="_blank" rel="noopener">Hibari paper</a>). Another problem is failure handling. There should be a way of detecting failed servers, which turns out to be non-trivial in such distributed world. There are also plenty of issues about recovering from failures, replication, and migration. In conclusion, this “simple” idea comes with a bunch of tough issues. </p><p>There are only few open source projects based on chain replication, such as <a href="https://github.com/hibari/hibari" target="_blank" rel="noopener">Hibari</a> and <a href="https://github.com/CorfuDB/CorfuDB" target="_blank" rel="noopener">CorfuDB</a>. One fundamental reason may be the cost paid for strong consistency is too high. One killer application for object storage is handling highly massive objects such as user data in social network companies. However, the chain can never cross data centers in order for low latency. The idea of using chained servers is not really new. HDFS also use a pipeline to optimize data transfer latency while achieving strong consistency. Therefore, if the number of files is not a issue, storing them directly on HDFS might be a reasonable choice, given the advantage of naive integration with other Hadoop components. </p>]]></content>
    
    <summary type="html">
    
      A brief understanding about chain replication.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="distributed" scheme="liqul.github.io/blog/tags/distributed/"/>
    
      <category term="replication" scheme="liqul.github.io/blog/tags/replication/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习笔记</title>
    <link href="liqul.github.io/blog/2017/07/07/2017-07-07-notes-learning-spark/"/>
    <id>liqul.github.io/blog/2017/07/07/2017-07-07-notes-learning-spark/</id>
    <published>2017-07-07T11:19:09.000Z</published>
    <updated>2018-04-10T13:59:45.240Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark与Scala"><a href="#Spark与Scala" class="headerlink" title="Spark与Scala"></a>Spark与Scala</h2><p>在学习Spark之前务必对Scala有所理解，否则面对完全陌生的语法是很痛苦的。</p><p>Scala的一种入门方式是：</p><ol><li>学习<a href="https://www.coursera.org/learn/progfun1/home/welcome" target="_blank" rel="noopener">Scala 函数式程序设计原理</a>。这是Scala作者自己开的课程。没什么比语言作者更加能理解这门语言的了，是切入Scala编程的最好入门方式。课程习题参考了《计算机程序的构造和解释》一书，非常经典。</li><li>阅读《Scala in depth》一书，对一些Scala的重点概念有更加详细的讨论。</li><li>根据特定的topic，Google各种网络资料。</li></ol><h2 id="RDD-Resilient-Distributed-Datasets"><a href="#RDD-Resilient-Distributed-Datasets" class="headerlink" title="RDD (Resilient Distributed Datasets)"></a>RDD (Resilient Distributed Datasets)</h2><h3 id="RDD的含义"><a href="#RDD的含义" class="headerlink" title="RDD的含义"></a>RDD的含义</h3><p>RDD是spark中用于记录数据的数据结构。根据具体的RDD类型，数据有不同的组织形式。一个RDD包含多个partition，partition是并行的基本单位。RDD可能存在内存中，也可能存在硬盘里，或者两者皆有。一个RDD可以由数据源创建，也可能由其它RDD计算得到，所有参与计算RDD的RDD称为父RDD。若对mapreduce有所了解，可以把partition看作mapper的一个split。</p><h3 id="RDD中的窄依赖（Narrow-Dependency）和宽依赖（Wide-Dependency）"><a href="#RDD中的窄依赖（Narrow-Dependency）和宽依赖（Wide-Dependency）" class="headerlink" title="RDD中的窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）"></a>RDD中的窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</h3><p>若一个RDD中每一条记录仅仅依赖父RDD中唯一一条记录，则其为窄依赖，否则为宽依赖。比如在map中，每一条子RDD中的记录就对应着唯一父RDD中的对应记录。而groupByKey这样的操作中，子RDD中的一条记录，我们并不知道它究竟来自哪个父RDD中的哪个partition。</p><p>利用mapreduce的概念来理解，一组连续的窄依赖操作可以用一个mapper来实现，而宽依赖操作则只能依赖reducer。正因如此，一组连续窄依赖中产生的“中间结果”（实际并不需要产生这些中间结果）是没有存在的意义的，只要知道输入、操作就能直接计算输出了。举个具体的例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resRDD = srcRDD.map(_ + <span class="number">1</span>).map(_ + <span class="number">2</span>).filter( _ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>中的transformation链可以看作mapreduce下的一个mapper，一条记录从左到右执行不依赖其它记录。若把上面例子改为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> resRDD = srcRDD.map(_ + <span class="number">1</span>).distinct().filter( _ % <span class="number">2</span> == <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>其中加入了distinct意味着一条记录从左到右无法利用一个mapper就完成，必须截断加入一个reducer。这里需要理解mapreduce中的一个mapper并不是等价于spark中的一个map操作，而是对应所有窄操作的组合，例如filter、flatMap、union等等。</p><blockquote><p>补充材料：<a href="https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=67043332" target="_blank" rel="noopener">why spark’s mapPartitions transformation is faster than map</a>。其中的一句话讲的非常清楚——you probably know that “narrow” transformations/tasks happen independently on each of the partitions. 即窄操作在单机即可完成，不需要依赖保存在其它主机上的partition。</p></blockquote><h3 id="RDD中persist和checkpoint的逻辑"><a href="#RDD中persist和checkpoint的逻辑" class="headerlink" title="RDD中persist和checkpoint的逻辑"></a>RDD中persist和checkpoint的逻辑</h3><p>persist的目的是为了加快后续对该RDD的操作；checkpoint的目的是为了减少长执行链失败带来的开销。由于目的不同，如果persist的RDD丢失了，可以重新计算一遍（这就是普通cache的基本逻辑）。反过来，如果checkpoint丢失了，则无法重新计算，因为该checkpoint之前的内容都遗忘了。cache只是persist的一个子操作，其storage level为memory_only。</p><p>persist和checkpoint都是异步操作，执行persist或checkpoint命令仅仅给对应的RDD加上一个mark，后续交给block manager完成具体的物化操作（？？？）。persist有多种storage level，包括memory, off heap memory, disk等等。在spark中，block manager负责所有的数据存储管理，包括persist、checkpoint、或shuffle产生的中间数据等。</p><p>值得一提的是关于off heap memory的<a href="http://stackoverflow.com/questions/6091615/difference-between-on-heap-and-off-heap" target="_blank" rel="noopener">概念说明</a>。简而言之，off heap memory就是不受JVM管控的一块内存空间，由于不受管控所以不存在GC的开销；另一方面由于并非JVM native环境，所以并不能识别其中存储的Java对象这样的结构，需要序列化和反序列化来支持。off heap memory的典型应用场景则是缓存一些较大的静态数据。</p><h3 id="重要的方法"><a href="#重要的方法" class="headerlink" title="重要的方法"></a>重要的方法</h3><h4 id="compute"><a href="#compute" class="headerlink" title="compute"></a>compute</h4><p><strong>def compute(split: Partition, context: TaskContext): Iterator[T]</strong><br>根据给定的partition计算一个interator，可以遍历该partition下的所有记录。有意思的是partition的名字为split，与mapreduce下mapper的处理单位名字一样。</p><h3 id="RDD中的基础transformation"><a href="#RDD中的基础transformation" class="headerlink" title="RDD中的基础transformation"></a>RDD中的基础transformation</h3><h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><p><strong>def map[U: ClassTag](f: T =&gt; U): RDD[U]</strong><br>返回的RDD为MapPartitionsRDD类型，其compute方法会对其父RDD中的记录执行f映射。</p><h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p><strong>def mapPartitions[U: ClassTag](f: Iterator[T] =&gt; Iterator[U], preservesPartitioning: Boolean = false): RDD[U]</strong><br>与map的区别在于映射f的作用对象是整个partition，而不是一条partition中的记录。在一些初始化代价较高的场景下，mapPartition比map更加合理和高效。</p><blockquote><p>补充材料：<a href="https://martin.atlassian.net/wiki/pages/viewpage.action?pageId=67043332" target="_blank" rel="noopener">why spark’s mapPartitions transformation is faster than map</a>。</p></blockquote><h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p><strong>def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U]</strong><br>与map类似，仅仅将对iterator的map操作换成flatMap操作。这里f映射的输出类型为TraversableOnce，表示只要能完成单次遍历即可，可以是Traversable或Iterable。</p><h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p><strong>def filter(f: T =&gt; Boolean): RDD[T]</strong><br>与map类似，仅仅将对iterator的map操作换作filter操作。</p><h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p><strong>def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]</strong><br>首先这个方法存在一个implicit的参数ord，类型为scala.math.Ordering。Ordering中实现了各种基础类型（Int, Long, Double, String等）的比较方法，这意味着如果T是一种基础类型则无须实现自己的比较方法，只需要import scala.math.Ordering即可。</p><p>与前几种transformation最大的不同在于distinct依赖reduce，即它是一种宽依赖操作。其具体实现代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">map(x =&gt; (x, <span class="literal">null</span>)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)</span><br></pre></td></tr></table></figure><p>可见其首先将一条记录映射为一个pair，然后执行reduceByKey的操作。这里reduceByKey方法并非RDD所有，之所以可以调用是因为object RDD里定义了从RDD转换为PairRDDFunctions的implicit方法。这种针对特定情况下的RDD增加操作的抽象方式可以学习。reduceByKey中给出了合并两个value的方式，即把相同的key的alue合并为一个（在此为null），然后根据给定的numPartitions数量进行hash partition。最终结果通过map仅保留key即可。</p><p>与mapreduce一致，这里的合并会发生在本地和reducer处，类似mapreduce中的combiner。在调用reduceByKey后的调用逻辑为：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey((x, y) =&gt; x, numPartitions)</span><br><span class="line">combineByKeyWithClassTag(x=&gt;x, (x,y)=&gt;x, (x,y)=&gt;x, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions))</span><br></pre></td></tr></table></figure><p>在combineByKeyWithClassTag中会根据传入的三个映射分别创建createCombiner、mergeValue和mergeCombiner。其中，createCombiner用于产生合并的初始值；mergeValue用于合并两条记录；mergeCombiner用于将mergeValue得到的结果再次合并。上述三者组成一个Aggregator对象。</p><h4 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h4><p><strong>def coalesce(numPartitions: Int, shuffle: Boolean = false)(implicit ord: Ordering[T] = null) : RDD[T]</strong><br>连接的作用是重新整理原有的RDD。有两种情况：（1）若shuffle\=\=false，表示一种虚拟的RDD分区变化，此时numPartitions应该比原来的少，否则无意义。注意此时是不会发生真实的IO的；（2）若shuffle\=\=true，表示要做一次真实的shuffle，即会带有真实的数据IO。对于第二种情况，在coalesce方法内部会做一次随机的mapping操作，把每个元素与结果RDD中的partition做一次mapping。在第二种情况下，numPartitions可以比父RDD的分区数量更多。</p><p>虽然前一种情况只是虚拟的分区变化，但究竟把哪些父partition分入同一个子partition是可以考虑locality因素的，CoalescedRDD的balanceSlack参数用来控制locality在分配父partition时起的权重。</p><p>看代码中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// include a shuffle step so that our upstream tasks are still distributed</span><br><span class="line">      new CoalescedRDD(</span><br><span class="line">        new ShuffledRDD[Int, T, T](mapPartitionsWithIndex(distributePartition),</span><br><span class="line">        new HashPartitioner(numPartitions)),</span><br><span class="line">        numPartitions).values</span><br></pre></td></tr></table></figure></p><p>这段话比较难懂，而实际上是做了几件事：首先，在ShuffledRDD中根据随机生成的key将父RDD各partiton中的数据分散到子RDD的各partiton中；然后，隐式转换为PairRDDFunctions的values方法转换成普通的RDD。</p><h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p><strong>def sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = withScope</strong><br>对当前RDD的每个partition进行一次sample。withReplacement用于控制是否可出现重复sample，fraction控制sample的比例，seed即随机种子。</p><h4 id="randomSplit"><a href="#randomSplit" class="headerlink" title="randomSplit"></a>randomSplit</h4><p><strong>def randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]</strong><br>给定一组weights，例如Array(2.0,3.0,5.0)，将父RDD按这样的比例划分，得到一个子RDD数组。<br>示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.makeRDD(1 to 10,10)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at makeRDD at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res1: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) </span><br><span class="line"></span><br><span class="line">scala&gt; val randomSplittedRDD = rdd.randomSplit(Array(2.0, 3.0, 5.0))</span><br><span class="line">randomSplittedRDD: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[12] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[13] at randomSplit at &lt;console&gt;:29, MapPartitionsRDD[14] at randomSplit at &lt;console&gt;:29)</span><br><span class="line"></span><br><span class="line">scala&gt; randomSplittedRDD.foreach(x =&gt; println(x.collect.mkString(&quot; &quot;)))</span><br><span class="line">9 10</span><br><span class="line">2 4 8</span><br><span class="line">1 3 5 6 7</span><br></pre></td></tr></table></figure></p><p>其内部实现实际上是利用了BernoulliCellSampler完成的，每次把父RDD的某个partition做一次sample得到一个子partition，通过一个MapPartitionsRDD实现从父RDD到子RDD的映射。但由于产生的是一组子RDD，因此每多一个子RDD就需要把父RDD做一次sample。由于每次调用时random seed是在内部保持不变的，所以即使多次sample，也不会导致某个元素被分到不同的子RDD里去。这一点是开始一直想不通的，因为我一直以为只需要sample一遍就能完成整个过程。</p><h4 id="takeSample"><a href="#takeSample" class="headerlink" title="takeSample"></a>takeSample</h4><p><strong>def takeSample(withReplacement: Boolean, num: Int, seed: Long = Utils.random.nextLong): Array[T]</strong><br>返回指定数量的sample。</p><h4 id="union（同-）"><a href="#union（同-）" class="headerlink" title="union（同++）"></a>union（同++）</h4><p><strong>def union(other: RDD[T]): RDD[T]</strong><br>获取两个RDD的并集，若一个元素出现多次，并不会通过union操作去重，因此union本身属于窄依赖。根据partitioner的情况，分两种情况处理：（1）如果两个RDD的partitioner都定义了且相同，那两RDD的partition数量一样，得到的并集RDD也有相同数量的partition。在考虑locality时，会按照多数原则处理，即如果大多数属于某个并集partition的父partition都倾向某个locality选择，那么就以此多数为准；（2）如果不满足（1）的情况，则并集RDD的partition数量为两父RDD的数量之和，即简单的合并关系。</p><h4 id="keyBy"><a href="#keyBy" class="headerlink" title="keyBy"></a>keyBy</h4><p><strong>def keyBy[K](f: T =&gt; K): RDD[(K, T)]</strong><br>根据映射f抽取原RDD中每条记录的key，使结果RDD中每条记录为一个kv二元组。</p><h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p><strong>def sortBy[K](f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</strong><br>对RDD排序，key由映射f抽取。这个方法的实现比较有趣，如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this.keyBy[K](f)  //生成一个基于kv二元组的RDD</span><br><span class="line">        .sortByKey(ascending, numPartitions)  //sortByKey是OrderedRDDFunctions中的方法，由隐式转换rddToOrderedRDDFunctions支持</span><br><span class="line">        .values //排好序的RDD再退化由原来的元素组成，也是隐式转换支持</span><br></pre></td></tr></table></figure></p><p>实现过程经过两次隐式转换，非常有scala的特色，这种隐式转换往往发生在特殊的RDD之上。排序的具体过程参考Shuffle一节。</p><h4 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h4><p><strong>def intersection(other: RDD[T]): RDD[T]</strong><br>计算两个父RDD的交集，得到子RDD，交集元素无重复。实现如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">this.map(v =&gt; (v, null)).cogroup(other.map(v =&gt; (v, null))) //map成kv二元组后，隐式转换PairRDDFunctions调用其cogroup方法得到(k, (v1, v2))的结构</span><br><span class="line">        .filter &#123; case (_, (leftGroup, rightGroup)) =&gt; leftGroup.nonEmpty &amp;&amp; rightGroup.nonEmpty &#125;  //把两边都不是空的情况筛选出来</span><br><span class="line">        .keys //退化为普通的RDD</span><br></pre></td></tr></table></figure></p><p>其中cogroup依赖shuffle，所以是宽依赖操作。intersection操作还有一些重载，但基本实现是相同的。</p><h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p><strong>def glom(): RDD[Array[T]]</strong><br>将原来的RDD变成新的RDD，其原有的每个partition变成一个数组。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line"></span><br><span class="line">scala&gt; a.glom.collect</span><br><span class="line">res66: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9))</span><br></pre></td></tr></table></figure></p><p>这篇<a href="http://blog.madhukaraphatak.com/glom-in-spark/" target="_blank" rel="noopener">文章</a>把glom的作用讲的非常清楚。其中的例1和例2都是在处理一个数组要比挨个处理每个元素好很多的时候。当然，这消耗的内存要更大（<strong>TODO</strong>: 具体使用情况如何？是否会导致OOM？），是一个折衷。</p><h4 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h4><p><strong>def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)]</strong><br>生成当前RDD与另一个RDD的笛卡尔积，即列举所有a in this和b in other而组成的(a,b)的集合。生成的新RDD的partition数量等于原两个RDD各自的partition数量的乘积。</p><h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p><strong>def groupBy[K](f: T =&gt; K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null) : RDD[(K, Iterable[T])]</strong><br>将当前RDD中的元素按f映射的key做group操作，结果RDD可根据传入的partitioner来进行分区。源代码中有如下注释：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">* Note: This operation may be very expensive. If you are grouping in order to perform an</span><br><span class="line">* aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]]</span><br><span class="line">* or [[PairRDDFunctions.reduceByKey]] will provide much better performance.</span><br><span class="line">*</span><br><span class="line">* Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any</span><br><span class="line">* key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]].</span><br></pre></td></tr></table></figure></p><p>其中指出当前实现中一个key的所有value会需要保存在内存中，从而可能导致OOM，这可能是combine的过程中必须将所有value保存在内存中有关（推测）。另外，聚合或reduce可以解决大部分问题，而不需要groupBy，依此推测这个操作仅用于一些value较少又不得不获取这个中间结果的场景。</p><p>这篇<a href="https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html" target="_blank" rel="noopener">文章</a>很好的讲述了groupBy引入的内存问题的原因。</p><h4 id="pipe"><a href="#pipe" class="headerlink" title="pipe"></a>pipe</h4><p><strong>def pipe(command: String): RDD[String]</strong><br>pipe类似于mapreduce中的streaming，即能通过stdin来把数据发往外部进程，在通过stdout把结果读回来。这篇<a href="http://blog.madhukaraphatak.com/pipe-in-spark/" target="_blank" rel="noopener">文章</a>讲的非常清楚。但是这似乎只是map的过程，并不能包括reduce。</p><p>其内部实现实际上就是把参数中包含的command启动一个进程，然后通过stdin/out来完成上述算子操作过程。</p><h4 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h4><p><strong>def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]</strong><br>将当前RDD与other组合成一个新的包含二元组的RDD，要求两个RDD包含相同数量的partition，且每对partition包含相同数量的元素。</p><h4 id="zipPartitions"><a href="#zipPartitions" class="headerlink" title="zipPartitions"></a>zipPartitions</h4><p><strong>def zipPartitions[B: ClassTag, V: ClassTag](rdd2: RDD[B], preservesPartitioning: Boolean)(f: (Iterator[T], Iterator[B]) =&gt; Iterator[V]): RDD[V]</strong><br>与zip的关系类似map与mapPartitions的关系，但又不完全一样。zip要求对应的partition里包含的元素数量也完全一样，但这里f映射并不需要两个partiton里元素数量相同。但显然可以利用zipPartitions来实现zip的功能，且与zip比较起来应该有更好的效率。</p><h4 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h4><p><strong>def subtract(other: RDD[T], p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]</strong><br>得到在当前RDD中且不在other中的元素组成的RDD，由于需要按元素做key，属于宽依赖。</p><h4 id="DataFrame-repartion-vs-DataFrameWriter-partitionBy"><a href="#DataFrame-repartion-vs-DataFrameWriter-partitionBy" class="headerlink" title="DataFrame.repartion vs. DataFrameWriter.partitionBy"></a>DataFrame.repartion vs. DataFrameWriter.partitionBy</h4><p><strong>def repartition(numPartitions: Int, partitionExprs: Column*): DataFrame</strong><br><strong>def partitionBy(colNames: String*): DataFrameWriter</strong><br>这里的<a href="https://stackoverflow.com/questions/40416357/spark-sql-difference-between-df-repartition-and-dataframewriter-partitionby" target="_blank" rel="noopener">讨论</a>非常清楚。repartition的参数是numPartitions和partitionExprs，partitionExprs将指定的列做hash后对numPartitions求模，得到对应的partition的index。这样得到的最终分区数量是numPartitions，但实际上如果numPartitons大于分组数量，可能有一些partition是空的；反之，如果numPartitions小于分组数量，有一些partiton里包含多个分组。partitionBy是把每个partition按照指定的列拆分为一到多个文件。</p><p>一个应用实力：如果希望输出的文件里，每个文件有且仅有一个分组，那么就可以dataframe.repartiton(n, columns).write.partitionBy(columns).csv(xxx)。其中n可以控制并发的数量，跟实际的数据分布有关。</p><h4 id="zipWithUniqueId"><a href="#zipWithUniqueId" class="headerlink" title="zipWithUniqueId"></a>zipWithUniqueId</h4><p><strong>def zipWithUniqueId(): RDD[(T, Long)]</strong><br>为了解决zipWithIndex带来的性能问题，这里放松了条件，只要求id是唯一的。zipWithUniqueId只是个算子，第k个partition的元素对应的id分别为k, k+n, k+2n, …，这里的n是partition的数量。</p><h3 id="RDD中的actions"><a href="#RDD中的actions" class="headerlink" title="RDD中的actions"></a>RDD中的actions</h3><h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><p><strong>def foreach(f: T =&gt; Unit): Unit</strong><br>将映射f应用到每个元素上。</p><h4 id="foreachPartition"><a href="#foreachPartition" class="headerlink" title="foreachPartition"></a>foreachPartition</h4><p><strong>def foreachPartition(f: Iterator[T] =&gt; Unit): Unit</strong><br>将映射f应用到每个partition上。</p><h4 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h4><p><strong>def collect(): Array[T]</strong><br>将RDD中所有元素作为一个数组返回。<strong>注意不要将collect作用于一个过大的RDD，否则会抛出内存异常，可先利用take和takeSample只取一个子集</strong>。</p><h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p><strong>def reduce(f: (T, T) =&gt; T): T</strong><br>执行映射f对应的reduce操作。其操作基本步骤是：（1）每个partition执行f映射对应的reduce过程；（2）在driver的host机器上执行基于f映射的reduce过程，输入来自各个partition的输出。步骤（2）的复杂度与partition的数量呈线性增加。</p><h4 id="treeReduce"><a href="#treeReduce" class="headerlink" title="treeReduce"></a>treeReduce</h4><p><strong>def treeReduce(f: (T, T) =&gt; T, depth: Int = 2): T</strong><br>为了改进reduce里步骤（2）的瓶颈问题，对各partition的输出先逐层聚合，最后再到driver处生成最终结果，类似一棵树的聚合过程。在<a href="https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html" target="_blank" rel="noopener">文章</a>里有详细的描述。reduce和treeReduce的关系类似aggregate和treeAggregate的关系。</p><h4 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h4><p><strong>def fold(zeroValue: T)(op: (T, T) =&gt; T): T</strong><br>将映射op应用到每对元素上面。在实现过程中，spark不限定元素之间的执行顺序，实际上是先在partition内部做，然后再在partition之间，所以不能保证一个预先设定好的顺序来执行。因此，fold算子适用于那种不需要考虑左右操作元素的顺序，例如max。</p><h4 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h4><p><strong>def aggregate<a href="zeroValue: U" target="_blank" rel="noopener">U: ClassTag\</a>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U</strong><br>与fold的不同在于aggregate可以返回一个新的类型U，而不是原来的类型Ｔ。从定义的角度，fold是aggregate的一种特例。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a = sc.parallelize(1 to 9, 3)</span><br><span class="line">scala&gt; a.fold(0)&#123; _ + _ &#125;</span><br><span class="line">res0: Int = 45</span><br><span class="line"></span><br><span class="line">scala&gt; a.aggregate(0) ( _ + _, _ + _ )</span><br><span class="line">res1: Int = 45</span><br></pre></td></tr></table></figure><h4 id="treeAggregate"><a href="#treeAggregate" class="headerlink" title="treeAggregate"></a>treeAggregate</h4><p><strong>def treeAggregate<a href="zeroValue: U" target="_blank" rel="noopener">U: ClassTag</a>(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U, depth: Int = 2): U</strong><br>aggregate与treeAggregate和reduce与treeReduce的关系类似。</p><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p><strong>def count(): Long</strong><br>计算整个RDD中元素的个数。</p><h4 id="countApprox"><a href="#countApprox" class="headerlink" title="countApprox"></a>countApprox</h4><p><strong>countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble]</strong></p><p>在给定timeout期限的情况下，返回RDD中元素个数的估计。其中confidence是认为评估结果符合高斯分布的假设条件下估算的置信度，而<strong>不是结果的可信度</strong>。其核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">override def currentResult(): BoundedDouble = &#123;</span><br><span class="line">    if (outputsMerged == totalOutputs) &#123;</span><br><span class="line">      new BoundedDouble(sum, 1.0, sum, sum)</span><br><span class="line">    &#125; else if (outputsMerged == 0) &#123;</span><br><span class="line">      new BoundedDouble(0, 0.0, Double.NegativeInfinity, Double.PositiveInfinity)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      val p = outputsMerged.toDouble / totalOutputs</span><br><span class="line">      val mean = (sum + 1 - p) / p</span><br><span class="line">      val variance = (sum + 1) * (1 - p) / (p * p)</span><br><span class="line">      val stdev = math.sqrt(variance)</span><br><span class="line">      val confFactor = new NormalDistribution().</span><br><span class="line">        inverseCumulativeProbability(1 - (1 - confidence) / 2)</span><br><span class="line">      val low = mean - confFactor * stdev</span><br><span class="line">      val high = mean + confFactor * stdev</span><br><span class="line">      new BoundedDouble(mean, confidence, low, high)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>其中totalOutputs是partition的个数。上面代码的逻辑是：如果已经计算了所有partition，则返回的结果是100%准确的；如果一个partition都未完成，那么结果完全不可信；否则，按比例计算mean，variance跟已返回比例有关，越多则variance越小，其low/high都是根据confidence和mean算出来的。</p><h4 id="countByValue"><a href="#countByValue" class="headerlink" title="countByValue"></a>countByValue</h4><p><strong>def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long]</strong><br>实际上就是一个map + reduce的过程，而所得结果因为需要转化为Map，需要把所得内容完全载入driver的内存，所以只适合不同的value的数量比较小的情况。</p><h4 id="countByValueApprox"><a href="#countByValueApprox" class="headerlink" title="countByValueApprox"></a>countByValueApprox</h4><p><strong>def countByValueApprox(timeout: Long, confidence: Double = 0.95)(implicit ord: Ordering[T] = null) : PartialResult[Map[T, BoundedDouble]]</strong><br>与前面提到的countApprox实现类似。</p><h4 id="zipWithIndex"><a href="#zipWithIndex" class="headerlink" title="zipWithIndex"></a>zipWithIndex</h4><p><strong>def zipWithIndex(): RDD[(T, Long)]</strong><br>获得一个新的RDD，其中每个元素都是一个二元组，其中value是元素所在RDD中的全局index。该操作不保证重复时index的顺序不变。这个操作表面上是一个算子，但实际上会触发一个spark job，因为在执行之前需要知道每个partition的起始index，而这只能通过count每个partition来得到。</p><h4 id="take"><a href="#take" class="headerlink" title="take"></a>take</h4><p><strong>def take(num: Int): Array[T]</strong><br>take的作用是从一个RDD中获取给定数量num个数的元素，得到一个数组。实现的基本思路是，首先尝试读一个partition，然后根据得到的元素数量与num的比较决定是否需要再探索其它的partition，以及探索的partition数量。这个探索数量的策略似乎比较heuristic，大体上是每次探索的partition数量小于等于已探索的4倍，而具体的值跟已探索到的元素数量与num的关系来定。从实现上看，take返回的所有元素都保存在一个数组内，所以如果num数量过大会引起内存问题。</p><h4 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h4><p><strong>def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]</strong><br>takeOrdered除了获取num个元素外，还要求这些元素按照ord给出的排序方式排序。其实现的核心代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val mapRDDs = mapPartitions &#123; items =&gt;</span><br><span class="line">        // Priority keeps the largest elements, so let&apos;s reverse the ordering.</span><br><span class="line">        val queue = new BoundedPriorityQueue[T](num)(ord.reverse)</span><br><span class="line">        queue ++= util.collection.Utils.takeOrdered(items, num)(ord)</span><br><span class="line">        Iterator.single(queue)</span><br><span class="line">      &#125;</span><br><span class="line">      if (mapRDDs.partitions.length == 0) &#123;</span><br><span class="line">        Array.empty</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        mapRDDs.reduce &#123; (queue1, queue2) =&gt;</span><br><span class="line">          queue1 ++= queue2</span><br><span class="line">          queue1</span><br><span class="line">        &#125;.toArray.sorted(ord)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p>首先，对每个partition需要得到一个BoundedPriorityQueue，其大小固定为num。若partition内元素少于num个，则queue不满。随后，在一个reduce中，把每个partition得到的queue拼接为一个queue。BoundedPriorityQueue的拼接会按照每个元素插入队列。根据这个实现，每次takeOrdered或top操作都需要对所有partition排序，然后在结果里拼出一个大小为num的队列，代价是比较大的。</p><h3 id="常见的RDD派生类"><a href="#常见的RDD派生类" class="headerlink" title="常见的RDD派生类"></a>常见的RDD派生类</h3><h2 id="Spark-Architecture"><a href="#Spark-Architecture" class="headerlink" title="Spark Architecture"></a>Spark Architecture</h2><blockquote><p><a href="http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/" target="_blank" rel="noopener">http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/</a></p></blockquote><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><p>Shuffle的目的是把key相同的记录发送到相同的parition以供后续处理。Mapreduce中同样存在shuffle阶段。回顾mapreduce中shuffle的过程：（1）mapper将数据分为多个partition，然后parition内按照key排序（实际分两步完成），这些partition一部分写入磁盘，一部分缓存在内存里；（2）mapper输出的partition分发到对应的reducer；（3）reducer对已经排好续的记录再次进行合并排序；（4）key相同的记录被group为一个iterable交给reduce方法处理。</p><blockquote><p>补充材料：《Hadoop: The Definitive Guide》英文版，197页</p></blockquote><h3 id="Shuffle的两种方法"><a href="#Shuffle的两种方法" class="headerlink" title="Shuffle的两种方法"></a>Shuffle的两种方法</h3><p>Spark中shuffle“目前”有两种实现，分别是基于hash和sort。</p><p>基于hash的方式在spark 1.2.0以前是默认的方式。其实现思路非常简单，对于任意输入RDD中的partition，根据hash结果产生N个文件。N表示“reducer”的数量。由于没有排序，每条记录经过hash后直接写入文件，因此速度较快。对于后续处理不需要排序的情况，基于hash的shuffle性能较好。其缺陷是产生的文件数量较大。</p><p>基于sort的方式达到的效果与mapreduce里的shuffle一样，但实现上有较大的差异。首先，从“mapper”写出的数据是不做本地排序的，只有在“reducer”从远端获取数据时才会触发排序过程。这里需要了解spark中的AppendOnlyMap的数据结构。简单来说，在数据量足够小的情况下，“mapper”输出的数据会保存在内存一个AppendOnlyMap中。如果数据较多，则会将AppendOnlyMap变换为一个priority queue，按key排序后保存到外部文件中。这样一来，一次map操作的所有数据会保存在一个内存里的AppendOnlyMap加若干外部的文件。当“reducer”请求数据的时候，这些数据分片会被组织成一个最小堆，每次读取一个key最小的记录，从而实现了排序的功能。“Reducer“拿到各个数据分片后，采用TimSort来对所有数据排序，而不是mapreduce中的合并排序。</p><blockquote><p>补充材料：<br><a href="https://0x0fff.com/spark-architecture-shuffle/" target="_blank" rel="noopener">Spark Architecture: Shuffle</a><br><a href="http://dataknocker.github.io/2014/07/23/spark-appendonlymap/" target="_blank" rel="noopener">spark的外排:AppendOnlyMap与ExternalAppendOnlyMap</a></p></blockquote><h2 id="Block-Manager"><a href="#Block-Manager" class="headerlink" title="Block Manager"></a>Block Manager</h2><p>Block Manager在spark中作为一层存储抽象层存在。RDD的iterator方法里有读取缓存的partition的入口getOrCompute，其中block的id定义为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val key = RDDBlockId(rdd.id, partition.index)</span><br><span class="line"></span><br><span class="line">case class RDDBlockId(rddId: Int, splitIndex: Int) extends BlockId &#123;</span><br><span class="line">  override def name: String = &quot;rdd_&quot; + rddId + &quot;_&quot; + splitIndex</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从实现上看每个RDD的partition都有一个唯一的key，用于blockmanager存储的键值。一个partition应该与一个block一一对应的。Block的存储完全由block manager来管理。</p><p><a href="https://issues.apache.org/jira/browse/SPARK-6235" target="_blank" rel="noopener">关于block size不能超过2g限制的issue tracker</a></p><p>不错的参考资料<br><a href="http://jerryshao.me/architecture/2013/10/08/spark-storage-module-analysis/" target="_blank" rel="noopener">Spark源码分析之-Storage模块</a><br><a href="http://cholerae.com/2015/03/06/Spark%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/" target="_blank" rel="noopener">Spark缓存机制分析</a><br><a href="http://www.cnblogs.com/hseagle/p/3673138.html" target="_blank" rel="noopener">Apache Spark源码走读之6 – 存储子系统分析</a></p><h2 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h2><p><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-dagscheduler.html" target="_blank" rel="noopener">DAGScheduler</a><br><a href="https://issues.apache.org/jira/browse/SPARK-9850" target="_blank" rel="noopener">Adaptive execution in Spark</a></p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><p>可以反复学习的blog<br><a href="http://dataknocker.github.io" target="_blank" rel="noopener">http://dataknocker.github.io</a></p>]]></content>
    
    <summary type="html">
    
      My notes taken for learning spark.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="big data" scheme="liqul.github.io/blog/tags/big-data/"/>
    
      <category term="spark" scheme="liqul.github.io/blog/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Two-phase Commit</title>
    <link href="liqul.github.io/blog/2017/06/09/2017-06-09-two-phase-commit-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2017/06/09/2017-06-09-two-phase-commit-NSConflict-liqul/</id>
    <published>2017-06-09T06:32:00.000Z</published>
    <updated>2018-04-10T13:59:45.238Z</updated>
    
    <content type="html"><![CDATA[<p>I recently came across a good description of two-phase commit from actordb’s document. I decide to borrow it as a note. The following is copied from <a href="http://www.actordb.com/docs-howitworks.html#h_323" target="_blank" rel="noopener">actordb’s document</a>:</p><p>3.2.3 Multi-actor transactions<br>Multi-actor transactions need to be ACID compliant. They are executed by a transaction manager. The manager is itself an actor. It has name and a transaction number that is incremented for every transaction.</p><p>Sequence of events from the transaction manager point of view:</p><ol><li>Start transaction by writing the number and state (uncommitted) to transaction table of transaction manager actor.</li><li>Go through all actors in the transaction and execute their belonging SQL to check if it can execute, but do not commit it. If actor successfully executes SQL it will lock itself (queue all reads and writes).</li><li>All actors returned success. Change state in transaction table for transaction to committed.</li><li>Inform all actors that they should commit.</li></ol><p>Sequence of events from an actors point of view:</p><ol><li>Actor receives SQL with a transaction ID, transaction number and which node transaction manager belongs to.</li><li>Store the actual SQL statement with transaction info to a transaction table (not execute it).</li><li>Once it is stored, the SQL will be executed but not committed. If there was no error, return success.</li><li>Actor waits for confirm or abort from transaction manager. It will also periodically check back with the transaction manager in case the node where it was running from went down and confirmation message is lost.</li><li>Once it has a confirmation or abort message it executes it and unlocks itself.</li></ol><p>Problem scenarios:</p><ol><li>Node where transaction manager lives goes down before committing transaction: Actors will be checking back to see what state a transaction is in. If transaction manager actor resumes on another node and sees an uncommitted transaction, it will mark it as aborted. Actors will in turn abort the transaction as well.</li><li>Node where transaction manager lives goes down after committing transaction to local state, but before informing actors that transaction was confirmed. Actors checking back will detect a confirmed transaction and commit it.</li><li>Node where one or more actors live goes down after confirming that they can execute transaction. The actual SQL statements are stored in their databases. The next time actors start up, they will notice that transaction. Check back with the transaction manager and either commit or abort it.</li></ol>]]></content>
    
    <summary type="html">
    
      I recently came across a good description of two-phase commit from actordb&#39;s document. I decide to borrow it as a note. The following is copied from actordb&#39;s document
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="transaction" scheme="liqul.github.io/blog/tags/transaction/"/>
    
  </entry>
  
  <entry>
    <title>摄影笔记</title>
    <link href="liqul.github.io/blog/2017/05/04/2017-05-04-notes-on-photography-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2017/05/04/2017-05-04-notes-on-photography-NSConflict-liqul/</id>
    <published>2017-05-04T01:58:00.000Z</published>
    <updated>2018-04-10T13:59:45.238Z</updated>
    
    <content type="html"><![CDATA[<h3 id="焦段选择的一些感想："><a href="#焦段选择的一些感想：" class="headerlink" title="焦段选择的一些感想："></a>焦段选择的一些感想：</h3><p><strong>广角（&lt;35mm)</strong></p><ul><li>场面干净：由于广角会摄入较广的场景，所以必须保证其中不要有不希望被包括的主体</li><li>中心突出：没有中心的广角构图是非常失败的，这比其它焦段更加要求中心突出</li><li>线条整齐对称：没有细密整齐的线条，广角会非常乏味，这些线条可以是建筑、地面的纹路、天际线等等</li><li>身临其境：广角照片给人的印象是身历其境，所以角度一般不能太平庸，要么居高临下，要么自底向上</li><li>多元素：元素可以多一点但最好是能够相互呼应的</li></ul><p><strong>中焦(35mm~70mm)</strong></p><ul><li>现实感：由于其呈现的效果更加接近人眼所以能给人一种“旁观”的感觉，更加适合拍摄纪实的题材，其带来的震撼感要高于其它焦段</li><li>距离变化：在这个焦段范围中，一点点变化都能对拍摄距离产生较大影响</li></ul><p><strong>长焦(&gt;70mm)</strong></p><ul><li>微距：把较远处的主体拍到眼前是长焦的主要作用之一</li><li>压缩场景：由于长焦会把多个主体间的距离弱化，很像中国画的感觉，体现的是一种平面的美感</li><li>少元素：元素尽量少一点，画面简单一点，弱水三千只取一瓢</li><li>虚化加成：由于长焦带来的虚化加成，在稍微大一点的光圈下能达到所谓“空气切割”的感觉</li></ul><h3 id="场景-vs-焦段"><a href="#场景-vs-焦段" class="headerlink" title="场景 vs. 焦段"></a>场景 vs. 焦段</h3><ul><li>苏州园林：原本是为了人眼优化的布景，更加适合中焦和长焦</li><li>城市建筑：广角更能呈现出震撼的感觉，加上建筑的线条在广角中更具有表现力；一些广场上的建筑由于没有遮挡，在没人的时候也可以用长一点的焦段</li><li>人像：跟场景有关，在场景杂乱的地方就老实用中长焦大光圈虚化；在户外视场景而定广角可以突出人与宏达场景的相映，中焦更接近生活，长焦可以捕捉一些在无干扰情况下的活动，总而言之还是跟背景有关系</li></ul><h3 id="一些原则"><a href="#一些原则" class="headerlink" title="一些原则"></a>一些原则</h3><ul><li>色彩尽量少一点，不要给人一种杂乱的感觉</li><li>一定要有主体，不然没有着眼点</li><li>场景中的元素除非必要尽量不要包括进来</li></ul>]]></content>
    
    <summary type="html">
    
      My naive thoughts on photography...
    
    </summary>
    
      <category term="Photography" scheme="liqul.github.io/blog/categories/Photography/"/>
    
    
      <category term="photography" scheme="liqul.github.io/blog/tags/photography/"/>
    
  </entry>
  
  <entry>
    <title>Setup SBT Development Environment</title>
    <link href="liqul.github.io/blog/2017/04/10/2017-04-10-setup-sbt-development-environment-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2017/04/10/2017-04-10-setup-sbt-development-environment-NSConflict-liqul/</id>
    <published>2017-04-10T02:52:39.000Z</published>
    <updated>2018-04-10T13:59:45.237Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>Setup JDK following Oracle guidance.</p></li><li><p>Setup SBT</p></li></ol><p>No matter which platform you are on. I recommend downloading the <a href="https://dl.bintray.com/sbt/native-packages/sbt/0.13.15/sbt-0.13.15.zip" target="_blank" rel="noopener">zip</a> archive directly.</p><p>Put the following into <code>~/.sbt/repositories</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line"><span class="comment">#local</span></span><br><span class="line">public: http://maven.aliyun.com/nexus/content/groups/public/</span><br><span class="line">typesafe:http://dl.bintray.com/typesafe/ivy-releases/ , [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[<span class="built_in">type</span>]s/[artifact](-[classifier]).[ext], bootOnly</span><br><span class="line">ivy-sbt-plugin:http://dl.bintray.com/sbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[<span class="built_in">type</span>]s/[artifact](-[classifier]).[ext]</span><br><span class="line">sonatype-oss-releases</span><br><span class="line"></span><br><span class="line">sonatype-oss-snapshots</span><br></pre></td></tr></table></figure><p>Run <code>sbt</code> and <code>sbt console</code>. If you see all downloads from aliyun, you’ve setup it successfully. Test creating a new SBT project in intellij to see if everything ok. </p>]]></content>
    
    <summary type="html">
    
      Deploying SBT is not easy...
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="sbt" scheme="liqul.github.io/blog/tags/sbt/"/>
    
      <category term="scala" scheme="liqul.github.io/blog/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>Notes on Using &quot;Select ... For Update&quot; for Uniqueness in Mysql</title>
    <link href="liqul.github.io/blog/2017/03/31/2017-04-07-using-select-for-update-for-uniqueness-in-mysql-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2017/03/31/2017-04-07-using-select-for-update-for-uniqueness-in-mysql-NSConflict-liqul/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-04-10T13:59:45.236Z</updated>
    
    <content type="html"><![CDATA[<p>I encountered a deadlock recently. Similar questions have been asked on StackOverflow, e.g., <a href="http://stackoverflow.com/questions/21851119/deadlock-using-select-for-update-in-mysql" target="_blank" rel="noopener">this</a> and <a href="http://stackoverflow.com/questions/43251975/mysql-select-for-update-blocks-1st-insert-if-using-non-primary-key-in-where-clau" target="_blank" rel="noopener">this</a>. But the answers didn’t really explain why this happens. </p><p>The situation is quite easy to reproduce @ Mysql 5.7.17 (also tested on other versions in 5.5 or 5.6):</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`test`</span> (</span><br><span class="line">  <span class="string">`id`</span> <span class="built_in">bigint</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  <span class="string">`val`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>),</span><br><span class="line">  <span class="keyword">KEY</span> <span class="string">`search`</span> (<span class="string">`val`</span>)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> AUTO_INCREMENT=<span class="number">4</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'pre-lock'</span>;</span><br></pre></td></tr></table></figure><p><strong>session1</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session2</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session1</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'/a/b/c'</span>;</span><br></pre></td></tr></table></figure></p><p><strong>session2</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR 1213 (40001): Deadlock found when trying to get <span class="keyword">lock</span>; try restarting transaction</span><br></pre></td></tr></table></figure></p><p>The result of show engine innodb status:</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">LATEST DETECTED DEADLOCK</span><br><span class="line"><span class="comment">------------------------</span></span><br><span class="line">2017-04-06 23:54:03 0x7000057db000</span><br><span class="line">*** (1) TRANSACTION:</span><br><span class="line">TRANSACTION 1333, ACTIVE 18 sec starting index read</span><br><span class="line">mysql tables in <span class="keyword">use</span> <span class="number">1</span>, <span class="keyword">locked</span> <span class="number">1</span></span><br><span class="line"><span class="keyword">LOCK</span> <span class="keyword">WAIT</span> <span class="number">2</span> <span class="keyword">lock</span> <span class="keyword">struct</span>(s), <span class="keyword">heap</span> <span class="keyword">size</span> <span class="number">1136</span>, <span class="number">1</span> <span class="keyword">row</span> <span class="keyword">lock</span>(s)</span><br><span class="line">MySQL <span class="keyword">thread</span> <span class="keyword">id</span> <span class="number">5</span>, OS <span class="keyword">thread</span> handle <span class="number">123145394155520</span>, <span class="keyword">query</span> <span class="keyword">id</span> <span class="number">62</span> localhost root Sending <span class="keyword">data</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">test</span> <span class="keyword">where</span> val=<span class="string">'pre-lock'</span> <span class="keyword">for</span> <span class="keyword">update</span></span><br><span class="line">*** (<span class="number">1</span>) WAITING <span class="keyword">FOR</span> THIS <span class="keyword">LOCK</span> <span class="keyword">TO</span> BE GRANTED:</span><br><span class="line"><span class="built_in">RECORD</span> LOCKS <span class="keyword">space</span> <span class="keyword">id</span> <span class="number">24</span> page <span class="keyword">no</span> <span class="number">4</span> n bits <span class="number">72</span> <span class="keyword">index</span> <span class="keyword">search</span> <span class="keyword">of</span> <span class="keyword">table</span> <span class="string">`test_tnx`</span>.<span class="string">`test`</span> trx <span class="keyword">id</span> <span class="number">1333</span> lock_mode X waiting</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** (2) TRANSACTION:</span><br><span class="line">TRANSACTION 1332, ACTIVE 29 sec inserting</span><br><span class="line">mysql tables in <span class="keyword">use</span> <span class="number">1</span>, <span class="keyword">locked</span> <span class="number">1</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">lock</span> <span class="keyword">struct</span>(s), <span class="keyword">heap</span> <span class="keyword">size</span> <span class="number">1136</span>, <span class="number">4</span> <span class="keyword">row</span> <span class="keyword">lock</span>(s), <span class="keyword">undo</span> <span class="keyword">log</span> entries <span class="number">1</span></span><br><span class="line">MySQL <span class="keyword">thread</span> <span class="keyword">id</span> <span class="number">62</span>, OS <span class="keyword">thread</span> handle <span class="number">123145394434048</span>, <span class="keyword">query</span> <span class="keyword">id</span> <span class="number">63</span> localhost root <span class="keyword">update</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">test</span> <span class="keyword">set</span> val=<span class="string">'/a/b/c'</span></span><br><span class="line">*** (<span class="number">2</span>) HOLDS THE <span class="keyword">LOCK</span>(S):</span><br><span class="line"><span class="built_in">RECORD</span> LOCKS <span class="keyword">space</span> <span class="keyword">id</span> <span class="number">24</span> page <span class="keyword">no</span> <span class="number">4</span> n bits <span class="number">72</span> <span class="keyword">index</span> <span class="keyword">search</span> <span class="keyword">of</span> <span class="keyword">table</span> <span class="string">`test_tnx`</span>.<span class="string">`test`</span> trx <span class="keyword">id</span> <span class="number">1332</span> lock_mode X</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">1</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">1</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 73757072656d756d; asc supremum;;</span><br><span class="line"></span><br><span class="line">Record <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** (2) WAITING FOR THIS LOCK TO BE GRANTED:</span><br><span class="line">RECORD LOCKS space id 24 page no 4 n bits 72 index search of table `test_tnx`.`test` trx id 1332 lock_mode X locks gap before rec <span class="keyword">insert</span> intention waiting</span><br><span class="line"><span class="built_in">Record</span> <span class="keyword">lock</span>, <span class="keyword">heap</span> <span class="keyword">no</span> <span class="number">2</span> <span class="keyword">PHYSICAL</span> <span class="built_in">RECORD</span>: n_fields <span class="number">2</span>; compact format; info bits 0</span><br><span class="line"> 0: len 8; hex 7072652d6c6f636b; asc pre-<span class="keyword">lock</span>;;</span><br><span class="line"> 1: len 8; hex 8000000000000001; asc         ;;</span><br><span class="line"></span><br><span class="line">*** WE ROLL BACK TRANSACTION (1)</span><br></pre></td></tr></table></figure><p>My objective is to use <code>select ... for update</code> as a uniqueness check for a following sequence of insertions. I expected that Tnx 2 would wait until Tnx 1 released the lock, and then continue its own business. However, Tnx 2 is rolled back due to deadlock. The innodb status looks quite confusing. Tnx 1 is holding and waiting for the same lock. </p><p>After some research, though I still cannot figure out the root cause, my perception is that the insertion in Tnx 1 acquires a gap lock which is somehow overlapping with the gap lock by the <code>select ... for update</code>. And therefore, this create a deadlock where Tnx 1 waits for Tnx 2 and Tnx 2 waits for Tnx 1. </p><p>During my research, I found that the right <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-next-key-locking.html" target="_blank" rel="noopener">use case</a> for <code>select ... for update</code> is as follows:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> [<span class="keyword">table</span>] <span class="keyword">where</span> [condition] <span class="keyword">for</span> <span class="keyword">update</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> [<span class="keyword">table</span>] <span class="keyword">values</span> [belongs <span class="keyword">to</span> condition];</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> [<span class="keyword">table</span>] <span class="keyword">where</span> [belongs <span class="keyword">to</span> condition];</span><br></pre></td></tr></table></figure><p>The rows being mutated should be explicitly locked by the <code>select ... for update</code>. Also, the condition should be as clear as possible. For example, put only an unique key in the condition. This is to make the gap lock with a simple and clear range, in order not to cause deadlocks.</p><p>Generally, using <code>select ... for update</code> is non-trivial since the underlying locking mechanism seems quite complicated. For my scenario, I got two workarounds:</p><ol><li>Disable gap locks by setting the <a href="https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html" target="_blank" rel="noopener">isolation level</a> to <code>READ COMMITTED</code>.</li><li>Apply <code>select ... for update</code> on a row from another table, which avoid possible lock overlap. </li></ol>]]></content>
    
    <summary type="html">
    
      I encountered a deadlock recently. Similar questions have been asked on StackOverflow, e.g., this and this. But the answers didn&#39;t really explain why this happens.
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="lock" scheme="liqul.github.io/blog/tags/lock/"/>
    
      <category term="Mysql" scheme="liqul.github.io/blog/tags/Mysql/"/>
    
      <category term="Deadlock" scheme="liqul.github.io/blog/tags/Deadlock/"/>
    
      <category term="Uniqueness" scheme="liqul.github.io/blog/tags/Uniqueness/"/>
    
  </entry>
  
  <entry>
    <title>Notes on The Raft Consensus Algorithm</title>
    <link href="liqul.github.io/blog/2017/03/31/2017-04-01-the-raft-consensus-algorithm-NSConflict-liqul/"/>
    <id>liqul.github.io/blog/2017/03/31/2017-04-01-the-raft-consensus-algorithm-NSConflict-liqul/</id>
    <published>2017-03-31T06:25:39.000Z</published>
    <updated>2018-04-10T13:59:45.236Z</updated>
    
    <content type="html"><![CDATA[<p>What’s consensus?</p><blockquote><p>It allows a collection of machines to work as a coherent group that can survive the failures of some of its members.</p></blockquote><p>It means not only a group of machines reach a final decision for a request, but also the state machine is replicated across these machines, so that some failures do not affect the functioning. Raft is a consensus algorithm seeking to be correct, implementable, and understandable. </p><p>The <a href="https://ramcloud.stanford.edu/~ongaro/thesis.pdf" target="_blank" rel="noopener">thesis</a> is very well written. It is much more comprehensive compared to the NSDI paper. Implementing Raft based on the thesis shouldn’t be too difficult (of course, also not trivial). The author also built a <a href="https://raft.github.io/" target="_blank" rel="noopener">website</a> putting all kinds of helping things there. I read the paper and decide to take some notes here.</p><p><img src="/blog/assets/state.png" width="800"></p><p>There are two key parts sitting in the core of the algorithm:</p><p><strong>Leader election</strong></p><p>The election is triggered by a timeout. If a server failed to detect heartbeats from the current leader, it start a new <em>term</em> of election. During the term, it broadcast requests to collect votes from other servers. If equal or more than majority of servers reply with a vote, the server becomes the leader of this term. The “term” here is a monotonically increasing logic time. From the perspective of a server receiving the vote request, it decides whether to give the vote based on a few considerations. First of all, if the sender even falls behind the receiver in terms of log index, the receiver should not vote for it. Also, if the receiver can still hear the heartbeats from current leader, it should not vote too. In this case, the requester might be a <em>disruptive server</em>. In other cases, the receiver should vote for the sender. </p><p><img src="/blog/assets/leader election.png" width="800"></p><p><strong>Log replication</strong></p><p>Once a server becomes the leader, it’s mission is simply replicate it’s log to every other follower. The replication means make the log of a follower <em>exactly</em> the same as the leader. For each pair of leader and follower, the leader first identify the highest index where they reach an agreement. Starting from there, the leader overwrite its log to the follower. The leader handles all requests from clients. Once it receives a new request, it first put the request into its own log. Then, it replicate the request to all followers. If equal or more than majority followers (including the leader itself) answer the replication request with success, the leader apply the request into its state machine (this is called commit). The leader put the new log index into its heartbeats, so followers know if the request has been committed, after which each follower commit the request too.</p><p><img src="/blog/assets/log replication.png" width="800"></p><p>More formal introduction of the core Raft could be found in Fig. 3.1 in the thesis paper. There are also a few extensions to make the algorithm practical to be used in production systems, such as the group management. I also found Fig. 10.1 a very good reference of architecture. </p><p>There are quite a lot of implementations of Raft, which could be found <a href="https://raft.github.io/" target="_blank" rel="noopener">here</a>. I also find a project named Copycat, with code <a href="https://github.com/atomix/copycat" target="_blank" rel="noopener">here</a> and document <a href="http://atomix.io/copycat/" target="_blank" rel="noopener">here</a>. Copycat is a full featured implementation of Raft in java. Building your own application based on Copycat shouldn’t be too difficult. They provide an example of implementing a KV store based on Copycat in their source code <a href="https://github.com/atomix/copycat/tree/master/examples" target="_blank" rel="noopener">here</a>, which is used as the “<a href="http://atomix.io/copycat/docs/getting-started/" target="_blank" rel="noopener">Get Started</a>“ tutorial. Another very important reason, why I think Copycat a good reference, is that it emphases the abstraction of state machine, client, server, and operations. Therefore, going through it’s document enhanced my understanding of Raft. </p><p>If you don’t want to build your own Raft, may be Copycat is worthwhile a try, though I haven’t any real experience beyond a toy project.</p><p>The annotated thesis could be found <a href="/blog/assets/raft-thesis.pdf">here</a>.</p><p><strong>A go-through case for understanding</strong></p><p>A typical request handling process is as follows:</p><ol><li>The client sends a request to the cluster;</li><li>The leader handles the request by putting it to a WAL;</li><li>The leader sends the request to all followers;</li><li>Each follower puts the received request to its WAL, and responds to the leader;</li><li>Once the leader has heard a majority number of responses from its followers, the leader commit the request by applying the WAL to its state machine;</li><li>The leader inform the client that the request has been handled properly, and then, put the index of the request into its heartbeat to let all followers know the status of each request;</li><li>Once the follower knows that the request has been committed by the leader, the follower commit the request too by applying it to its own state machine. </li></ol><p>There are a few key points to understand in the process above:</p><p>1.Does the client always know if its request has been handled properly?</p><p>No. If the leader commits the request and then crashes, the client will not know if the request has been actually successfully handled. In some cases, the client will resend the request which may lead to duplicated data. It leaves for the client to avoid such kind of duplication. </p><p>2.How about the leader crashes before inform its followers that the request has been committed?</p><p>If the leader crashes, a follower will be elected to be the next leader. The follower must have the latest state according to the mechanism of Raft. Therefore, the next leader definitely has the WAL for the request, and the request has definitely been replicated across a majority number of hosts. Therefore, it is safe to replicate its state to all followers. </p><p>3.Key feature of a consensus algorithm (or strong consistency)?</p><p>Under normal situations, if there’s a state change, the key step changing the state should be always handled by a certain node. The state changing should be replicated to a majority number of followers before informing the requester a success. Each read request goes to that certain node as well. Once there’s node failures or networking partitions, the service stop working until returning to the normal situation again.</p>]]></content>
    
    <summary type="html">
    
      It allows a collection of machines to work as a coherent group that can survive the failures of some of its members
    
    </summary>
    
      <category term="Tech" scheme="liqul.github.io/blog/categories/Tech/"/>
    
    
      <category term="Raft" scheme="liqul.github.io/blog/tags/Raft/"/>
    
      <category term="Consensus" scheme="liqul.github.io/blog/tags/Consensus/"/>
    
      <category term="Copycat" scheme="liqul.github.io/blog/tags/Copycat/"/>
    
  </entry>
  
</feed>
